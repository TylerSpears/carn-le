{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13edb204-c964-4525-80d8-0fdd2730df3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pain in the Net - Laplacian Pyramid Translation Network (LPTN)\n",
    "Application of Laplacian Pyramid Translation Network (LPTN) to domain adaptation of diffusion MRI.\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Based on the following work(s):\n",
    "\n",
    "* `J. Liang, H. Zeng, and L. Zhang, “High-Resolution Photorealistic Image Translation in Real-Time: A Laplacian Pyramid Translation Network,” 2021, pp. 9392–9400. Accessed: Aug. 26, 2021. [Online]. Available: https://openaccess.thecvf.com/content/CVPR2021/html/Liang_High-Resolution_Photorealistic_Image_Translation_in_Real-Time_A_Laplacian_Pyramid_Translation_CVPR_2021_paper.html\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da03c5c0-aea8-4c1c-98ca-bf001e59dde2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f91f0-eca2-4990-9214-f34aa16b4d05",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbe9cc-5b3f-4204-870d-4a5bc5f28aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import inspect\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "import zipfile\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dipy.viz\n",
    "import dipy.viz.regtools\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import IPython\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "import ipyplot\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import addict\n",
    "from addict import Addict\n",
    "import box\n",
    "from box import Box\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "import monai\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import scipy\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8326b-55e1-4195-a85f-7627f591f1f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8449ea-9648-4517-8aab-7e85308436de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a706635-b544-4f41-886b-d6997dda27e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498af6cf-4263-4415-b6f3-fd32abab4bcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2952b-b4a3-4241-9322-e317c32cb2db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae38f0a-8614-4e0d-a40a-bee5a3585baa",
   "metadata": {
    "tags": [
     "keep_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-09-24T18:31:24.463846+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.23.1\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-84-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: ee869f65131c73fb8cc6d2ecf04f2d31014b8426\n",
      "\n",
      "seaborn          : 0.11.1\n",
      "IPython          : 7.23.1\n",
      "pandas           : 1.2.3\n",
      "addict           : 2.4.0\n",
      "torch            : 1.9.0\n",
      "dipy             : 1.4.1\n",
      "nibabel          : 3.2.1\n",
      "torchio          : 0.18.37\n",
      "numpy            : 1.20.2\n",
      "ipywidgets       : 7.6.3\n",
      "natsort          : 7.1.1\n",
      "matplotlib       : 3.4.1\n",
      "box              : 5.4.1\n",
      "skimage          : 0.18.1\n",
      "json             : 2.0.9\n",
      "pytorch_lightning: 1.4.5\n",
      "sys              : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "GPUtil           : 1.4.0\n",
      "ants             : 0.2.7\n",
      "monai            : 0.7.dev2138\n",
      "scipy            : 1.5.3\n",
      "\n",
      "================================================== GPU Specs ==================================================\n",
      "  id  Name       Driver Version      CUDA Version  Total Memory    uuid\n",
      "----  ---------  ----------------  --------------  --------------  ----------------------------------------\n",
      "   0  TITAN RTX  460.91.03                   11.1  24217.0MB       GPU-586d2016-71ef-ebb3-437b-6721a22191ec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09447c83-298e-444a-91ca-aa46058eb956",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc41112b-67dc-4fa3-828e-48a2c88dfb49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"])\n",
    "hcp_source_data_dir = data_dir / \"hcp\"\n",
    "clinic_source_data_dir = data_dir / \"uva/chronic_pain_head_and_neck\"\n",
    "assert hcp_source_data_dir.exists() and clinic_source_data_dir.exists()\n",
    "processed_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"])\n",
    "hcp_processed_data_dir = processed_data_dir / \"hcp\"\n",
    "clinic_processed_data_dir = processed_data_dir / \"uva/chronic_pain_head_and_neck\"\n",
    "assert hcp_processed_data_dir.exists() and clinic_processed_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()\n",
    "tmp_results_dir = pathlib.Path(os.environ[\"TMP_RESULTS_DIR\"])\n",
    "assert tmp_results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a7233-48c1-439d-af3f-62f628af527c",
   "metadata": {},
   "source": [
    "### Experiment Logging Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc6f9-42fc-4ce6-905e-c938b89edb0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensorboard experiment logging setup.\n",
    "EXPERIMENT_NAME = \"debug_da_gan\"\n",
    "\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = ts + \"__\" + EXPERIMENT_NAME\n",
    "run_name = experiment_name\n",
    "print(experiment_name)\n",
    "# experiment_results_dir = results_dir / experiment_name\n",
    "\n",
    "# Create temporary directory for results directory, in case experiment does not finish.\n",
    "tmp_dirs = list(tmp_results_dir.glob(\"*\"))\n",
    "\n",
    "# Only keep up to N tmp results.\n",
    "n_tmp_to_keep = 3\n",
    "if len(tmp_dirs) > (n_tmp_to_keep - 1):\n",
    "    print(f\"More than {n_tmp_to_keep} temporary results, culling to the most recent\")\n",
    "    tmps_to_delete = natsorted([str(tmp_dir) for tmp_dir in tmp_dirs])[\n",
    "        : -(n_tmp_to_keep - 1)\n",
    "    ]\n",
    "    for tmp_dir in tmps_to_delete:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        print(\"Deleted temporary results directory \", tmp_dir)\n",
    "\n",
    "experiment_results_dir = tmp_results_dir / experiment_name\n",
    "# Final target directory, to be made when experiment is complete.\n",
    "final_experiment_results_dir = results_dir / experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe6460c-da7e-4391-980a-344508313cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pass this object into the pytorchlightning Trainer object, for easier logging within\n",
    "# the training/testing loops.\n",
    "pl_logger = pl.loggers.TensorBoardLogger(\n",
    "    tmp_results_dir,\n",
    "    name=experiment_name,\n",
    "    version=\"\",\n",
    "    log_graph=False,\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "# Use the lower-level logger for logging histograms, images, etc.\n",
    "logger = pl_logger.experiment\n",
    "\n",
    "# Create a separate txt file to log streams of events & info besides parameters & results.\n",
    "log_txt_file = Path(logger.log_dir) / \"log.txt\"\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")\n",
    "    # cap is defined in an ipython magic command\n",
    "    f.write(f\"Environment and Hardware Info:\\n {cap}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e4788-bced-42d5-b341-3652317f2427",
   "metadata": {},
   "source": [
    "### Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06aa61-9f96-4b68-92c9-4a54bf351870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = Box(default_box=True)\n",
    "\n",
    "# Data params.\n",
    "params.num_channels = 6\n",
    "params.hcp.num_subjects = 5\n",
    "params.clinic.num_subjects = 1\n",
    "params.clamp_percentiles = (0.01, 99.99)\n",
    "# Must be a factor of 2**num_laplace_high_freq\n",
    "params.patch_size = (32, 32, 32)\n",
    "\n",
    "# Network params.\n",
    "params.num_laplace_high_freq = 3\n",
    "params.discriminator_downscale_factors = [1, 2, 4]\n",
    "params.lambda_adversary_loss = 0.1\n",
    "params.optim.lr = 0.001\n",
    "params.optim.betas = (0.9, 0.999)\n",
    "\n",
    "# Training, validation, & testing params\n",
    "params.batch_size = 12\n",
    "params.samples_per_subj_per_epoch = 100\n",
    "params.max_epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1009ab0f-71cd-4692-8722-e6fa947366d1",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ea665-dc8b-45b8-be6f-b9cc3c5f62b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformation pipeline.\n",
    "# The input to the laplacian pyramid must be divisible by 2 for the number of high-\n",
    "# frequency levels in the pyramid.\n",
    "laplace_pyramid_divisible_by_shape = 2 ** params.num_laplace_high_freq\n",
    "\n",
    "pre_process_pipeline = monai.transforms.Compose(\n",
    "    [\n",
    "        monai.transforms.CropForegroundd([\"dti\", \"mask\"], source_key=\"mask\", margin=3),\n",
    "        monai.transforms.DivisiblePadd(\n",
    "            [\"dti\", \"mask\"], laplace_pyramid_divisible_by_shape\n",
    "        ),\n",
    "        pitn.transforms.ClipPercentileTransformd(\n",
    "            \"dti\",\n",
    "            params.clamp_percentiles[0],\n",
    "            params.clamp_percentiles[1],\n",
    "            nonzero=True,\n",
    "            channel_wise=True,\n",
    "        ),\n",
    "        monai.transforms.ToTensord(\"dti\", dtype=torch.float),\n",
    "        monai.transforms.ToTensord(\"mask\", dtype=torch.bool),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03506296-ec92-4083-93c1-83308f181847",
   "metadata": {},
   "source": [
    "### Load and Pre-Process HCP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5a22-bd1a-41e3-ac99-3d4cdb1ee14c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "hcp_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.hcp.num_subjects)\n",
    "if params.hcp.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    hcp_subj_dirs[subj_id] = (\n",
    "        hcp_processed_data_dir\n",
    "        / f\"derivatives/diffusion/mean_downsample/scale-2.00mm/sub-{subj_id}\"\n",
    "    )\n",
    "    assert hcp_subj_dirs[subj_id].exists()\n",
    "ppr(hcp_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f5408d-5934-44cc-9aba-01df3461e7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected HCP Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"hcp_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f30dbd-f4d2-4144-9ba1-6dbf9097b441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "hcp_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=False)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "dti_file_prefix = \"dti\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in hcp_subj_dirs.items():\n",
    "    subj_id = int(subj_id)\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the DTIs\n",
    "    img_dir = subj_dir / dti_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"sub-{subj_id}*dti.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"dti\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"dti_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"sub-{subj_id}*mask.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject DTIs.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    hcp_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all HCP subjects included.\n",
    "hcp_subj_dataset = monai.data.Dataset(hcp_subj_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3d4d9-333b-464e-be59-485d8de80582",
   "metadata": {},
   "source": [
    "### Load & Pre-Process Clinical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cda4b-7289-4b78-bb67-88e2c3d3d67d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "clinic_subj_dirs: dict = dict()\n",
    "\n",
    "possible_ids = [\"001\"]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(possible_ids, params.clinic.num_subjects)\n",
    "if params.clinic.num_subjects < len(possible_ids):\n",
    "    warnings.warn(\n",
    "        \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "        + f\"Subj IDs selected: {selected_ids}\"\n",
    "    )\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(selected_ids)\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    clinic_subj_dirs[subj_id] = (\n",
    "        clinic_processed_data_dir / f\"derivatives/diffusion/sub-{subj_id}/ses-01\"\n",
    "    )\n",
    "    assert clinic_subj_dirs[subj_id].exists()\n",
    "ppr(clinic_subj_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166fa58d-b619-4a17-8440-43f15f2f5c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log to file and experiment.\n",
    "with open(log_txt_file, \"a+\") as f:\n",
    "    f.write(f\"Selected Clinically-Scanned Subjects: {selected_ids}\\n\")\n",
    "\n",
    "logger.add_text(\"clinic_data_subjs\", pprint.pformat(selected_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed602676-361b-49cb-af88-c7976f653e13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data loading and processing loop.\n",
    "clinic_subj_data = list()\n",
    "# Data reader object for NIFTI files.\n",
    "nib_reader = monai.data.NibabelReader(as_closest_canonical=False)\n",
    "\n",
    "# Directory prefixes for each image to be read.\n",
    "dti_file_prefix = \"dti\"\n",
    "mask_file_prefix = \"mask\"\n",
    "\n",
    "for subj_id, subj_dir in clinic_subj_dirs.items():\n",
    "    subj_data = dict()\n",
    "    subj_data[\"subj_id\"] = subj_id\n",
    "\n",
    "    # Load the DTIs\n",
    "    img_dir = subj_dir / dti_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"sub-{subj_id}*dti.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"dti\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"dti_meta_dict\"] = metadata\n",
    "\n",
    "    # Load masks\n",
    "    img_dir = subj_dir / mask_file_prefix\n",
    "    img_filename = list(img_dir.glob(f\"sub-{subj_id}*mask.nii.gz\"))\n",
    "    # Make sure the glob pattern only matches one file.\n",
    "    assert len(img_filename) == 1\n",
    "    img_filename = img_filename[0]\n",
    "    nib_img = nib_reader.read(img_filename)\n",
    "    img, metadata = nib_reader.get_data(nib_img)\n",
    "    subj_data[\"mask\"] = img\n",
    "    # The default metadata key name for monai.\n",
    "    subj_data[\"mask_meta_dict\"] = metadata\n",
    "\n",
    "    # Pre-process subject DTIs.\n",
    "    subj_data = pre_process_pipeline(subj_data)\n",
    "\n",
    "    clinic_subj_data.append(subj_data)\n",
    "\n",
    "# Create dataset with all \"clinical quality\" subjects included.\n",
    "clinic_subj_dataset = monai.data.Dataset(clinic_subj_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800b62a-238d-44ba-b81a-853e9211dd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up HCP scan data.\n",
    "source_part_patch_sampler = functools.partial(\n",
    "    pitn.samplers.random_patches_from_mask,\n",
    "    patch_size=params.patch_size,\n",
    "    num_patches=params.samples_per_subj_per_epoch,\n",
    ")\n",
    "source_random_patch_sample_f = lambda p: source_part_patch_sampler(p[\"dti\"], p[\"mask\"])\n",
    "source_train_dataset = monai.data.PatchDataset(\n",
    "    hcp_subj_dataset,\n",
    "    patch_func=source_random_patch_sample_f,\n",
    "    samples_per_image=params.samples_per_subj_per_epoch,\n",
    ")\n",
    "\n",
    "# Set up clinic scan data.\n",
    "# Calculate the number of clinic samples per subject to match the total length of the\n",
    "# source domain dataset.\n",
    "num_clinic_samples_per_img = int(\n",
    "    np.floor(len(source_train_dataset) / params.clinic.num_subjects)\n",
    ")\n",
    "target_part_patch_sampler = functools.partial(\n",
    "    pitn.samplers.random_patches_from_mask,\n",
    "    patch_size=params.patch_size,\n",
    "    num_patches=num_clinic_samples_per_img,\n",
    ")\n",
    "target_random_patch_sample_f = lambda p: target_part_patch_sampler(p[\"dti\"], p[\"mask\"])\n",
    "\n",
    "target_train_dataset = monai.data.PatchDataset(\n",
    "    clinic_subj_dataset,\n",
    "    patch_func=target_random_patch_sample_f,\n",
    "    samples_per_image=num_clinic_samples_per_img,\n",
    ")\n",
    "\n",
    "# ! This will cause the source domain patches and the target domain patches to *not*\n",
    "# be aligned; this is intentional for unpaired I2I.\n",
    "\n",
    "# Zip together the source domain and target domain to feed into each training step.\n",
    "train_dataset = monai.data.ZipDataset([source_train_dataset, target_train_dataset])\n",
    "\n",
    "train_loader = monai.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    #     shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    "    #     persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29f380-824d-46c4-8c25-b90abc61a720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tio_subject_ds = torchio.SubjectsDataset(\n",
    "#     [\n",
    "#         torchio.Subject(\n",
    "#             {\n",
    "#                 **dict(\n",
    "#                     tio_img=torchio.ScalarImage(\n",
    "#                         tensor=s[\"dti\"], affine=s[\"dti_meta_dict\"][\"affine\"]\n",
    "#                     )\n",
    "#                 ),\n",
    "#                 **s,\n",
    "#             }\n",
    "#         )\n",
    "#         for s in hcp_subj_dataset\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b36bd2-f1f6-4b24-b67a-8f5a9ceb3281",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49101a65-8de8-4e51-b935-0950ce3c5393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClinicMatchGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_channels: int,\n",
    "        gen_num_high_freq: int = 3,\n",
    "        discriminator_downsample_factors=[1, 2, 4],\n",
    "        lambda_adversary_loss: float = 0.1,\n",
    "        lr=0.001,\n",
    "        betas=(0.9, 0.999),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.generator = pitn.nn.gan.generative.LPTN(\n",
    "            num_channels, num_high_freq_levels=self.hparams.gen_num_high_freq\n",
    "        )\n",
    "\n",
    "        self.discriminator = pitn.nn.gan.adversarial.MultiDiscriminator(\n",
    "            num_channels, self.hparams.discriminator_downsample_factors\n",
    "        )\n",
    "\n",
    "        self.plain_log = Box(loss_gen=dict(), loss_discrim=dict())\n",
    "        print(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generator(x)\n",
    "\n",
    "    def reconstruction_loss(self, y_source, y_pred):\n",
    "        return F.mse_loss(y_source, y_pred, reduction=\"sum\")\n",
    "\n",
    "    def ls_adversarial_loss(self, sample, label: int):\n",
    "\n",
    "        sample_pred = self.discriminator(sample)\n",
    "        sample_loss = (\n",
    "            (sample_pred - (torch.ones_like(sample_pred) * label)) ** 2\n",
    "        ).mean()\n",
    "\n",
    "        return sample_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        source_samples, target_samples = batch\n",
    "\n",
    "        # Optimizer index decides whether this step updates the generator or discriminator.\n",
    "        if optimizer_idx == self._GENERATOR_OPTIMIZER_IDX:\n",
    "            translated_samples = self.generator(source_samples)\n",
    "            l_g_reconstruct = self.reconstruction_loss(\n",
    "                source_samples, translated_samples\n",
    "            )\n",
    "\n",
    "            l_g_adversarial = (\n",
    "                self.ls_adversarial_loss(\n",
    "                    translated_samples,\n",
    "                    label=0,\n",
    "                )\n",
    "                / 2\n",
    "            )\n",
    "\n",
    "            loss_gen = l_g_reconstruct + (\n",
    "                l_g_adversarial * self.hparams.lambda_adversary_loss\n",
    "            )\n",
    "\n",
    "            # Log loss and set up return dictionary.\n",
    "            self.plain_log.loss_gen[self.global_step] = float(\n",
    "                loss_gen.detach().cpu().item()\n",
    "            )\n",
    "            tqdm_dict = {\"loss_gen\": loss_gen}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_gen, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "\n",
    "        elif optimizer_idx == self._DISCRIMINATOR_OPTIMIZER_IDX:\n",
    "            # Real images.\n",
    "            loss_real = self.ls_adversarial_loss(target_samples, label=1)\n",
    "            # Translated (i.e., fake) images\n",
    "            # We aren't updating the generator weights here, so there's no need to\n",
    "            # keep track of the generator's gradients.\n",
    "            with torch.no_grad():\n",
    "                translated_samples = self.generator(source_samples)\n",
    "            loss_fake = self.ls_adversarial_loss(translated_samples, label=-1)\n",
    "\n",
    "            loss_discrim = (loss_fake / 2) + (loss_real / 2)\n",
    "\n",
    "            # Record loss and set up return dictionary.\n",
    "            self.plain_log.loss_discrim[self.global_step] = float(\n",
    "                loss_discrim.detach().cpu().item()\n",
    "            )\n",
    "            tqdm_dict = {\"loss_discrim\": loss_discrim}\n",
    "            output = collections.OrderedDict(\n",
    "                {\"loss\": loss_discrim, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict}\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(f\"ERROR: Invalid optimizer index {optimizer_idx}\")\n",
    "        print(\n",
    "            f\"Step {self.global_step} optimizer {optimizer_idx} | loss {list(tqdm_dict.keys())[0]}: {output['loss']} | batch_size {len(source_samples)}\"\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        betas = self.hparams.betas\n",
    "\n",
    "        opt_gen = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=betas)\n",
    "        opt_discriminator = torch.optim.Adam(\n",
    "            self.discriminator.parameters(), lr=lr, betas=betas\n",
    "        )\n",
    "\n",
    "        self._GENERATOR_OPTIMIZER_IDX = 0\n",
    "        self._DISCRIMINATOR_OPTIMIZER_IDX = 1\n",
    "        return [opt_gen, opt_discriminator], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448d46aa-c344-4b15-8e08-2fdf62124a99",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c31863-63bc-40ae-b62b-62844d75f61a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# Instantiate model.\n",
    "model = ClinicMatchGAN(\n",
    "    params.num_channels,\n",
    "    gen_num_high_freq=params.num_laplace_high_freq,\n",
    "    discriminator_downsample_factors=params.discriminator_downscale_factors,\n",
    "    lambda_adversary_loss=params.lambda_adversary_loss,\n",
    "    lr=params.optim.lr,\n",
    "    betas=params.optim.betas,\n",
    ")\n",
    "\n",
    "# Create trainer object.\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=params.max_epochs,\n",
    "    logger=pl_logger,\n",
    "    #     log_every_n_steps=50,\n",
    "    #     progress_bar_refresh_rate=10,\n",
    "    terminate_on_nan=True,\n",
    ")\n",
    "\n",
    "# Many warnings are produced here, so it's better for my sanity (i.e., worse in every other\n",
    "# way) to just filter and ignore them...\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.simplefilter(\"ignore\")\n",
    "trainer.fit(model, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b909b30-f473-42a3-a840-e2755f91562d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85564e-b5de-4f73-ab08-8b86c3311950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = torch.utils.data.TensorDataset(torch.arange(0, 5, step=1 / 9).reshape(5, 3, 3))\n",
    "\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    torch.ones(len(ds)), num_samples=3, replacement=False\n",
    ")\n",
    "loader = torch.utils.data.DataLoader(ds, sampler=sampler)\n",
    "\n",
    "print(len(ds), len(loader))\n",
    "\n",
    "for epoch in range(3):\n",
    "    i = 0\n",
    "    print(epoch)\n",
    "    for el in loader:\n",
    "        print(i, el)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff218dee-92a0-4e4b-8187-39f7a2435787",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskFilteredPatchDataset3d(torch.utils.data.Dataset):\n",
    "    def __init__(self, img: torch.Tensor, mask: torch.Tensor, patch_size: tuple):\n",
    "        \"\"\"Dataset of static patches where the center voxels lies in the foreground/mask.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img : torch.Tensor\n",
    "            Source image, of shape [C x H x W x D]\n",
    "        mask : torch.Tensor\n",
    "            Mask that corresponds to the target locations in the img, of shape [H x W x D]\n",
    "        patch_size : tuple\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.patch_size = monai.utils.misc.ensure_tuple_rep(patch_size, 3)\n",
    "        self.img = img\n",
    "\n",
    "        if mask.ndim == 4:\n",
    "            mask = mask[0]\n",
    "        if self.img.ndim == 3:\n",
    "            self.img = self.img[None, ...]\n",
    "\n",
    "        patch_centers = torch.stack(torch.where(mask))\n",
    "        for i_dim, patch_dim_size in enumerate(self.patch_size):\n",
    "            offset_lower = int(np.floor(patch_dim_size / 2))\n",
    "            offset_upper = int(np.ceil(patch_dim_size / 2))\n",
    "            patch_centers = patch_centers[\n",
    "                :,\n",
    "                (patch_centers[i_dim] >= offset_lower)\n",
    "                & (patch_centers[i_dim] <= img.shape[i_dim + 1] - offset_upper),\n",
    "            ]\n",
    "\n",
    "        self.patch_starts = (\n",
    "            patch_centers\n",
    "            - torch.floor(torch.as_tensor(self.patch_size)[:, None] / 2).int()\n",
    "        ).T\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.patch_starts)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            patch_start = self.patch_starts[index]\n",
    "            patch = self.img[\n",
    "                :,\n",
    "                patch_start[0] : patch_start[0] + self.patch_size[0],\n",
    "                patch_start[1] : patch_start[1] + self.patch_size[1],\n",
    "                patch_start[2] : patch_start[2] + self.patch_size[2],\n",
    "            ]\n",
    "            return patch\n",
    "\n",
    "\n",
    "class ConcatDatasetBalancedRandomSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, datasets, max_samples_per_dataset, generator=None):\n",
    "        \"\"\"Sampler that draws a given number of samples from each dataset.\n",
    "\n",
    "        datasets: List[torch.utils.data.Dataset]\n",
    "\n",
    "        max_samples_per_dataset: int or List[int]\n",
    "            Give a single integer to make sample amounts the same for all datasets, or\n",
    "            a list of integers with length equal to the number of datasets to specify\n",
    "            a sample number particular to each dataset. If a dataset is smaller than\n",
    "            the requested number of samples, the entire length of the dataset will be\n",
    "            used instead. Each Dataset *must* be a Map-style Dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds_lens = [len(ds) for ds in datasets]\n",
    "        if (\n",
    "            np.isscalar(max_samples_per_dataset)\n",
    "            and int(max_samples_per_dataset) == max_samples_per_dataset\n",
    "        ):\n",
    "            self.sample_sizes = [\n",
    "                max_samples_per_dataset,\n",
    "            ] * len(self.ds_lens)\n",
    "        else:\n",
    "            self.sample_sizes = max_samples_per_dataset\n",
    "            if len(self.sample_sizes) != len(self.ds_lens):\n",
    "                raise ValueError(\n",
    "                    \"Must request sample sizes with length equal to the number of datasets\"\n",
    "                )\n",
    "        # Make sure we don't assign more samples to a dataset than there are elements\n",
    "        # in that dataset.\n",
    "        self.sample_sizes = list(map(min, zip(self.ds_lens, self.sample_sizes)))\n",
    "        cum_lens = list(itertools.accumulate(self.ds_lens))\n",
    "        self.start_idx = [\n",
    "            0,\n",
    "        ] + cum_lens[:-1]\n",
    "        self._total_samples = sum(self.sample_sizes)\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        samples = list()\n",
    "        for sample_size, len_i, i_start in zip(\n",
    "            self.sample_sizes, self.ds_lens, self.start_idx\n",
    "        ):\n",
    "            idx_i = (\n",
    "                i_start\n",
    "                + torch.randperm(len_i, generator=self.generator)[:sample_size]\n",
    "            )\n",
    "            samples.extend(idx_i.tolist())\n",
    "        \n",
    "        return (samples[i] for i in torch.randperm(len(samples), generator=self.generator))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc4c87-77c2-4afe-bad4-83cf24898da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_ds = [\n",
    "    torch.utils.data.TensorDataset(torch.randint(0, 10, (4, 4, 4))),\n",
    "    torch.utils.data.TensorDataset(torch.randint(0, 10, (4, 4, 4))*10),\n",
    "#     torch.utils.data.TensorDataset(torch.randint(0, 10, (2, 4, 4))*100),\n",
    "]\n",
    "ds = torch.utils.data.ConcatDataset(cat_ds)\n",
    "sampler = ConcatDatasetBalancedRandomSampler(cat_ds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c9e959-0cfd-4471-a23d-371bebc75cd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx in sampler:\n",
    "    print(idx)\n",
    "    print(ds[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211b0a49-70b4-4d81-b75a-64472ddac498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
