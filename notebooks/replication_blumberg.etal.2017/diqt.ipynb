{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain in the Net\n",
    "Replication of *Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images*\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source work:\n",
    "`S. B. Blumberg, R. Tanno, I. Kokkinos, and D. C. Alexander, “Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Cham, 2018, pp. 118–125, doi: 10.1007/978-3-030-00928-1_14.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import pdb\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Generator\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import nilearn.plotting\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import torchvision\n",
    "from natsort import natsorted\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    src_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    src_location = str(Path(\"../../\").resolve())\n",
    "sys.path.append(src_location)\n",
    "import src as pitn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-04-01T14:52:07.477953+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-52-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 1dd50c3b9ef4c348d426c908d9ef77485e3c3ed3\n",
      "\n",
      "sys              : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "torchvision      : 0.2.2\n",
      "nibabel          : 3.2.1\n",
      "ants             : 0.2.7\n",
      "dipy             : 1.4.0\n",
      "matplotlib       : 3.4.1\n",
      "pandas           : 1.2.3\n",
      "skimage          : 0.18.1\n",
      "torchio          : 0.18.31\n",
      "seaborn          : 0.11.1\n",
      "numpy            : 1.20.2\n",
      "nilearn          : 0.7.1\n",
      "torch            : 1.8.1\n",
      "natsort          : 7.1.1\n",
      "pytorch_lightning: 1.2.6\n",
      "\n",
      "CUDA Version:  11.1\n"
     ]
    }
   ],
   "source": [
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "print(\"CUDA Version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Function & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For more clearly designating the return values of a reader function given to\n",
    "# the `torchio.Image` object.\n",
    "ReaderOutput = collections.namedtuple(\"ReaderOutput\", [\"dwi\", \"affine\"])\n",
    "\n",
    "\n",
    "def nifti_reader(\n",
    "    f_dwi,\n",
    ") -> ReaderOutput:\n",
    "    \"\"\"Reader that reads in NIFTI files quickly.\n",
    "\n",
    "    Meant for use with the `torchio.Image` object and its sub-classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load with nibabel first to get the correct affine matrix. See\n",
    "    # <https://github.com/ANTsX/ANTsPy/issues/52> for why I don't trust antspy for this.\n",
    "    # This does not require loading the entire NIFTI file into memory.\n",
    "    affine = nib.load(f_dwi).affine.copy()\n",
    "    affine = affine.astype(np.float32)\n",
    "    print(\"Loading NIFTI image\", flush=True)\n",
    "    # Load entire image with antspy, then slice and (possibly) downsample that full image.\n",
    "    # A float32 is the smallest representation that doesn't lose data.\n",
    "    dwi = ants.image_read(str(f_dwi), pixeltype=\"float\")\n",
    "    print(\"\\tLoaded NIFTI image\", flush=True)\n",
    "\n",
    "    # Use `torch.tensor()` to explicitly copy the numpy array. May have issues with\n",
    "    # underlying memory getting garbage collected when using `torch.from_numpy`.\n",
    "    # <https://pytorch.org/docs/1.8.0/generated/torch.tensor.html#torch.tensor>\n",
    "    return ReaderOutput(dwi=torch.tensor(dwi.view()), affine=torch.tensor(affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torchio.Transform functions/objects.\n",
    "\n",
    "\n",
    "class BValSelectionTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Sub-selects scans that are within a certain range of bvals.\n",
    "\n",
    "    Expects:\n",
    "    - volumes in canonical (RAS+) format with *channels first.*\n",
    "    - bvecs to be of shape (N, 3), with N being the number of scans/bvals.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bval_range: tuple, bval_key, bvec_key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_range = bval_range\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(\"Selecting with bvals\", flush=True)\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            bvals = img[self.bval_key]\n",
    "            scans_to_keep = (self.bval_range[0] <= bvals) & (\n",
    "                bvals <= self.bval_range[-1]\n",
    "            )\n",
    "            img[self.bvec_key] = img[self.bvec_key][scans_to_keep, :]\n",
    "            img.set_data(img.data[scans_to_keep, ...])\n",
    "            img[self.bval_key] = img[self.bval_key][scans_to_keep]\n",
    "        print(\"\\tSelected\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class MeanDownsampleTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Mean downsampling transformation.\n",
    "\n",
    "    Expects volumes in canonical (RAS+) format with *channels first.*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsample_factor: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(\"Downsampling\", flush=True)\n",
    "        # Get reference to Image objects that have been included for transformation.\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            img[\"downsample_factor\"] = self.downsample_factor\n",
    "            if self.downsample_factor == 1:\n",
    "                continue\n",
    "            # Determine dimension-specific downsample factors\n",
    "            img_ndarray = img.data.numpy()\n",
    "            dim_factors = np.asarray(\n",
    "                [\n",
    "                    self.downsample_factor,\n",
    "                ]\n",
    "                * img_ndarray.ndim\n",
    "            )\n",
    "            # Only spatial dimensions should be downsampled.\n",
    "            if img.data.ndim > 3:\n",
    "                # Don't downsample the channels\n",
    "                dim_factors[0] = 1\n",
    "                # Or anything else outside of spatial dims.\n",
    "                dim_factors[3:] = 1\n",
    "\n",
    "            downsample_vol = skimage.transform.downscale_local_mean(\n",
    "                img_ndarray, factors=tuple(dim_factors), cval=0\n",
    "            )\n",
    "            downsample_vol = torch.from_numpy(\n",
    "                downsample_vol.astype(img_ndarray.dtype)\n",
    "            ).to(img.data.dtype)\n",
    "\n",
    "            img.set_data(downsample_vol)\n",
    "            scaled_affine = img.affine.copy()\n",
    "            # Scale the XYZ coordinates on the main diagonal.\n",
    "            scaled_affine[(0, 1, 2), (0, 1, 2)] = (\n",
    "                scaled_affine[(0, 1, 2), (0, 1, 2)] * self.downsample_factor\n",
    "            )\n",
    "            img.affine = scaled_affine\n",
    "        print(\"\\tDownsampled\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class FitDTITransform(torchio.SpatialTransform, torchio.IntensityTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bval_key,\n",
    "        bvec_key,\n",
    "        mask_img_key=None,\n",
    "        fit_method=\"WLS\",\n",
    "        tensor_model_kwargs=dict(),\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "        self.mask_img_key = mask_img_key\n",
    "        self.fit_method = fit_method\n",
    "        self.tensor_model_kwargs = tensor_model_kwargs\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "\n",
    "        print(\"Fitting to DTI\", flush=True)\n",
    "        mask_img = subject[self.mask_img_key] if self.mask_img_key is not None else None\n",
    "        for img in self.get_images(subject):\n",
    "\n",
    "            gradient_table = dipy.core.gradients.gradient_table_from_bvals_bvecs(\n",
    "                bvals=img[self.bval_key],\n",
    "                bvecs=img[self.bvec_key],\n",
    "            )\n",
    "\n",
    "            tensor_model = dipy.reconst.dti.TensorModel(\n",
    "                gradient_table, fit_method=self.fit_method, **self.tensor_model_kwargs\n",
    "            )\n",
    "            # dipy does not like the channels being first, apparently.\n",
    "            if mask_img is not None:\n",
    "                dti = tensor_model.fit(\n",
    "                    np.moveaxis(img.numpy(), 0, -1),\n",
    "                    mask=mask_img.numpy().squeeze().astype(bool),\n",
    "                )\n",
    "            else:\n",
    "                dti = tensor_model.fit(np.moveaxis(img.numpy(), 0, -1))\n",
    "\n",
    "            # Pull only the lower-triangular part of the DTI (the non-symmetric coefficients.)\n",
    "            img.set_data(\n",
    "                torch.from_numpy(dti.lower_triangular().astype(np.float32)).to(img.data)\n",
    "            )\n",
    "        print(\"\\tFitted DTI model\", flush=True)\n",
    "\n",
    "        return subject\n",
    "\n",
    "\n",
    "class RenameImageTransform(torchio.Transform):\n",
    "    def __init__(self, name_mapping: dict, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.name_mapping = name_mapping\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        for old_name, new_name in self.name_mapping.items():\n",
    "            tmp = subject[old_name]\n",
    "            subject.remove_image(old_name)\n",
    "            subject.add_image(tmp, new_name)\n",
    "\n",
    "        return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions for sampling and loading patches from volumes of different resolutions in a `pytorch.utils.data.DataLoader`.\n",
    "\n",
    "# Return type wrapper\n",
    "MultiresSample = collections.namedtuple(\"MultiresSample\", (\"full_res\", \"low_res\"))\n",
    "\n",
    "# Custom sampler for sampling multiple volumes of different resolutions.\n",
    "class MultiresSampler(torchio.LabelSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_img_key,\n",
    "        low_res_key,\n",
    "        downsample_factor_key,\n",
    "        label_name,\n",
    "        subj_keys_to_copy=tuple(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(label_name=label_name, **kwargs)\n",
    "        self.source_img_key = source_img_key\n",
    "        self.low_res_key = low_res_key\n",
    "        self.downsample_factor_key = downsample_factor_key\n",
    "        self.subj_keys_to_copy = subj_keys_to_copy\n",
    "\n",
    "    def __call__(\n",
    "        self, subject: torchio.Subject, num_patches=None\n",
    "    ) -> Generator[torchio.Subject, None, None]:\n",
    "\n",
    "        # Setup copied from the `torchio.WeightedSampler.__call__` function definition.\n",
    "        subject.check_consistent_space()\n",
    "        if np.any(self.patch_size > subject.spatial_shape):\n",
    "            message = (\n",
    "                f\"Patch size {tuple(self.patch_size)} cannot be\"\n",
    "                f\" larger than image size {tuple(subject.spatial_shape)}\"\n",
    "            )\n",
    "            raise RuntimeError(message)\n",
    "        probability_map = self.get_probability_map(subject)\n",
    "        probability_map = self.process_probability_map(probability_map, subject)\n",
    "        cdf = self.get_cumulative_distribution_function(probability_map)\n",
    "\n",
    "        patches_left = num_patches if num_patches is not None else True\n",
    "        while patches_left:\n",
    "            subj_fields_transfer = dict(\n",
    "                ((k, subject[k]) for k in self.subj_keys_to_copy)\n",
    "            )\n",
    "            # Create a new subject that only contains patches.\n",
    "            patch_subj = torchio.Subject(**subj_fields_transfer)\n",
    "            # Sample an index from the full-res image.\n",
    "            source_index_ini = self.get_random_index_ini(probability_map, cdf)\n",
    "            # Include the index in the subject.\n",
    "            patch_subj[\"index_ini\"] = np.array(source_index_ini).astype(int)\n",
    "\n",
    "            # Add the patch from the full-res image into the subject.\n",
    "            patch_subj.add_image(\n",
    "                self.extract_subj_patch(\n",
    "                    subject,\n",
    "                    img_key=self.source_img_key,\n",
    "                    index_ini=source_index_ini,\n",
    "                    patch_size=self.patch_size,\n",
    "                ),\n",
    "                self.img_key,\n",
    "            )\n",
    "\n",
    "            # Crop low-res image and add to the subject.\n",
    "            lr_index_ini = tuple(\n",
    "                np.array(source_index_ini).astype(int)\n",
    "                // subject[self.low_res_key][self.downsample_factor_key]\n",
    "            )\n",
    "            patch_subj.add_image(\n",
    "                self.extract_subj_patch(\n",
    "                    subject,\n",
    "                    img_key=self.low_res_key,\n",
    "                    index_ini=lr_index_ini,\n",
    "                    patch_size=self.patch_size,\n",
    "                ),\n",
    "                self.low_res_key,\n",
    "            )\n",
    "\n",
    "            # Return the new patch subject.\n",
    "            yield patch_subj\n",
    "            if num_patches is not None:\n",
    "                patches_left -= 1\n",
    "\n",
    "    @classmethod\n",
    "    def extract_subj_patch(\n",
    "        cls, subject: torchio.Subject, img_key, index_ini, patch_size\n",
    "    ) -> torchio.Image:\n",
    "\n",
    "        img = subject[img_key]\n",
    "        cropping = cls.get_crop_transform(subject, index_ini, patch_size).cropping\n",
    "        crop_transform = torchio.transforms.Crop(cropping, copy=False)\n",
    "        return crop_transform(img)\n",
    "\n",
    "\n",
    "# Collate function for the DataLoader to combine multiple samples.\n",
    "def collate_subj(samples, full_res_key: str, low_res_key: str):\n",
    "    full_res_stack = torch.stack([subj[full_res_key] for subj in samples])\n",
    "    low_res_stack = torch.stack([subj[low_res_key] for subj in samples])\n",
    "\n",
    "    return MultiresSample(full_res=full_res_stack, low_res=low_res_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downsample_factor = 2\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 1500)\n",
    "dti_fit_method = \"WLS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-7ad468fcbfed>:25: UserWarning:\n",
      "\n",
      "WARNING: Sub-selecting participants for dev and debugging. Subj IDs selected: ['224022', '141422', '135528']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{135528: PosixPath('/mnt/storage/data/pitn/hcp/135528/T1w/Diffusion'),\n",
       " 141422: PosixPath('/mnt/storage/data/pitn/hcp/141422/T1w/Diffusion'),\n",
       " 224022: PosixPath('/mnt/storage/data/pitn/hcp/224022/T1w/Diffusion')}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, 3)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90 scans are taken from the $b=1000 \\ s/mm^2$. However, the $b=0$ shells are still required for fitting the diffusion tensors (DTI's), so those will need to be kept, too.\n",
    "\n",
    "To find those, sub-select with the $0 < bvals < 1500$, or roughly thereabout. A b-val of $995$ or $1005$ still counts as a b=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NIFTI image\n",
      "\tLoaded NIFTI image\n",
      "Selecting with bvals\n",
      "\tSelected\n",
      "Downsampling\n",
      "\tDownsampled\n",
      "Fitting to DTI\n",
      "\tFitted DTI model\n",
      "Fitting to DTI\n",
      "\tFitted DTI model\n"
     ]
    }
   ],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    # grad = torchio.ScalarImage(subj_dir/\"grad_dev.nii.gz\")\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "\n",
    "    # The brain mask is binary.\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "\n",
    "    preproc_transforms = torchio.Compose(\n",
    "        [\n",
    "            torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "            BValSelectionTransform(\n",
    "                bval_range=bval_range,\n",
    "                bval_key=\"bvals\",\n",
    "                bvec_key=\"bvecs\",\n",
    "                include=\"dwi\",\n",
    "                copy=False,\n",
    "            ),\n",
    "            MeanDownsampleTransform(\n",
    "                downsample_factor,\n",
    "                include=(\"dwi\", \"brain_mask\"),\n",
    "                keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform(\n",
    "                {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"fr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"fr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"lr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"lr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform({\"fr_dwi\": \"fr_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    subj_data[subj_id] = preproc_transforms(subject_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_dataset = torchio.SubjectsDataset(list(subj_data.values()), load_getitem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "batch_size = 16\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Input patch parameters\n",
    "h_in = 11\n",
    "w_in = 11\n",
    "d_in = 11\n",
    "input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "# Output patch parameters\n",
    "h_out = downsample_factor * h_in\n",
    "w_out = downsample_factor * w_in\n",
    "d_out = downsample_factor * d_in\n",
    "unshuffled_channels_out = channels * downsample_factor ** 3\n",
    "# Output before shuffling\n",
    "unshuffled_output_patch_shape = (unshuffled_channels_out, h_in, w_in, d_in)\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Patch-Based Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data train/validation/test split\n",
    "test_percent = 0.2\n",
    "train_percent = 1 - test_percent\n",
    "# val_percent = 0.1\n",
    "\n",
    "num_subjs = len(subj_dataset)\n",
    "num_test_subjs = int(np.ceil(num_subjs * test_percent))\n",
    "num_train_subjs = num_subjs - num_test_subjs\n",
    "subj_list = subj_dataset.dry_iter()\n",
    "# Randomly shuffle the list of subjects, then choose the first `num_test_subjs` subjects\n",
    "# for testing.\n",
    "random.shuffle(subj_list)\n",
    "test_dataset = torchio.SubjectsDataset(subj_list[:num_test_subjs], load_getitem=False)\n",
    "# Choose the remaining for training/validation.\n",
    "subj_list = subj_list[num_test_subjs:]\n",
    "train_dataset = torchio.SubjectsDataset(subj_list, load_getitem=False)\n",
    "torchio.LabelSampler()\n",
    "# Training patch sampler, random across all patches of all volumes.\n",
    "train_sampler = MultiresSampler(\n",
    "    source_img_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    downsample_factor_key=\"downsample_factor\",\n",
    "    label_name=\"brain_mask\",\n",
    "    patch_size=(h_in, w_in, d_in),\n",
    "    label_probabilities={0: 0, 1: 1},\n",
    ")\n",
    "# Set up a torchio.Queue to act as a sampler proxy for the torch DataLoader\n",
    "train_queue = torchio.Queue(\n",
    "    train_dataset,\n",
    "    max_length=5 * batch_size,\n",
    "    samples_per_volume=32,\n",
    "    sampler=train_sampler,\n",
    "    shuffle_patches=True,\n",
    "    shuffle_subjects=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "# Create partial function to collect list of samples and form a tuple of tensors.\n",
    "collate_fn = functools.partial(\n",
    "    collate_subj, full_res_key=\"fr_dti\", low_res_key=\"lr_dti\"\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_queue,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# Test samplers must be dynamically created during testing.\n",
    "\n",
    "print(\"Test subject(s) IDs: \", [s.subj_id for s in test_dataset.dry_iter()])\n",
    "print(\"Training subject(s) IDs: \", [s.subj_id for s in train_dataset.dry_iter()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define pytorch-lightning module.\n",
    "class DIQTSystem(pl.LightningModule):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels_in = channels_in\n",
    "        self.channels_out = channels_out\n",
    "        # Parameters\n",
    "        # Network parameters\n",
    "        self.num_revnet_layers = 4\n",
    "        self.net = pitn.models.ESPCN_RN(\n",
    "            no_RevNet_layers=self.num_revnet_layers,\n",
    "            no_chans_in=self.channels_in,\n",
    "            no_chans_out=self.channels_out,\n",
    "            memory_efficient=True,\n",
    "        )\n",
    "\n",
    "        ## Training parameters\n",
    "        self.lr = 10e-4\n",
    "        mse_loss = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        self.loss_fn = lambda y_hat, y: torch.sqrt(mse_loss(y_hat, y))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        breakpoint()\n",
    "        opt = self.optimizers()\n",
    "        x, y = batch\n",
    "        y_pred_shuffled = self.net(x)\n",
    "        y_pred = y_pred_shuffled\n",
    "        # y_pred = deshuffle(y_pred_shuffled)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "\n",
    "        # Perform manual backprop\n",
    "        loss.backward()\n",
    "        self.net.backward(y_pred, y_pred.grad)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    #     def validation_step(self, batch, batch_idx):\n",
    "    #         pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DIQTSystem(channels_in=channels, channels_out=unshuffled_channels_out)\n",
    "# Create trainer object. Note: `automatic_optimization` needs to be set to `False` when\n",
    "# manually performing backprop. See\n",
    "# <https://colab.research.google.com/drive/1nGtvBFirIvtNQdppe2xBes6aJnZMjvl8?usp=sharing>\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1, max_epochs=1, automatic_optimization=False, progress_bar_refresh_rate=10\n",
    ")\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=1, max_epochs=max_epochs, automatic_optimization=False, progress_bar_refresh_rate=10\n",
    "# )\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
