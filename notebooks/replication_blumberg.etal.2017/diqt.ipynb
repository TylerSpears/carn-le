{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain in the Net\n",
    "Replication of *Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images*\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source work:\n",
    "`S. B. Blumberg, R. Tanno, I. Kokkinos, and D. C. Alexander, “Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Cham, 2018, pp. 118–125, doi: 10.1007/978-3-030-00928-1_14.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:09.209715Z",
     "iopub.status.busy": "2021-04-06T19:08:09.209051Z",
     "iopub.status.idle": "2021-04-06T19:08:12.472911Z",
     "shell.execute_reply": "2021-04-06T19:08:12.472237Z",
     "shell.execute_reply.started": "2021-04-06T19:08:09.209553Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/pitn/lib/python3.8/site-packages/nilearn/datasets/__init__.py:87: FutureWarning:\n",
      "\n",
      "Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import collections\n",
    "import functools\n",
    "import io\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Generator\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import natsort\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import nilearn.plotting\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchio\n",
    "import torchvision\n",
    "import einops\n",
    "from natsort import natsorted\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True)\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:12.474632Z",
     "iopub.status.busy": "2021-04-06T19:08:12.474384Z",
     "iopub.status.idle": "2021-04-06T19:08:13.915958Z",
     "shell.execute_reply": "2021-04-06T19:08:13.915184Z",
     "shell.execute_reply.started": "2021-04-06T19:08:12.474601Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:13.918286Z",
     "iopub.status.busy": "2021-04-06T19:08:13.918033Z",
     "iopub.status.idle": "2021-04-06T19:08:13.924271Z",
     "shell.execute_reply": "2021-04-06T19:08:13.923734Z",
     "shell.execute_reply.started": "2021-04-06T19:08:13.918250Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    src_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    src_location = str(Path(\"../../\").resolve())\n",
    "sys.path.append(src_location)\n",
    "import src as pitn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:13.926041Z",
     "iopub.status.busy": "2021-04-06T19:08:13.925816Z",
     "iopub.status.idle": "2021-04-06T19:08:13.962447Z",
     "shell.execute_reply": "2021-04-06T19:08:13.961600Z",
     "shell.execute_reply.started": "2021-04-06T19:08:13.926012Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# torch setup\n",
    "\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:13.964142Z",
     "iopub.status.busy": "2021-04-06T19:08:13.963745Z",
     "iopub.status.idle": "2021-04-06T19:08:14.114879Z",
     "shell.execute_reply": "2021-04-06T19:08:14.113974Z",
     "shell.execute_reply.started": "2021-04-06T19:08:13.964108Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Tyler Spears\n",
      "\n",
      "Last updated: 2021-04-06T19:08:13.974239+00:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.8\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : GCC 7.3.0\n",
      "OS          : Linux\n",
      "Release     : 5.4.0-52-generic\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 8\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: 28be1a6bbd33ab92c73f8ba56063373693c7516f\n",
      "\n",
      "numpy            : 1.20.2\n",
      "dipy             : 1.4.0\n",
      "nilearn          : 0.7.1\n",
      "matplotlib       : 3.4.1\n",
      "skimage          : 0.18.1\n",
      "einops           : 0.3.0\n",
      "pytorch_lightning: 1.2.6\n",
      "ants             : 0.2.7\n",
      "torchio          : 0.18.31\n",
      "torch            : 1.8.1\n",
      "seaborn          : 0.11.1\n",
      "pandas           : 1.2.3\n",
      "nibabel          : 3.2.1\n",
      "natsort          : 7.1.1\n",
      "torchvision      : 0.2.2\n",
      "sys              : 3.8.8 (default, Feb 24 2021, 21:46:12) \n",
      "[GCC 7.3.0]\n",
      "\n",
      "CUDA Version:  11.1\n"
     ]
    }
   ],
   "source": [
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "print(\"CUDA Version: \", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:14.116562Z",
     "iopub.status.busy": "2021-04-06T19:08:14.116318Z",
     "iopub.status.idle": "2021-04-06T19:08:14.121386Z",
     "shell.execute_reply": "2021-04-06T19:08:14.120749Z",
     "shell.execute_reply.started": "2021-04-06T19:08:14.116528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Function & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:08:14.122791Z",
     "iopub.status.busy": "2021-04-06T19:08:14.122563Z",
     "iopub.status.idle": "2021-04-06T19:08:14.128893Z",
     "shell.execute_reply": "2021-04-06T19:08:14.128295Z",
     "shell.execute_reply.started": "2021-04-06T19:08:14.122761Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For more clearly designating the return values of a reader function given to\n",
    "# the `torchio.Image` object.\n",
    "ReaderOutput = collections.namedtuple(\"ReaderOutput\", [\"dwi\", \"affine\"])\n",
    "\n",
    "\n",
    "def nifti_reader(\n",
    "    f_dwi,\n",
    ") -> ReaderOutput:\n",
    "    \"\"\"Reader that reads in NIFTI files quickly.\n",
    "\n",
    "    Meant for use with the `torchio.Image` object and its sub-classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load with nibabel first to get the correct affine matrix. See\n",
    "    # <https://github.com/ANTsX/ANTsPy/issues/52> for why I don't trust antspy for this.\n",
    "    # This does not require loading the entire NIFTI file into memory.\n",
    "    affine = nib.load(f_dwi).affine.copy()\n",
    "    affine = affine.astype(np.float32)\n",
    "    print(\"Loading NIFTI image\", flush=True)\n",
    "    # Load entire image with antspy, then slice and (possibly) downsample that full image.\n",
    "    # A float32 is the smallest representation that doesn't lose data.\n",
    "    dwi = ants.image_read(str(f_dwi), pixeltype=\"float\")\n",
    "    print(\"\\tLoaded NIFTI image\", flush=True)\n",
    "\n",
    "    # Use `torch.tensor()` to explicitly copy the numpy array. May have issues with\n",
    "    # underlying memory getting garbage collected when using `torch.from_numpy`.\n",
    "    # <https://pytorch.org/docs/1.8.0/generated/torch.tensor.html#torch.tensor>\n",
    "    return ReaderOutput(dwi=torch.tensor(dwi.view()), affine=torch.tensor(affine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:11:24.965174Z",
     "iopub.status.busy": "2021-04-06T19:11:24.965012Z",
     "iopub.status.idle": "2021-04-06T19:11:25.000444Z",
     "shell.execute_reply": "2021-04-06T19:11:24.999843Z",
     "shell.execute_reply.started": "2021-04-06T19:11:24.965154Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torchio.Transform functions/objects.\n",
    "\n",
    "\n",
    "class BValSelectionTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Sub-selects scans that are within a certain range of bvals.\n",
    "\n",
    "    Expects:\n",
    "    - volumes in canonical (RAS+) format with *channels first.*\n",
    "    - bvecs to be of shape (N, 3), with N being the number of scans/bvals.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bval_range: tuple, bval_key, bvec_key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_range = bval_range\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(\"Selecting with bvals\", flush=True)\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            bvals = img[self.bval_key]\n",
    "            scans_to_keep = (self.bval_range[0] <= bvals) & (\n",
    "                bvals <= self.bval_range[-1]\n",
    "            )\n",
    "            img[self.bvec_key] = img[self.bvec_key][scans_to_keep, :]\n",
    "            img.set_data(img.data[scans_to_keep, ...])\n",
    "            img[self.bval_key] = img[self.bval_key][scans_to_keep]\n",
    "        print(\"\\tSelected\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class MeanDownsampleTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Mean downsampling transformation.\n",
    "\n",
    "    Expects volumes in canonical (RAS+) format with *channels first.*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsample_factor: int, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(\"Downsampling\", flush=True)\n",
    "        # Get reference to Image objects that have been included for transformation.\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            img[\"downsample_factor\"] = self.downsample_factor\n",
    "            if self.downsample_factor == 1:\n",
    "                continue\n",
    "            # Determine dimension-specific downsample factors\n",
    "            img_ndarray = img.data.numpy()\n",
    "            dim_factors = np.asarray(\n",
    "                [\n",
    "                    self.downsample_factor,\n",
    "                ]\n",
    "                * img_ndarray.ndim\n",
    "            )\n",
    "            # Only spatial dimensions should be downsampled.\n",
    "            if img.data.ndim > 3:\n",
    "                # Don't downsample the channels\n",
    "                dim_factors[0] = 1\n",
    "                # Or anything else outside of spatial dims.\n",
    "                dim_factors[3:] = 1\n",
    "\n",
    "            downsample_vol = skimage.transform.downscale_local_mean(\n",
    "                img_ndarray, factors=tuple(dim_factors), cval=0\n",
    "            )\n",
    "            downsample_vol = torch.from_numpy(\n",
    "                downsample_vol.astype(img_ndarray.dtype)\n",
    "            ).to(img.data.dtype)\n",
    "            # Pad with a small number of 0's to account for sampling at the edge of the\n",
    "            # full-res image.\n",
    "            # Don't pad dims that were not scaled.\n",
    "            padding = ((dim_factors - 1).astype(bool)).astype(int).tolist()\n",
    "            downsample_vol = torchvision.transforms.functional.pad(\n",
    "                downsample_vol,\n",
    "                padding=padding,\n",
    "                fill=0,\n",
    "            )\n",
    "            img.set_data(downsample_vol)\n",
    "            scaled_affine = img.affine.copy()\n",
    "            # Scale the XYZ coordinates on the main diagonal.\n",
    "            scaled_affine[(0, 1, 2), (0, 1, 2)] = (\n",
    "                scaled_affine[(0, 1, 2), (0, 1, 2)] * self.downsample_factor\n",
    "            )\n",
    "            img.affine = scaled_affine\n",
    "        print(\"\\tDownsampled\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class FitDTITransform(torchio.SpatialTransform, torchio.IntensityTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bval_key,\n",
    "        bvec_key,\n",
    "        mask_img_key=None,\n",
    "        fit_method=\"WLS\",\n",
    "        tensor_model_kwargs=dict(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "        self.mask_img_key = mask_img_key\n",
    "        self.fit_method = fit_method\n",
    "        self.tensor_model_kwargs = tensor_model_kwargs\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "\n",
    "        print(\"Fitting to DTI\", flush=True)\n",
    "        mask_img = subject[self.mask_img_key] if self.mask_img_key is not None else None\n",
    "        for img in self.get_images(subject):\n",
    "\n",
    "            gradient_table = dipy.core.gradients.gradient_table_from_bvals_bvecs(\n",
    "                bvals=img[self.bval_key],\n",
    "                bvecs=img[self.bvec_key],\n",
    "            )\n",
    "\n",
    "            tensor_model = dipy.reconst.dti.TensorModel(\n",
    "                gradient_table, fit_method=self.fit_method, **self.tensor_model_kwargs\n",
    "            )\n",
    "            print(f\"\\tDWI shape: {img.data.shape}\", flush=True)\n",
    "            # dipy does not like the channels being first, apparently.\n",
    "            if mask_img is not None:\n",
    "                dti = tensor_model.fit(\n",
    "                    np.moveaxis(img.numpy(), 0, -1),\n",
    "                    mask=mask_img.numpy().squeeze().astype(bool),\n",
    "                )\n",
    "            else:\n",
    "                dti = tensor_model.fit(np.moveaxis(img.numpy(), 0, -1))\n",
    "\n",
    "            # Pull only the lower-triangular part of the DTI (the non-symmetric\n",
    "            # coefficients.)\n",
    "            # Do it all in one line to minimize the time that the DTI's have to be\n",
    "            # duplicated in memory.\n",
    "            img.set_data(\n",
    "                torch.from_numpy(\n",
    "                    np.moveaxis(dti.lower_triangular().astype(np.float32), -1, 0)\n",
    "                ).to(img.data)\n",
    "            )\n",
    "            print(f\"\\tDTI shape: {img.shape}\", flush=True)\n",
    "        print(f\"\\tFitted DTI model: {img.data.shape}\", flush=True)\n",
    "\n",
    "        return subject\n",
    "\n",
    "\n",
    "class RenameImageTransform(torchio.Transform):\n",
    "    def __init__(self, name_mapping: dict, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.name_mapping = name_mapping\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        for old_name, new_name in self.name_mapping.items():\n",
    "            tmp = subject[old_name]\n",
    "            subject.remove_image(old_name)\n",
    "            subject.add_image(tmp, new_name)\n",
    "        subject.update_attributes()\n",
    "        return subject\n",
    "\n",
    "\n",
    "class ImageToDictTransform(torchio.Transform):\n",
    "    \"\"\"Convert a Subject Image to a simple dict item.\n",
    "\n",
    "    Removes the `include`ed keys from calculation of the Subject's properties, such as\n",
    "    `spatial_shape`, `spacing`, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        for img_name in self.include:\n",
    "\n",
    "            img_dict = dict(subject[img_name])\n",
    "            subject.remove_image(img_name)\n",
    "\n",
    "            subject[img_name] = img_dict\n",
    "\n",
    "        subject.update_attributes()\n",
    "        return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:11:25.111242Z",
     "iopub.status.busy": "2021-04-06T19:11:25.110758Z",
     "iopub.status.idle": "2021-04-06T19:11:25.137025Z",
     "shell.execute_reply": "2021-04-06T19:11:25.136423Z",
     "shell.execute_reply.started": "2021-04-06T19:11:25.111184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitions for sampling and loading patches from volumes of different resolutions in a `pytorch.utils.data.DataLoader`.\n",
    "\n",
    "# Return type wrapper\n",
    "MultiresSample = collections.namedtuple(\"MultiresSample\", (\"low_res\", \"full_res\"))\n",
    "\n",
    "# Custom sampler for sampling multiple volumes of different resolutions.\n",
    "class MultiresSampler(torchio.LabelSampler):\n",
    "    \"\"\"\n",
    "\n",
    "    source_img_key: Key to the Subject that will fetch the source (a.k.a., the high-res\n",
    "        or full-res) image.\n",
    "\n",
    "    low_res_key: Key to the Subject that will fetch the low-res image. This image is\n",
    "        assumed to be a dictionary with a 'data' key.\n",
    "\n",
    "    downsample_factor_key: Key to the low-res image dict that gives the downsample\n",
    "        factor.\n",
    "\n",
    "    source_spatial_patch_size: 3-tuple of `(W, H, D)` that gives the spatial size of\n",
    "        patches drawn from the source image.\n",
    "\n",
    "    low_res_spatial_patch_size: 3-tuple of `(W, H, D)` that gives the spatial size of\n",
    "        patches drawn from the low-res image.\n",
    "\n",
    "    subj_keys_to_copy: Tuple of keys to copy from the Subject into the returned sample\n",
    "        patch(es).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_img_key,\n",
    "        low_res_key,\n",
    "        downsample_factor_key,\n",
    "        source_spatial_patch_size: tuple,\n",
    "        low_res_spatial_patch_size: tuple,\n",
    "        label_name,\n",
    "        subj_keys_to_copy=tuple(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            patch_size=source_spatial_patch_size, label_name=label_name, **kwargs\n",
    "        )\n",
    "        self.source_img_key = source_img_key\n",
    "        self.low_res_key = low_res_key\n",
    "        self.downsample_factor_key = downsample_factor_key\n",
    "        self.subj_keys_to_copy = subj_keys_to_copy\n",
    "        self.source_spatial_patch_size = source_spatial_patch_size\n",
    "        self.low_res_spatial_patch_size = low_res_spatial_patch_size\n",
    "\n",
    "    def __call__(\n",
    "        self, subject: torchio.Subject, num_patches=None\n",
    "    ) -> Generator[torchio.Subject, None, None]:\n",
    "\n",
    "        # Setup copied from the `torchio.WeightedSampler.__call__` function definition.\n",
    "        subject.check_consistent_space()\n",
    "        if np.any(self.patch_size > subject.spatial_shape):\n",
    "            message = (\n",
    "                f\"Patch size {tuple(self.patch_size)} cannot be\"\n",
    "                f\" larger than image size {tuple(subject.spatial_shape)}\"\n",
    "            )\n",
    "            raise RuntimeError(message)\n",
    "        probability_map = self.get_probability_map(subject)\n",
    "        probability_map = self.process_probability_map(probability_map, subject)\n",
    "        cdf = self.get_cumulative_distribution_function(probability_map)\n",
    "\n",
    "        patches_left = num_patches if num_patches is not None else True\n",
    "        while patches_left:\n",
    "            subj_fields_transfer = dict(\n",
    "                ((k, subject[k]) for k in self.subj_keys_to_copy)\n",
    "            )\n",
    "\n",
    "            # Sample an index from the full-res image.\n",
    "            source_index_ini = self.get_random_index_ini(probability_map, cdf)\n",
    "            # Create a new subject that only contains patches.\n",
    "            # Add the patch from the full-res image into the subject.\n",
    "            patch_subj = torchio.Subject(\n",
    "                **(\n",
    "                    dict(\n",
    "                        [\n",
    "                            (\n",
    "                                self.source_img_key,\n",
    "                                torchio.ScalarImage(\n",
    "                                    tensor=self.extract_patch(\n",
    "                                        subject[self.source_img_key].data,\n",
    "                                        img_spatial_shape=subject[\n",
    "                                            self.source_img_key\n",
    "                                        ].shape[1:],\n",
    "                                        index_ini=source_index_ini,\n",
    "                                        patch_size=self.source_spatial_patch_size,\n",
    "                                    ),\n",
    "                                    affine=subject[self.source_img_key].affine,\n",
    "                                ),\n",
    "                            ),\n",
    "                            *subj_fields_transfer.items(),\n",
    "                        ],\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "            #             breakpoint()\n",
    "            # Include the index in the subject.\n",
    "            patch_subj[\"index_ini\"] = np.array(source_index_ini).astype(int)\n",
    "            # Crop low-res image and add to the subject.\n",
    "            lr_index_ini = tuple(\n",
    "                np.array(source_index_ini).astype(int)\n",
    "                // subject[self.low_res_key][self.downsample_factor_key]\n",
    "            )\n",
    "\n",
    "            lr_patch = self.extract_patch(\n",
    "                subject[self.low_res_key][\"data\"],\n",
    "                img_spatial_shape=subject[self.low_res_key][\"data\"].shape[1:],\n",
    "                index_ini=lr_index_ini,\n",
    "                patch_size=self.low_res_spatial_patch_size,\n",
    "            )\n",
    "            if lr_patch.numel() == 0:\n",
    "                breakpoint()\n",
    "            # Add a dict to the subject patch, rather than a `torchio.Image`,\n",
    "            # because the fr and lr patch shapes will be different, and fail\n",
    "            # `torchio`'s shape consistency checks.)\n",
    "            lr_patch_dict = dict()\n",
    "            lr_patch_dict.update(subject[self.low_res_key])\n",
    "            lr_patch_dict.update({\"data\": lr_patch})\n",
    "\n",
    "            patch_subj[self.low_res_key] = lr_patch_dict\n",
    "            # Return the new patch subject.\n",
    "            yield patch_subj\n",
    "            if num_patches is not None:\n",
    "                patches_left -= 1\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_patch(img, img_spatial_shape, index_ini, patch_size) -> torchio.Image:\n",
    "\n",
    "        # Just take it straight from `torchio.transforms.Copy.apply_transform`\n",
    "        # and `torchio.sampler.Sampler`!\n",
    "\n",
    "        shape = np.array(img_spatial_shape, dtype=np.uint16)\n",
    "        index_ini = np.array(index_ini, dtype=np.uint16)\n",
    "        patch_size = np.array(patch_size, dtype=np.uint16)\n",
    "        index_fin = index_ini + patch_size\n",
    "\n",
    "        crop_ini = index_ini.tolist()\n",
    "        crop_fin = (shape - index_fin).tolist()\n",
    "        start = ()\n",
    "        cropping = sum(zip(crop_ini, crop_fin), start)\n",
    "\n",
    "        low = cropping[::2]\n",
    "        high = cropping[1::2]\n",
    "        initial_idx = low\n",
    "        final_idx = np.array(img_spatial_shape) - high\n",
    "\n",
    "        i0, j0, k0 = initial_idx\n",
    "        i1, j1, k1 = final_idx\n",
    "\n",
    "        return img[:, i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "\n",
    "# Collate function for the DataLoader to combine multiple samples.\n",
    "def collate_subj(samples, full_res_key: str, low_res_key: str):\n",
    "    full_res_stack = torch.stack([subj[full_res_key].data for subj in samples])\n",
    "    # Assume the low-res data are dicts, not `torchio.Image`'s\n",
    "    low_res_stack = torch.stack([subj[low_res_key][\"data\"] for subj in samples])\n",
    "\n",
    "    return MultiresSample(low_res=low_res_stack, full_res=full_res_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:11:26.498149Z",
     "iopub.status.busy": "2021-04-06T19:11:26.497656Z",
     "iopub.status.idle": "2021-04-06T19:11:26.504529Z",
     "shell.execute_reply": "2021-04-06T19:11:26.503156Z",
     "shell.execute_reply.started": "2021-04-06T19:11:26.498090Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "downsample_factor = 2\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 1500)\n",
    "dti_fit_method = \"WLS\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:11:27.249592Z",
     "iopub.status.busy": "2021-04-06T19:11:27.249086Z",
     "iopub.status.idle": "2021-04-06T19:11:27.268321Z",
     "shell.execute_reply": "2021-04-06T19:11:27.267416Z",
     "shell.execute_reply.started": "2021-04-06T19:11:27.249533Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-794723799987>:25: UserWarning:\n",
      "\n",
      "WARNING: Sub-selecting participants for dev and debugging. Subj IDs selected: ['141422', '700634']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{141422: PosixPath('/mnt/storage/data/pitn/hcp/141422/T1w/Diffusion'),\n",
       " 700634: PosixPath('/mnt/storage/data/pitn/hcp/700634/T1w/Diffusion')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, 2)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90 scans are taken from the $b=1000 \\ s/mm^2$. However, the $b=0$ shells are still required for fitting the diffusion tensors (DTI's), so those will need to be kept, too.\n",
    "\n",
    "To find those, sub-select with the $0 < bvals < 1500$, or roughly thereabout. A b-val of $995$ or $1005$ still counts as a b=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:11:27.947017Z",
     "iopub.status.busy": "2021-04-06T19:11:27.946515Z",
     "iopub.status.idle": "2021-04-06T19:11:53.247344Z",
     "shell.execute_reply": "2021-04-06T19:11:53.245918Z",
     "shell.execute_reply.started": "2021-04-06T19:11:27.946958Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading NIFTI image\n",
      "\tLoaded NIFTI image\n",
      "Selecting with bvals\n",
      "\tSelected\n",
      "Downsampling\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "img should be PIL Image. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-52644e3d3aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0msubj_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreproc_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/envs/pitn/lib/python3.8/site-packages/torchio/transforms/transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages_to_keep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/envs/pitn/lib/python3.8/site-packages/torchio/transforms/augmentation/composition.py\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, subject)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSubject\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSubject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/envs/pitn/lib/python3.8/site-packages/torchio/transforms/transform.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mtransformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages_to_keep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7a22c51d78c6>\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(self, subject)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Don't pad dims that were not scaled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_factors\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             downsample_vol = torchvision.transforms.functional.pad(\n\u001b[0m\u001b[1;32m     80\u001b[0m                 \u001b[0mdownsample_vol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda/envs/pitn/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mpad\u001b[0;34m(img, padding, fill, padding_mode)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \"\"\"\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'img should be PIL Image. Got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: img should be PIL Image. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    # grad = torchio.ScalarImage(subj_dir/\"grad_dev.nii.gz\")\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "\n",
    "    # The brain mask is binary.\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "\n",
    "    preproc_transforms = torchio.Compose(\n",
    "        [\n",
    "            torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "            BValSelectionTransform(\n",
    "                bval_range=bval_range,\n",
    "                bval_key=\"bvals\",\n",
    "                bvec_key=\"bvecs\",\n",
    "                include=\"dwi\",\n",
    "                copy=False,\n",
    "            ),\n",
    "            MeanDownsampleTransform(\n",
    "                downsample_factor,\n",
    "                include=(\"dwi\", \"brain_mask\"),\n",
    "                keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform(\n",
    "                {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"fr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"fr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"lr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"lr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform({\"fr_dwi\": \"fr_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False),\n",
    "            ImageToDictTransform(include=(\"lr_dti\", \"lr_brain_mask\"), copy=False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    subj_data[subj_id] = preproc_transforms(subject_dict)\n",
    "\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-04-06T19:14:33.663583Z",
     "iopub.status.busy": "2021-04-06T19:14:33.662948Z",
     "iopub.status.idle": "2021-04-06T19:14:33.672799Z",
     "shell.execute_reply": "2021-04-06T19:14:33.671187Z",
     "shell.execute_reply.started": "2021-04-06T19:14:33.663517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.2'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:11:53.248178Z",
     "iopub.status.idle": "2021-04-06T19:11:53.248413Z",
     "shell.execute_reply": "2021-04-06T19:11:53.248299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_dataset = torchio.SubjectsDataset(list(subj_data.values()), load_getitem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.391443Z",
     "iopub.status.idle": "2021-04-06T19:10:45.391665Z",
     "shell.execute_reply": "2021-04-06T19:10:45.391557Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "batch_size = 32\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Output patch shapes\n",
    "h_out = 14\n",
    "w_out = 14\n",
    "d_out = 14\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)\n",
    "output_spatial_patch_shape = output_patch_shape[1:]\n",
    "# This is the factor that determines how over-extended the input patch should be\n",
    "# relative to the size of the input patch.\n",
    "# $input_patch_dim = \\frac{output_patch_dim}{downsample_factor} \\times input_dilation$\n",
    "# A value of 1 indicates that the input patch dims will be exactly divided by the\n",
    "# downsample factor. A dilation > 1 increases the \"spatial extent\" of the input\n",
    "# patch, providing information outside of the target HR patch.\n",
    "input_dim_dilation = 1.6\n",
    "# Input patch parameters\n",
    "h_in = round(h_out / (downsample_factor) * input_dim_dilation)\n",
    "w_in = round(w_out / (downsample_factor) * input_dim_dilation)\n",
    "d_in = round(d_out / (downsample_factor) * input_dim_dilation)\n",
    "input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "# Pre-shuffle output patch sizes.\n",
    "unshuffled_channels_out = channels * downsample_factor ** 3\n",
    "# Output before shuffling\n",
    "unshuffled_output_patch_shape = (unshuffled_channels_out, h_in, w_in, d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Patch-Based Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.392231Z",
     "iopub.status.idle": "2021-04-06T19:10:45.392448Z",
     "shell.execute_reply": "2021-04-06T19:10:45.392341Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data train/validation/test split\n",
    "test_percent = 0.2\n",
    "train_percent = 1 - test_percent\n",
    "# val_percent = 0.1\n",
    "\n",
    "num_subjs = len(subj_dataset)\n",
    "num_test_subjs = int(np.ceil(num_subjs * test_percent))\n",
    "num_train_subjs = num_subjs - num_test_subjs\n",
    "subj_list = subj_dataset.dry_iter()\n",
    "# Randomly shuffle the list of subjects, then choose the first `num_test_subjs` subjects\n",
    "# for testing.\n",
    "random.shuffle(subj_list)\n",
    "test_dataset = torchio.SubjectsDataset(subj_list[:num_test_subjs], load_getitem=False)\n",
    "# Choose the remaining for training/validation.\n",
    "subj_list = subj_list[num_test_subjs:]\n",
    "train_dataset = torchio.SubjectsDataset(subj_list, load_getitem=False)\n",
    "\n",
    "# Training patch sampler, random across all patches of all volumes.\n",
    "train_sampler = MultiresSampler(\n",
    "    source_img_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    downsample_factor_key=\"downsample_factor\",\n",
    "    label_name=\"fr_brain_mask\",\n",
    "    source_spatial_patch_size=output_spatial_patch_shape,\n",
    "    low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "    label_probabilities={0: 0, 1: 1},\n",
    ")\n",
    "# Set up a torchio.Queue to act as a sampler proxy for the torch DataLoader\n",
    "train_queue = torchio.Queue(\n",
    "    train_dataset,\n",
    "    max_length=5 * batch_size,\n",
    "    samples_per_volume=batch_size,\n",
    "    sampler=train_sampler,\n",
    "    shuffle_patches=True,\n",
    "    shuffle_subjects=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "# Create partial function to collect list of samples and form a tuple of tensors.\n",
    "collate_fn = functools.partial(\n",
    "    collate_subj, full_res_key=\"fr_dti\", low_res_key=\"lr_dti\"\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_queue,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# Test samplers must be dynamically created during testing.\n",
    "\n",
    "print(\"Test subject(s) IDs: \", [s.subj_id for s in test_dataset.dry_iter()])\n",
    "print(\"Training subject(s) IDs: \", [s.subj_id for s in train_dataset.dry_iter()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.393024Z",
     "iopub.status.idle": "2021-04-06T19:10:45.393239Z",
     "shell.execute_reply": "2021-04-06T19:10:45.393132Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle operation as a function.\n",
    "def espcn_shuffle(x, channels):\n",
    "    \"\"\"Implements final-layer shuffle operation from ESPCN.\n",
    "\n",
    "    x: 4D or 5D Tensor. Expects a shape of $C \\times H \\times W \\times D$, or batched\n",
    "        with a shape of $B \\times C \\times H \\times W \\times D$.\n",
    "\n",
    "    channels: Integer giving the number of channels for the shuffled output.\n",
    "    \"\"\"\n",
    "    batched = True if x.ndim == 5 else False\n",
    "\n",
    "    if batched:\n",
    "        downsample_factor = int(np.power(x.shape[1] / channels, 1 / 3))\n",
    "        y = einops.rearrange(\n",
    "            x,\n",
    "            \"b (c r1 r2 r3) h w d -> b c (h r1) (w r2) (d r3)\",\n",
    "            c=channels,\n",
    "            r1=downsample_factor,\n",
    "            r2=downsample_factor,\n",
    "            r3=downsample_factor,\n",
    "        )\n",
    "    else:\n",
    "        downsample_factor = int(np.power(x.shape[0] / channels, 1 / 3))\n",
    "        y = einops.rearrange(\n",
    "            x,\n",
    "            \"(c r1 r2 r3) h w d -> c (h r1) (w r2) (d r3)\",\n",
    "            c=channels,\n",
    "            r1=downsample_factor,\n",
    "            r2=downsample_factor,\n",
    "            r3=downsample_factor,\n",
    "        )\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.393968Z",
     "iopub.status.idle": "2021-04-06T19:10:45.394185Z",
     "shell.execute_reply": "2021-04-06T19:10:45.394078Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic conv net definition.\n",
    "class ThreeConv(torch.nn.Module):\n",
    "    \"\"\"Basic three-layer 3D conv network for DIQT.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, downsample_factor: int):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "        # Set up Conv layers.\n",
    "        self.conv1 = torch.nn.Conv3d(self.channels, 50, kernel_size=(3, 3, 3))\n",
    "        self.conv2 = torch.nn.Conv3d(50, 100, kernel_size=(1, 1, 1))\n",
    "        self.conv3 = torch.nn.Conv3d(\n",
    "            100, self.channels * (self.downsample_factor ** 3), kernel_size=(3, 3, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         breakpoint()\n",
    "        y_hat = self.conv1(x)\n",
    "        y_hat = F.relu(y_hat)\n",
    "        y_hat = self.conv2(y_hat)\n",
    "        y_hat = F.relu(y_hat)\n",
    "        y_hat = self.conv3(y_hat)\n",
    "\n",
    "        # Shuffle output.\n",
    "        y_hat = espcn_shuffle(y_hat, self.channels)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.394796Z",
     "iopub.status.idle": "2021-04-06T19:10:45.395012Z",
     "shell.execute_reply": "2021-04-06T19:10:45.394904Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full pytorch-lightning module for contained training, validation, and testing.\n",
    "class DIQTSystem(pl.LightningModule):\n",
    "    def __init__(self, channels, downsample_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self.channels = channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "        # Parameters\n",
    "        # Network parameters\n",
    "        self.net = ThreeConv(self.channels, self.downsample_factor)\n",
    "\n",
    "        ## Training parameters\n",
    "        self.lr = 10e-3\n",
    "        self.betas = (0.9, 0.999)\n",
    "        self.loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        #         breakpoint()\n",
    "        x, y = batch\n",
    "        y_pred = self.net(x)\n",
    "        loss = self.loss_fn(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    #     def validation_step(self, batch, batch_idx):\n",
    "    #         pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.net.parameters(), lr=self.lr, betas=self.betas\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.395621Z",
     "iopub.status.idle": "2021-04-06T19:10:45.395836Z",
     "shell.execute_reply": "2021-04-06T19:10:45.395730Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-04-06T19:10:45.396392Z",
     "iopub.status.idle": "2021-04-06T19:10:45.396608Z",
     "shell.execute_reply": "2021-04-06T19:10:45.396501Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DIQTSystem(channels=channels, downsample_factor=downsample_factor)\n",
    "# Create trainer object. Note: `automatic_optimization` needs to be set to `False` when\n",
    "# manually performing backprop. See\n",
    "# <https://colab.research.google.com/drive/1nGtvBFirIvtNQdppe2xBes6aJnZMjvl8?usp=sharing>\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, progress_bar_refresh_rate=10)\n",
    "# trainer = pl.Trainer(\n",
    "#     gpus=1, max_epochs=max_epochs, automatic_optimization=False, progress_bar_refresh_rate=10\n",
    "# )\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
