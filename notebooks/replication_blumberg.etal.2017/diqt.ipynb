{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain in the Net\n",
    "Replication of *Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images*\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source work:\n",
    "`S. B. Blumberg, R. Tanno, I. Kokkinos, and D. C. Alexander, “Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Cham, 2018, pp. 118–125, doi: 10.1007/978-3-030-00928-1_14.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import collections\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Generator\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import nilearn.plotting\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import einops\n",
    "import einops.layers\n",
    "import einops.layers.torch\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    src_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    src_location = str(Path(\"../../\").resolve())\n",
    "sys.path.append(src_location)\n",
    "import src as pitn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA Version: \", torch.version.cuda)\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment logging setup\n",
    "experiment_name = \"loss_L1\"\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "experiment_name = ts + \"_\" + experiment_name\n",
    "experiment_results_dir = results_dir / experiment_name\n",
    "(experiment_results_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(experiment_results_dir, \"\\n\", experiment_results_dir.exists())\n",
    "assert experiment_results_dir.exists()\n",
    "\n",
    "experiment_results_log = experiment_results_dir / \"log.txt\"\n",
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Function & Class Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Reader Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For more clearly designating the return values of a reader function given to\n",
    "# the `torchio.Image` object.\n",
    "ReaderOutput = collections.namedtuple(\"ReaderOutput\", [\"dwi\", \"affine\"])\n",
    "\n",
    "\n",
    "def nifti_reader(\n",
    "    f_dwi,\n",
    ") -> ReaderOutput:\n",
    "    \"\"\"Reader that reads in NIFTI files quickly.\n",
    "\n",
    "    Meant for use with the `torchio.Image` object and its sub-classes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load with nibabel first to get the correct affine matrix. See\n",
    "    # <https://github.com/ANTsX/ANTsPy/issues/52> for why I don't trust antspy for this.\n",
    "    # This does not require loading the entire NIFTI file into memory.\n",
    "    affine = nib.load(f_dwi).affine.copy()\n",
    "    affine = affine.astype(np.float32)\n",
    "    print(f\"Loading NIFTI image: {f_dwi}\", flush=True)\n",
    "    # Load entire image with antspy, then slice and (possibly) downsample that full image.\n",
    "    # A float32 is the smallest representation that doesn't lose data.\n",
    "    dwi = ants.image_read(str(f_dwi), pixeltype=\"float\")\n",
    "    print(\"\\tLoaded NIFTI image\", flush=True)\n",
    "\n",
    "    # Use `torch.tensor()` to explicitly copy the numpy array. May have issues with\n",
    "    # underlying memory getting garbage collected when using `torch.from_numpy`.\n",
    "    # <https://pytorch.org/docs/1.8.0/generated/torch.tensor.html#torch.tensor>\n",
    "    return ReaderOutput(dwi=torch.tensor(dwi.view()), affine=torch.tensor(affine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torchio.Transform functions/objects.\n",
    "\n",
    "\n",
    "class BValSelectionTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Sub-selects scans that are within a certain range of bvals.\n",
    "\n",
    "    Expects:\n",
    "    - volumes in canonical (RAS+) format with *channels first.*\n",
    "    - bvecs to be of shape (N, 3), with N being the number of scans/bvals.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bval_range: tuple, bval_key, bvec_key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_range = bval_range\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(f\"Selecting with bvals: Subject {subject.subj_id}\", flush=True)\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            bvals = img[self.bval_key]\n",
    "            scans_to_keep = (self.bval_range[0] <= bvals) & (\n",
    "                bvals <= self.bval_range[-1]\n",
    "            )\n",
    "            img[self.bvec_key] = img[self.bvec_key][scans_to_keep, :]\n",
    "            img.set_data(img.data[scans_to_keep, ...])\n",
    "            img[self.bval_key] = img[self.bval_key][scans_to_keep]\n",
    "        print(\"\\tSelected\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class MeanDownsampleTransform(torchio.SpatialTransform):\n",
    "    \"\"\"Mean downsampling transformation.\n",
    "\n",
    "    sample_extension: the extension of the low-res patch size relative to the full-res\n",
    "        patch size.\n",
    "\n",
    "    Ex. sample_extension of 1.5\n",
    "\n",
    "    Expects volumes in canonical (RAS+) format with *channels first.*\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, downsample_factor: int, spatial_padding: int = 0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.downsample_factor = downsample_factor\n",
    "        self.spatial_padding = spatial_padding\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        print(f\"Downsampling: Subject {subject.subj_id}\", flush=True)\n",
    "        # Get reference to Image objects that have been included for transformation.\n",
    "\n",
    "        for img in self.get_images(subject):\n",
    "            img[\"downsample_factor\"] = self.downsample_factor\n",
    "            if self.downsample_factor == 1:\n",
    "                continue\n",
    "            # Determine dimension-specific downsample factors\n",
    "            img_ndarray = img.data.numpy()\n",
    "            dim_factors = np.asarray(\n",
    "                [\n",
    "                    self.downsample_factor,\n",
    "                ]\n",
    "                * img_ndarray.ndim\n",
    "            )\n",
    "            # Only spatial dimensions should be downsampled.\n",
    "            if img.data.ndim > 3:\n",
    "                # Don't downsample the channels\n",
    "                dim_factors[0] = 1\n",
    "                # Or anything else outside of spatial dims.\n",
    "                dim_factors[4:] = 1\n",
    "\n",
    "            downsample_vol = skimage.transform.downscale_local_mean(\n",
    "                img_ndarray, factors=tuple(dim_factors), cval=0\n",
    "            )\n",
    "            # Pad with a small number of 0's to account for sampling at the edge of the\n",
    "            # full-res image.\n",
    "            # Don't pad dims that were not scaled.\n",
    "            padding_mask = (dim_factors - 1).astype(bool).astype(int)\n",
    "            padding = self.spatial_padding * padding_mask\n",
    "            padding = [(0, p) for p in padding.tolist()]\n",
    "            downsample_vol = np.pad(downsample_vol, pad_width=padding, mode=\"constant\")\n",
    "\n",
    "            downsample_vol = torch.from_numpy(\n",
    "                downsample_vol.astype(img_ndarray.dtype)\n",
    "            ).to(img.data.dtype)\n",
    "            img.set_data(downsample_vol)\n",
    "\n",
    "            scaled_affine = img.affine.copy()\n",
    "            # Scale the XYZ coordinates on the main diagonal.\n",
    "            scaled_affine[(0, 1, 2), (0, 1, 2)] = (\n",
    "                scaled_affine[(0, 1, 2), (0, 1, 2)] * self.downsample_factor\n",
    "            )\n",
    "            img.affine = scaled_affine\n",
    "        print(\"\\tDownsampled\", flush=True)\n",
    "        return subject\n",
    "\n",
    "\n",
    "class FitDTITransform(torchio.SpatialTransform, torchio.IntensityTransform):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bval_key,\n",
    "        bvec_key,\n",
    "        mask_img_key=None,\n",
    "        fit_method=\"WLS\",\n",
    "        tensor_model_kwargs=dict(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.bval_key = bval_key\n",
    "        self.bvec_key = bvec_key\n",
    "        self.mask_img_key = mask_img_key\n",
    "        self.fit_method = fit_method\n",
    "        self.tensor_model_kwargs = tensor_model_kwargs\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "\n",
    "        print(f\"Fitting to DTI: Subject {subject.subj_id}\", flush=True)\n",
    "        mask_img = subject[self.mask_img_key] if self.mask_img_key is not None else None\n",
    "        for img in self.get_images(subject):\n",
    "\n",
    "            gradient_table = dipy.core.gradients.gradient_table_from_bvals_bvecs(\n",
    "                bvals=img[self.bval_key],\n",
    "                bvecs=img[self.bvec_key],\n",
    "            )\n",
    "\n",
    "            tensor_model = dipy.reconst.dti.TensorModel(\n",
    "                gradient_table, fit_method=self.fit_method, **self.tensor_model_kwargs\n",
    "            )\n",
    "            print(f\"\\tDWI shape: {img.data.shape}\", flush=True)\n",
    "            # dipy does not like the channels being first, apparently.\n",
    "            if mask_img is not None:\n",
    "                dti = tensor_model.fit(\n",
    "                    np.moveaxis(img.numpy(), 0, -1),\n",
    "                    mask=mask_img.numpy().squeeze().astype(bool),\n",
    "                )\n",
    "            else:\n",
    "                dti = tensor_model.fit(np.moveaxis(img.numpy(), 0, -1))\n",
    "\n",
    "            # Pull only the lower-triangular part of the DTI (the non-symmetric\n",
    "            # coefficients.)\n",
    "            # Do it all in one line to minimize the time that the DTI's have to be\n",
    "            # duplicated in memory.\n",
    "            img.set_data(\n",
    "                torch.from_numpy(\n",
    "                    np.moveaxis(dti.lower_triangular().astype(np.float32), -1, 0)\n",
    "                ).to(img.data)\n",
    "            )\n",
    "            print(f\"\\tDTI shape: {img.shape}\", flush=True)\n",
    "        print(f\"\\tFitted DTI model: {img.data.shape}\", flush=True)\n",
    "\n",
    "        return subject\n",
    "\n",
    "\n",
    "class RenameImageTransform(torchio.Transform):\n",
    "    def __init__(self, name_mapping: dict, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.name_mapping = name_mapping\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        for old_name, new_name in self.name_mapping.items():\n",
    "            tmp = subject[old_name]\n",
    "            subject.remove_image(old_name)\n",
    "            subject.add_image(tmp, new_name)\n",
    "        subject.update_attributes()\n",
    "        return subject\n",
    "\n",
    "\n",
    "class ImageToDictTransform(torchio.Transform):\n",
    "    \"\"\"Convert a Subject Image to a simple dict item.\n",
    "\n",
    "    Removes the `include`ed keys from calculation of the Subject's properties, such as\n",
    "    `spatial_shape`, `spacing`, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def apply_transform(self, subject: torchio.Subject) -> torchio.Subject:\n",
    "        for img_name in self.include:\n",
    "\n",
    "            img_dict = dict(subject[img_name])\n",
    "            subject.remove_image(img_name)\n",
    "\n",
    "            subject[img_name] = img_dict\n",
    "\n",
    "        subject.update_attributes()\n",
    "        return subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sampling Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definitions for sampling and loading patches from volumes of different resolutions in\n",
    "# a `pytorch.utils.data.DataLoader`.\n",
    "\n",
    "\n",
    "def normalize_batch(patches):\n",
    "    \"\"\"Normalize a batch of patches to have mean 0 and variance 1\n",
    "\n",
    "    Computed over the batches, so means & vars are calculated as:\n",
    "        (B x C x H x W x D) -> (1 x C x H x W x D)\n",
    "    \"\"\"\n",
    "\n",
    "    mean = torch.mean(patches, dim=0, keepdim=True)\n",
    "    var = torch.var(patches, dim=0, keepdim=True)\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    return (patches - mean) / torch.sqrt(var + epsilon)\n",
    "\n",
    "\n",
    "def normalize_dti(dti, mean, var):\n",
    "    \"\"\"Normalize a 6-channel DTI tensor to have mean 0 and variance 1.\n",
    "\n",
    "    The normalization occurs according to the dimensionality of the mean and var passed.\n",
    "        To perform channel-wise normalization, mean and var must have shape\n",
    "        (C x 1 x 1 x 1)\n",
    "    \"\"\"\n",
    "\n",
    "    epsilon = 1e-5\n",
    "    return (dti - mean) / torch.sqrt(var + epsilon)\n",
    "\n",
    "\n",
    "def denormalize_dti(normal_dti, mean, var):\n",
    "    \"\"\"Inverse of DTI normalization.\"\"\"\n",
    "\n",
    "    epsilon = 1e-5\n",
    "    return (normal_dti * torch.sqrt(var + epsilon)) + mean\n",
    "\n",
    "\n",
    "def extract_patch(img, img_spatial_shape, index_ini, patch_size) -> torchio.Image:\n",
    "    \"\"\"Draws a patch from img, given an initial index and patch size.\"\"\"\n",
    "\n",
    "    # Just take it straight from `torchio.transforms.Copy.apply_transform`\n",
    "    # and `torchio.sampler.Sampler`!\n",
    "\n",
    "    shape = np.array(img_spatial_shape, dtype=np.uint16)\n",
    "    index_ini = np.array(index_ini, dtype=np.uint16)\n",
    "    patch_size = np.array(patch_size, dtype=np.uint16)\n",
    "    index_fin = index_ini + patch_size\n",
    "\n",
    "    crop_ini = index_ini.tolist()\n",
    "    crop_fin = (shape - index_fin).tolist()\n",
    "    start = ()\n",
    "    cropping = sum(zip(crop_ini, crop_fin), start)\n",
    "\n",
    "    low = cropping[::2]\n",
    "    high = cropping[1::2]\n",
    "    initial_idx = low\n",
    "    final_idx = np.array(img_spatial_shape) - high\n",
    "\n",
    "    i0, j0, k0 = initial_idx\n",
    "    i1, j1, k1 = final_idx\n",
    "\n",
    "    return img[:, i0:i1, j0:j1, k0:k1]\n",
    "\n",
    "\n",
    "# Custom sampler for sampling multiple volumes of different resolutions.\n",
    "class MultiresSampler(torchio.LabelSampler):\n",
    "    \"\"\"\n",
    "\n",
    "    source_img_key: Key to the Subject that will fetch the source (a.k.a., the high-res\n",
    "        or full-res) image.\n",
    "\n",
    "    low_res_key: Key to the Subject that will fetch the low-res image. This image is\n",
    "        assumed to be a dictionary with a 'data' key.\n",
    "\n",
    "    downsample_factor_key: Key to the low-res image dict that gives the downsample\n",
    "        factor.\n",
    "\n",
    "    source_spatial_patch_size: 3-tuple of `(W, H, D)` that gives the spatial size of\n",
    "        patches drawn from the source image.\n",
    "\n",
    "    low_res_spatial_patch_size: 3-tuple of `(W, H, D)` that gives the spatial size of\n",
    "        patches drawn from the low-res image.\n",
    "\n",
    "    subj_keys_to_copy: Tuple of keys to copy from the Subject into the returned sample\n",
    "        patch(es).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_img_key,\n",
    "        low_res_key,\n",
    "        downsample_factor_key,\n",
    "        source_spatial_patch_size: tuple,\n",
    "        low_res_spatial_patch_size: tuple,\n",
    "        label_name,\n",
    "        subj_keys_to_copy=tuple(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            patch_size=source_spatial_patch_size, label_name=label_name, **kwargs\n",
    "        )\n",
    "        self.source_img_key = source_img_key\n",
    "        self.low_res_key = low_res_key\n",
    "        self.downsample_factor_key = downsample_factor_key\n",
    "        self.subj_keys_to_copy = subj_keys_to_copy\n",
    "        self.source_spatial_patch_size = source_spatial_patch_size\n",
    "        self.low_res_spatial_patch_size = low_res_spatial_patch_size\n",
    "\n",
    "    def __call__(\n",
    "        self, subject: torchio.Subject, num_patches=None\n",
    "    ) -> Generator[torchio.Subject, None, None]:\n",
    "\n",
    "        # Setup copied from the `torchio.WeightedSampler.__call__` function definition.\n",
    "        subject.check_consistent_space()\n",
    "        if np.any(self.patch_size > subject.spatial_shape):\n",
    "            message = (\n",
    "                f\"Patch size {tuple(self.patch_size)} cannot be\"\n",
    "                f\" larger than image size {tuple(subject.spatial_shape)}\"\n",
    "            )\n",
    "            raise RuntimeError(message)\n",
    "        probability_map = self.get_probability_map(subject)\n",
    "        probability_map = self.process_probability_map(probability_map, subject)\n",
    "        cdf = self.get_cumulative_distribution_function(probability_map)\n",
    "\n",
    "        patches_left = num_patches if num_patches is not None else True\n",
    "        while patches_left:\n",
    "            subj_fields_transfer = dict(\n",
    "                ((k, subject[k]) for k in self.subj_keys_to_copy)\n",
    "            )\n",
    "\n",
    "            # Sample an index from the full-res image.\n",
    "            source_index_ini = self.get_random_index_ini(probability_map, cdf)\n",
    "            # Create a new subject that only contains patches.\n",
    "            # Add the patch from the full-res image into the subject.\n",
    "            source_tensor = extract_patch(\n",
    "                subject[self.source_img_key].data,\n",
    "                img_spatial_shape=subject[self.source_img_key].shape[1:],\n",
    "                index_ini=source_index_ini,\n",
    "                patch_size=self.source_spatial_patch_size,\n",
    "            )\n",
    "\n",
    "            patch_subj = torchio.Subject(\n",
    "                **(\n",
    "                    dict(\n",
    "                        [\n",
    "                            (\n",
    "                                self.source_img_key,\n",
    "                                torchio.ScalarImage(\n",
    "                                    tensor=source_tensor,\n",
    "                                    affine=subject[self.source_img_key].affine,\n",
    "                                ),\n",
    "                            ),\n",
    "                            *subj_fields_transfer.items(),\n",
    "                        ],\n",
    "                    )\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Include the index in the subject.\n",
    "            patch_subj[\"index_ini\"] = np.array(source_index_ini).astype(int)\n",
    "            # Crop low-res image and add to the subject.\n",
    "            lr_index_ini = tuple(\n",
    "                np.array(source_index_ini).astype(int)\n",
    "                // subject[self.low_res_key][self.downsample_factor_key]\n",
    "            )\n",
    "\n",
    "            lr_patch = extract_patch(\n",
    "                subject[self.low_res_key][\"data\"],\n",
    "                img_spatial_shape=subject[self.low_res_key][\"data\"].shape[1:],\n",
    "                index_ini=lr_index_ini,\n",
    "                patch_size=self.low_res_spatial_patch_size,\n",
    "            )\n",
    "            if lr_patch.numel() == 0:\n",
    "                raise RuntimeError(\n",
    "                    f\"ERROR: Invalid low-res patch: {lr_patch}, {lr_patch.shape} |\"\n",
    "                    + f\"Index: {lr_index_ini}\"\n",
    "                )\n",
    "            # Add a dict to the subject patch, rather than a `torchio.Image`,\n",
    "            # because the fr and lr patch shapes will be different, and fail\n",
    "            # `torchio`'s shape consistency checks.)\n",
    "            lr_patch_dict = dict()\n",
    "            lr_patch_dict.update(subject[self.low_res_key])\n",
    "            lr_patch_dict.update({\"data\": lr_patch})\n",
    "\n",
    "            patch_subj[self.low_res_key] = lr_patch_dict\n",
    "            # Return the new patch subject.\n",
    "            yield patch_subj\n",
    "            if num_patches is not None:\n",
    "                patches_left -= 1\n",
    "\n",
    "\n",
    "class MultiresGridSampler(torchio.GridSampler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_img_key,\n",
    "        low_res_key,\n",
    "        downsample_factor_key,\n",
    "        source_spatial_patch_size: tuple,\n",
    "        low_res_spatial_patch_size: tuple,\n",
    "        subj_keys_to_copy=tuple(),\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(patch_size=source_spatial_patch_size, **kwargs)\n",
    "        self.source_img_key = source_img_key\n",
    "        self.low_res_key = low_res_key\n",
    "        self.downsample_factor_key = downsample_factor_key\n",
    "        self.subj_keys_to_copy = subj_keys_to_copy\n",
    "        self.source_spatial_patch_size = source_spatial_patch_size\n",
    "        self.low_res_spatial_patch_size = low_res_spatial_patch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        location = self.locations[index]\n",
    "        source_index_ini = location[:3]\n",
    "\n",
    "        subj_fields_transfer = dict(\n",
    "            ((k, self.subject[k]) for k in self.subj_keys_to_copy)\n",
    "        )\n",
    "\n",
    "        # Create a new subject that only contains patches.\n",
    "        # Add the patch from the full-res image into the subject.\n",
    "        source_tensor = extract_patch(\n",
    "            self.subject[self.source_img_key].data,\n",
    "            img_spatial_shape=self.subject[self.source_img_key].shape[1:],\n",
    "            index_ini=source_index_ini,\n",
    "            patch_size=self.source_spatial_patch_size,\n",
    "        )\n",
    "\n",
    "        patch_subj = torchio.Subject(\n",
    "            **(\n",
    "                dict(\n",
    "                    [\n",
    "                        (\n",
    "                            self.source_img_key,\n",
    "                            torchio.ScalarImage(\n",
    "                                tensor=source_tensor,\n",
    "                                affine=self.subject[self.source_img_key].affine,\n",
    "                            ),\n",
    "                        ),\n",
    "                        *subj_fields_transfer.items(),\n",
    "                    ],\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Include the index in the subject.\n",
    "        patch_subj[\"index_ini\"] = np.array(source_index_ini).astype(int)\n",
    "        patch_subj[torchio.LOCATION] = location\n",
    "        # Crop low-res image and add to the subject.\n",
    "        lr_index_ini = tuple(\n",
    "            np.array(source_index_ini).astype(int)\n",
    "            // self.subject[self.low_res_key][self.downsample_factor_key]\n",
    "        )\n",
    "\n",
    "        lr_patch = extract_patch(\n",
    "            self.subject[self.low_res_key][\"data\"],\n",
    "            img_spatial_shape=self.subject[self.low_res_key][\"data\"].shape[1:],\n",
    "            index_ini=lr_index_ini,\n",
    "            patch_size=self.low_res_spatial_patch_size,\n",
    "        )\n",
    "        if lr_patch.numel() == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"ERROR: Invalid low-res patch: {lr_patch}, {lr_patch.shape} |\"\n",
    "                + f\"Index: {lr_index_ini}\"\n",
    "            )\n",
    "        # Add a dict to the subject patch, rather than a `torchio.Image`,\n",
    "        # because the fr and lr patch shapes will be different, and fail\n",
    "        # `torchio`'s shape consistency checks.)\n",
    "        lr_patch_dict = dict()\n",
    "        lr_patch_dict.update(self.subject[self.low_res_key])\n",
    "        lr_patch_dict.update({\"data\": lr_patch})\n",
    "\n",
    "        patch_subj[self.low_res_key] = lr_patch_dict\n",
    "\n",
    "        return patch_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Return type wrapper\n",
    "MultiresSample = collections.namedtuple(\"MultiresSample\", (\"low_res\", \"full_res\"))\n",
    "\n",
    "# Collate function for the DataLoader to combine multiple samples.\n",
    "def collate_subj(samples, full_res_key: str, low_res_key: str):\n",
    "    full_res_stack = torch.stack([subj[full_res_key].data for subj in samples])\n",
    "    # Assume the low-res data are dicts, not `torchio.Image`'s\n",
    "    low_res_stack = torch.stack([subj[low_res_key][\"data\"] for subj in samples])\n",
    "\n",
    "    return MultiresSample(low_res=low_res_stack, full_res=full_res_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create FA map from DTI's\n",
    "def fa_map(dti, channels_first=True) -> np.ndarray:\n",
    "    if torch.is_tensor(dti):\n",
    "        t = dti.cpu().numpy()\n",
    "    else:\n",
    "        t = np.asarray(dti)\n",
    "    # Reshape to work with dipy.\n",
    "    if channels_first:\n",
    "        t = t.transpose(1, 2, 3, 0)\n",
    "\n",
    "    # Re-create the symmetric DTI's (3x3) from the lower-triangular portion (6).\n",
    "    t = dipy.reconst.dti.from_lower_triangular(t)\n",
    "    eigvals, eigvecs = dipy.reconst.dti.decompose_tensor(t)\n",
    "\n",
    "    fa = dipy.reconst.dti.fractional_anisotropy(eigvals)\n",
    "\n",
    "    return fa\n",
    "\n",
    "\n",
    "# Generate FA-weighted diffusion direction map.\n",
    "def direction_map(dti, channels_first=True) -> np.ndarray:\n",
    "\n",
    "    if torch.is_tensor(dti):\n",
    "        t = dti.cpu().numpy()\n",
    "    else:\n",
    "        t = np.asarray(dti)\n",
    "    # Reshape to work with dipy.\n",
    "    if channels_first:\n",
    "        t = t.transpose(1, 2, 3, 0)\n",
    "\n",
    "    # Re-create the symmetric DTI's (3x3) from the lower-triangular portion (6).\n",
    "    t = dipy.reconst.dti.from_lower_triangular(t)\n",
    "    eigvals, eigvecs = dipy.reconst.dti.decompose_tensor(t)\n",
    "\n",
    "    fa = dipy.reconst.dti.fractional_anisotropy(eigvals)\n",
    "    direction_map = dipy.reconst.dti.color_fa(fa, eigvecs)\n",
    "\n",
    "    if channels_first:\n",
    "        return direction_map.transpose(3, 0, 1, 2)\n",
    "\n",
    "    return direction_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downsample_factor = 2\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 1500)\n",
    "dti_fit_method = \"WLS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "batch_size = 64\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Output patch shapes\n",
    "h_out = 14\n",
    "w_out = 14\n",
    "d_out = 14\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)\n",
    "output_spatial_patch_shape = output_patch_shape[1:]\n",
    "# This is the factor that determines how over-extended the input patch should be\n",
    "# relative to the size of the full-res patch.\n",
    "# $low_res_patch_dim = \\frac{full_res_patch_dim}{downsample_factor} \\times low_res_sample_extension$\n",
    "# A value of 1 indicates that the input patch dims will be exactly divided by the\n",
    "# downsample factor. A dilation > 1 increases the \"spatial extent\" of the input\n",
    "# patch, providing information outside of the target HR patch.\n",
    "low_res_sample_extension = 1.57\n",
    "# Input patch parameters\n",
    "h_in = round(h_out / (downsample_factor) * low_res_sample_extension)\n",
    "w_in = round(w_out / (downsample_factor) * low_res_sample_extension)\n",
    "d_in = round(d_out / (downsample_factor) * low_res_sample_extension)\n",
    "input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "# Pre-shuffle output patch sizes.\n",
    "unshuffled_channels_out = channels * downsample_factor ** 3\n",
    "# Output before shuffling\n",
    "unshuffled_output_patch_shape = (unshuffled_channels_out, h_in, w_in, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Downsample Factor: {downsample_factor}\\n\")\n",
    "    f.write(f\"DTI Fit Method: {dti_fit_method}\\n\")\n",
    "    f.write(f\"Input Patch Size: {input_patch_shape}\\n\")\n",
    "    f.write(f\"Output Patch Size: {output_patch_shape}\\n\")\n",
    "    f.write(f\"Batch Size: {batch_size}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject ID Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, 10)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90 scans are taken from the $b=1000 \\ s/mm^2$. However, the $b=0$ shells are still required for fitting the diffusion tensors (DTI's), so those will need to be kept, too.\n",
    "\n",
    "To find those, sub-select with the $0 < bvals < 1500$, or roughly thereabout. A b-val of $995$ or $1005$ still counts as a b=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Selected Subjects: {selected_ids}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    # grad = torchio.ScalarImage(subj_dir/\"grad_dev.nii.gz\")\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "\n",
    "    # The brain mask is binary.\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    # Padding amount for the downsampled DWI's. Padded to account for low-res patches\n",
    "    # that extend beyond the corresponding full-res patch; otherwise, patches will be\n",
    "    # sampled outside the low-res volume.\n",
    "    lr_spatial_padding = int(\n",
    "        max(\n",
    "            0,\n",
    "            np.ceil(\n",
    "                low_res_sample_extension * (h_out // downsample_factor)\n",
    "                - (h_out // downsample_factor)\n",
    "            )\n",
    "            + 1,\n",
    "        )\n",
    "    )\n",
    "    channel_mean = dwi.data.mean(dim=(1, 2, 3))\n",
    "    #     channel_std\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "\n",
    "    preproc_transforms = torchio.Compose(\n",
    "        [\n",
    "            torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "            BValSelectionTransform(\n",
    "                bval_range=bval_range,\n",
    "                bval_key=\"bvals\",\n",
    "                bvec_key=\"bvecs\",\n",
    "                include=\"dwi\",\n",
    "                copy=False,\n",
    "            ),\n",
    "            MeanDownsampleTransform(\n",
    "                downsample_factor,\n",
    "                spatial_padding=lr_spatial_padding,\n",
    "                include=(\"dwi\", \"brain_mask\"),\n",
    "                keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform(\n",
    "                {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"fr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"fr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"lr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"lr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            RenameImageTransform({\"fr_dwi\": \"fr_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False),\n",
    "            ImageToDictTransform(include=(\"lr_dti\", \"lr_brain_mask\"), copy=False),\n",
    "        ]\n",
    "    )\n",
    "    # Normalize DTI's to have mean 0 and variance 1.\n",
    "    preproced_subj = preproc_transforms(subject_dict)\n",
    "    # Store subject-and-channel-wise means and vars in order to reverse the normalization\n",
    "    # for the final visualization/output.\n",
    "    fr_mask = preproced_subj.fr_brain_mask.tensor\n",
    "\n",
    "    preproced_subj[\"channel_means\"] = (\n",
    "        (preproced_subj.fr_dti.tensor * fr_mask)\n",
    "        .mean(dim=(1, 2, 3), keepdim=True)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    preproced_subj[\"channel_var\"] = (\n",
    "        (preproced_subj.fr_dti.tensor * fr_mask)\n",
    "        .var(dim=(1, 2, 3), keepdim=True)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    preproced_subj[\"fr_dti\"].set_data(\n",
    "        normalize_dti(\n",
    "            preproced_subj[\"fr_dti\"].data,\n",
    "            torch.as_tensor(preproced_subj[\"channel_means\"]),\n",
    "            torch.as_tensor(preproced_subj[\"channel_var\"]),\n",
    "        )\n",
    "    )\n",
    "    preproced_subj[\"lr_dti\"][\"data\"] = normalize_dti(\n",
    "        preproced_subj[\"lr_dti\"][\"data\"],\n",
    "        torch.as_tensor(preproced_subj[\"channel_means\"]),\n",
    "        torch.as_tensor(preproced_subj[\"channel_var\"]),\n",
    "    )\n",
    "    subj_data[subj_id] = preproced_subj\n",
    "\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_dataset = torchio.SubjectsDataset(list(subj_data.values()), load_getitem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Patch-Based Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data train/validation/test split\n",
    "test_percent = 0.2\n",
    "# test_percent = 0.5\n",
    "train_percent = 1 - test_percent\n",
    "# val_percent = 0.1\n",
    "\n",
    "num_subjs = len(subj_dataset)\n",
    "num_test_subjs = int(np.ceil(num_subjs * test_percent))\n",
    "num_train_subjs = num_subjs - num_test_subjs\n",
    "subj_list = subj_dataset.dry_iter()\n",
    "# Randomly shuffle the list of subjects, then choose the first `num_test_subjs` subjects\n",
    "# for testing.\n",
    "random.shuffle(subj_list)\n",
    "\n",
    "# Create partial function to collect list of samples and form a tuple of tensors.\n",
    "collate_fn = functools.partial(\n",
    "    collate_subj, full_res_key=\"fr_dti\", low_res_key=\"lr_dti\"\n",
    ")\n",
    "\n",
    "test_dataset = torchio.SubjectsDataset(subj_list[:num_test_subjs], load_getitem=False)\n",
    "# Choose the remaining for training/validation.\n",
    "# If only 1 subject is available, assume this is a debugging run.\n",
    "if num_subjs == 1 and num_train_subjs == 0:\n",
    "    print(\"DEBUG: Only 1 subject with no training subjects, mixing train and test set\")\n",
    "    subj_list = subj_list[:]\n",
    "    num_train_subjs = num_test_subjs\n",
    "else:\n",
    "    subj_list = subj_list[num_test_subjs:]\n",
    "\n",
    "train_dataset = torchio.SubjectsDataset(subj_list, load_getitem=False)\n",
    "\n",
    "# Training patch sampler, random across all patches of all volumes.\n",
    "train_sampler = MultiresSampler(\n",
    "    source_img_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    downsample_factor_key=\"downsample_factor\",\n",
    "    label_name=\"fr_brain_mask\",\n",
    "    source_spatial_patch_size=output_spatial_patch_shape,\n",
    "    low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "    label_probabilities={0: 0, 1: 1},\n",
    ")\n",
    "\n",
    "patches_per_subj = 8000\n",
    "queue_max_length = patches_per_subj * num_train_subjs\n",
    "\n",
    "# Set up a torchio.Queue to act as a sampler proxy for the torch DataLoader\n",
    "train_queue = torchio.Queue(\n",
    "    train_dataset,\n",
    "    max_length=queue_max_length,\n",
    "    samples_per_volume=patches_per_subj,\n",
    "    sampler=train_sampler,\n",
    "    shuffle_patches=True,\n",
    "    shuffle_subjects=True,\n",
    "    num_workers=0,\n",
    "    #     verbose=True,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_queue,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "# Set up testing objects.\n",
    "# Calculate the patch overlap needed for ~50% of the patch volume overlapping (which is\n",
    "# not the same as dividing each dimension by 2).\n",
    "input_vol_half_overlap = int(\n",
    "    np.floor(np.power(np.prod(input_spatial_patch_shape) / 2, 1 / 3))\n",
    ")\n",
    "# torchio requires an even-numbered overlap.\n",
    "if input_vol_half_overlap % 2 == 1:\n",
    "    input_vol_half_overlap += 1\n",
    "input_vol_half_overlap = np.repeat(input_vol_half_overlap, 3)\n",
    "\n",
    "# Repeat for the output.\n",
    "output_vol_half_overlap = np.floor(\n",
    "    np.power(np.prod(output_spatial_patch_shape) / 2, 1 / 3)\n",
    ").astype(int)\n",
    "# torchio requires an even-numbered overlap.\n",
    "if output_vol_half_overlap % 2 == 1:\n",
    "    output_vol_half_overlap += 1\n",
    "output_vol_half_overlap = np.repeat(output_vol_half_overlap, 3)\n",
    "\n",
    "# Test samplers\n",
    "test_samplers = list()\n",
    "for subj in test_dataset.dry_iter():\n",
    "    test_samplers.append(\n",
    "        MultiresGridSampler(\n",
    "            subject=subj,\n",
    "            source_img_key=\"fr_dti\",\n",
    "            low_res_key=\"lr_dti\",\n",
    "            downsample_factor_key=\"downsample_factor\",\n",
    "            source_spatial_patch_size=output_spatial_patch_shape,\n",
    "            low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "            patch_overlap=output_vol_half_overlap,\n",
    "        )\n",
    "    )\n",
    "\n",
    "concat_test_dataset = torch.utils.data.ConcatDataset(test_samplers)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    concat_test_dataset, batch_size=batch_size, collate_fn=collate_fn, pin_memory=True\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Test subject(s) IDs: \", [s.subj_id for s in test_dataset.dry_iter()])\n",
    "print(\"Training subject(s) IDs: \", [s.subj_id for s in train_dataset.dry_iter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Training Set Subjects: {[s.subj_id for s in test_dataset.dry_iter()]}\\n\")\n",
    "    f.write(f\"Test Set Subjects: {[s.subj_id for s in train_dataset.dry_iter()]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shuffle operation as a function.\n",
    "def espcn_shuffle(x, channels):\n",
    "    \"\"\"Implements final-layer shuffle operation from ESPCN.\n",
    "\n",
    "    x: 4D or 5D Tensor. Expects a shape of $C \\times H \\times W \\times D$, or batched\n",
    "        with a shape of $B \\times C \\times H \\times W \\times D$.\n",
    "\n",
    "    channels: Integer giving the number of channels for the shuffled output.\n",
    "    \"\"\"\n",
    "    batched = True if x.ndim == 5 else False\n",
    "\n",
    "    if batched:\n",
    "        downsample_factor = int(np.power(x.shape[1] / channels, 1 / 3))\n",
    "        y = einops.rearrange(\n",
    "            x,\n",
    "            \"b (c r1 r2 r3) h w d -> b c (h r1) (w r2) (d r3)\",\n",
    "            c=channels,\n",
    "            r1=downsample_factor,\n",
    "            r2=downsample_factor,\n",
    "            r3=downsample_factor,\n",
    "        )\n",
    "    else:\n",
    "        downsample_factor = int(np.power(x.shape[0] / channels, 1 / 3))\n",
    "        y = einops.rearrange(\n",
    "            x,\n",
    "            \"(c r1 r2 r3) h w d -> c (h r1) (w r2) (d r3)\",\n",
    "            c=channels,\n",
    "            r1=downsample_factor,\n",
    "            r2=downsample_factor,\n",
    "            r3=downsample_factor,\n",
    "        )\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic conv net definition.\n",
    "class ThreeConv(torch.nn.Module):\n",
    "    \"\"\"Basic three-layer 3D conv network for DIQT.\"\"\"\n",
    "\n",
    "    def __init__(self, channels: int, downsample_factor: int):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.downsample_factor = downsample_factor\n",
    "\n",
    "        # Set up Conv layers.\n",
    "        self.conv1 = torch.nn.Conv3d(self.channels, 50, kernel_size=(3, 3, 3))\n",
    "        self.conv2 = torch.nn.Conv3d(50, 100, kernel_size=(1, 1, 1))\n",
    "        self.conv3 = torch.nn.Conv3d(\n",
    "            100, self.channels * (self.downsample_factor ** 3), kernel_size=(3, 3, 3)\n",
    "        )\n",
    "        self.output_shuffle = einops.layers.torch.Rearrange(\n",
    "            \"b (c r1 r2 r3) h w d -> b c (h r1) (w r2) (d r3)\",\n",
    "            c=self.channels,\n",
    "            r1=self.downsample_factor,\n",
    "            r2=self.downsample_factor,\n",
    "            r3=self.downsample_factor,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #         breakpoint()\n",
    "        y_hat = self.conv1(x)\n",
    "        y_hat = F.relu(y_hat)\n",
    "        y_hat = self.conv2(y_hat)\n",
    "        y_hat = F.relu(y_hat)\n",
    "        y_hat = self.conv3(y_hat)\n",
    "\n",
    "        # Shuffle output.\n",
    "        y_hat = self.output_shuffle(y_hat)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full pytorch-lightning module for contained training, validation, and testing.\n",
    "class DIQTSystem(pl.LightningModule):\n",
    "    def __init__(self, channels, downsample_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        self._channels = channels\n",
    "        self._downsample_factor = downsample_factor\n",
    "\n",
    "        # Parameters\n",
    "        # Network parameters\n",
    "        self.net = ThreeConv(self._channels, self._downsample_factor)\n",
    "\n",
    "        ## Training parameters\n",
    "        self._lr = 10e-3\n",
    "        self._betas = (0.9, 0.999)\n",
    "        #         self._loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        #         self._loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "        self._loss_fn = torch.nn.L1Loss(reduction=\"mean\")\n",
    "        #         self._loss_fn = lambda y_hat, y: torch.sqrt(\n",
    "        #             F.mse_loss(y_hat, y, reduction=\"mean\")\n",
    "        #         )\n",
    "\n",
    "        # My own dinky logging object.\n",
    "        self.plain_log = {\"train_loss\": list(), \"val_loss\": list(), \"test_loss\": list()}\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        #         x = normalize_batch(x)\n",
    "        #         y = normalize_batch(y)\n",
    "        y_pred = self.net(x)\n",
    "        #         y_pred = normalize_batch(y_pred)\n",
    "        loss = self._loss_fn(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.plain_log[\"train_loss\"].append(float(loss.cpu()))\n",
    "        return loss\n",
    "\n",
    "    #     def validation_step(self, batch, batch_idx):\n",
    "    #         pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "        #         x = normalize_batch(x)\n",
    "        #         y = normalize_batch(y)\n",
    "        y_pred = self.net(x)\n",
    "        # Normalize the prediction, to make sure the RMSE is scaled correctly.\n",
    "        #         y_pred = normalize_batch(y_pred)\n",
    "        test_loss = torch.sqrt(F.mse_loss(y_pred, y))\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        self.plain_log[\"test_loss\"].append(float(test_loss.cpu()))\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.net.parameters(), lr=self._lr, betas=self._betas\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_epochs = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DIQTSystem(channels=channels, downsample_factor=downsample_factor)\n",
    "# Create trainer object. Note: `automatic_optimization` needs to be set to `False` when\n",
    "# manually performing backprop. See\n",
    "# <https://colab.research.google.com/drive/1nGtvBFirIvtNQdppe2xBes6aJnZMjvl8?usp=sharing>\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, progress_bar_refresh_rate=5)\n",
    "\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_name = \"Mean L1 Loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot rolling average window of training loss values.\n",
    "plt.figure(dpi=110)\n",
    "window = 1000\n",
    "rolling_mean = (\n",
    "    np.convolve(model.plain_log[\"train_loss\"], np.ones(window), \"valid\") / window\n",
    ")\n",
    "rolling_start = 100\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        window + rolling_start,\n",
    "        window + rolling_start + len(rolling_mean[rolling_start:]),\n",
    "    ),\n",
    "    rolling_mean[rolling_start:],\n",
    ")\n",
    "plt.title(train_loss_name + f\"\\nRolling Mean {window}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.ylim(0, 1)\n",
    "print(np.median(rolling_mean))\n",
    "print(\n",
    "    np.mean(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.var(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.max(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    ")\n",
    "\n",
    "plt.savefig(experiment_results_dir / \"train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(model.plain_log[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Model overview: {model}\\n\")\n",
    "    f.write(f\"Training loss function: {train_loss_name}\\n\")\n",
    "    f.write(f'Training loss: \\n\\t {model.plain_log[\"train_loss\"]} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss_name = \"RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=120)\n",
    "sns.histplot(np.asarray(model.plain_log[\"test_loss\"]), kde=True, bins=20)\n",
    "plt.xlabel(\"Loss\")\n",
    "plt.title(f\"Test Loss Frequencies with {test_loss_name}\")\n",
    "plt.savefig(experiment_results_dir / \"test_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Test loss function: {test_loss_name}\\n\")\n",
    "    f.write(f'Testing loss: \\n\\t {model.plain_log[\"test_loss\"]} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.asarray(model.plain_log[\"test_loss\"]).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create full 3D volume of full-res ground truth, low-res downsample, and high-res\n",
    "# inferences.\n",
    "@dataclass\n",
    "class SubjResult:\n",
    "    subj_id: int\n",
    "    full_res: torch.Tensor\n",
    "    low_res: torch.Tensor\n",
    "    full_res_predicted: torch.Tensor\n",
    "\n",
    "\n",
    "test_vol_results = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for subj in test_dataset.dry_iter():\n",
    "\n",
    "        # Create a grid sampler for this subject.\n",
    "        subj_sampler = MultiresGridSampler(\n",
    "            subject=subj,\n",
    "            source_img_key=\"fr_dti\",\n",
    "            low_res_key=\"lr_dti\",\n",
    "            downsample_factor_key=\"downsample_factor\",\n",
    "            source_spatial_patch_size=output_spatial_patch_shape,\n",
    "            low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "            patch_overlap=output_vol_half_overlap,\n",
    "        )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            subj_sampler, batch_size=256, pin_memory=True\n",
    "        )\n",
    "        aggregator = torchio.GridAggregator(subj_sampler, \"average\")\n",
    "\n",
    "        # Iterate over all batches of patches.\n",
    "        for batch in loader:\n",
    "\n",
    "            x = batch[\"lr_dti\"][\"data\"]\n",
    "            #             x = normalize_batch(x)\n",
    "            #             y = batch['fr_dti']['data']\n",
    "            # Locations are in reference to the full-res groud truth.\n",
    "            locations = batch[\"location\"]\n",
    "            predictions = model(x.to(model.device))\n",
    "            #             predictions = normalize_batch(predictions)\n",
    "            aggregator.add_batch(predictions.cpu(), locations)\n",
    "\n",
    "        # Collect the full-res ground truth, the low-res input, and the high-res\n",
    "        # prediction (aggregated) into one container object.\n",
    "        subj_means = torch.as_tensor(subj[\"channel_means\"])\n",
    "        subj_vars = torch.as_tensor(subj[\"channel_var\"])\n",
    "        subj_result = SubjResult(\n",
    "            subj_id=subj[\"subj_id\"],\n",
    "            full_res=denormalize_dti(\n",
    "                subj[\"fr_dti\"].tensor,\n",
    "                subj_means.to(subj[\"fr_dti\"].tensor),\n",
    "                subj_vars.to(subj[\"fr_dti\"].tensor),\n",
    "            ),\n",
    "            low_res=denormalize_dti(\n",
    "                subj[\"lr_dti\"][\"data\"],\n",
    "                subj_means.to(subj[\"lr_dti\"][\"data\"]),\n",
    "                subj_vars.to(subj[\"lr_dti\"][\"data\"]),\n",
    "            ),\n",
    "            full_res_predicted=denormalize_dti(\n",
    "                aggregator.get_output_tensor(), subj_means, subj_vars\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        test_vol_results.append(subj_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_idx = (89, slice(None, None, None), slice(None, None, None))\n",
    "vis_subj_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate FA-weighted diffusion direction map.\n",
    "tensor_key = \"full_res_predicted\"\n",
    "pred_dir_map = direction_map(\n",
    "    test_vol_results[vis_subj_idx].__getattribute__(tensor_key).data.cpu().numpy()\n",
    ")\n",
    "# Set channels last for matplotlib\n",
    "pred_dir_map = pred_dir_map.transpose(1, 2, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(pred_dir_map[slice_idx])\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(experiment_results_dir / \"pred_dir_map_sample.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate FA-weighted diffusion direction map.\n",
    "tensor_key = \"full_res\"\n",
    "fr_dir_map = direction_map(\n",
    "    test_vol_results[vis_subj_idx].__getattribute__(tensor_key).data.cpu().numpy()\n",
    ")\n",
    "# Set channels last for matplotlib\n",
    "fr_dir_map = fr_dir_map.transpose(1, 2, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(fr_dir_map[slice_idx])\n",
    "# plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(experiment_results_dir / \"ground_truth_dir_map_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "pred_fa = fa_map(test_vol_results[vis_subj_idx].full_res_predicted.data.cpu().numpy())\n",
    "fr_fa = fa_map(test_vol_results[vis_subj_idx].full_res.data.cpu().numpy())\n",
    "\n",
    "plt.title(\"Root Squared Error of FA\")\n",
    "plt.imshow(np.sqrt((pred_fa[slice_idx] - fr_fa[slice_idx]) ** 2))\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(experiment_results_dir / \"RMSE_FA_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tensor_key = \"full_res_predicted\"\n",
    "# for result in test_vol_results:\n",
    "#     np.save(\n",
    "#         results_dir / f\"subj_{result.subj_id}_full_res_predicted\",\n",
    "#         result.__getattribute__(tensor_key).data.cpu().numpy(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
