{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pain in the Net\n",
    "Replication of *Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images*\n",
    "\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source work:\n",
    "`S. B. Blumberg, R. Tanno, I. Kokkinos, and D. C. Alexander, “Deeper Image Quality Transfer: Training Low-Memory Neural Networks for 3D Images,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2018, Cham, 2018, pp. 118–125, doi: 10.1007/978-3-030-00928-1_14.`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "import functools\n",
    "import io\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import math\n",
    "import itertools\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import copy\n",
    "import pdb\n",
    "import random\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import typing\n",
    "from typing import Generator\n",
    "\n",
    "import ants\n",
    "import dipy\n",
    "import dipy.core\n",
    "import dipy.reconst\n",
    "import dipy.reconst.dti\n",
    "import dipy.segment.mask\n",
    "import dotenv\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Data management libraries.\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import nilearn.plotting\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import einops\n",
    "import einops.layers\n",
    "import einops.layers.torch\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(suppress=True, threshold=100, linewidth=88)\n",
    "torch.set_printoptions(sci_mode=False, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    src_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    src_location = str(Path(\"../../\").resolve())\n",
    "sys.path.append(src_location)\n",
    "import src as pitn\n",
    "\n",
    "%aimport src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # GPU information\n",
    "    # Taken from\n",
    "    # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "    # If GPUtil is not installed, skip this step.\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "        list_gpus = []\n",
    "        for gpu in gpus:\n",
    "            # get the GPU id\n",
    "            gpu_id = gpu.id\n",
    "            # name of GPU\n",
    "            gpu_name = gpu.name\n",
    "            driver_version = gpu.driver\n",
    "            cuda_version = torch.version.cuda\n",
    "            # get total memory\n",
    "            gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "            gpu_uuid = gpu.uuid\n",
    "            list_gpus.append(\n",
    "                (\n",
    "                    gpu_id,\n",
    "                    gpu_name,\n",
    "                    driver_version,\n",
    "                    cuda_version,\n",
    "                    gpu_total_memory,\n",
    "                    gpu_uuid,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            tabulate(\n",
    "                list_gpus,\n",
    "                headers=(\n",
    "                    \"id\",\n",
    "                    \"Name\",\n",
    "                    \"Driver Version\",\n",
    "                    \"CUDA Version\",\n",
    "                    \"Total Memory\",\n",
    "                    \"uuid\",\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "else:\n",
    "    print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment logging setup\n",
    "experiment_name = \"debug\"\n",
    "\n",
    "ts = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# Break ISO format because many programs don't like having colons ':' in a filename.\n",
    "ts = ts.replace(\":\", \"_\")\n",
    "experiment_name = ts + \"__\" + experiment_name\n",
    "\n",
    "# Create temporary directory for results directory, in case experiment does not finish.\n",
    "tmp_results_dir = results_dir / \"tmp\"\n",
    "tmp_dirs = list(tmp_results_dir.glob(\"*\"))\n",
    "\n",
    "# Only keep up to N tmp results.\n",
    "n_tmp_to_keep = 3\n",
    "if len(tmp_dirs) > (n_tmp_to_keep - 1):\n",
    "    print(\n",
    "        f\"More than {n_tmp_to_keep} temporary results, culling to the most recent \"\n",
    "        + f\"{n_tmp_to_keep-1}\"\n",
    "    )\n",
    "    tmps_to_delete = natsorted([str(tmp_dir) for tmp_dir in tmp_dirs])[\n",
    "        : -(n_tmp_to_keep - 1)\n",
    "    ]\n",
    "    for tmp_dir in tmps_to_delete:\n",
    "        shutil.rmtree(tmp_dir)\n",
    "        print(\"Deleted temporary results directory \", tmp_dir)\n",
    "\n",
    "experiment_results_dir = tmp_results_dir / experiment_name\n",
    "# Final target directory, to be made when experiment is complete.\n",
    "final_experiment_results_dir = results_dir / experiment_name\n",
    "\n",
    "(experiment_results_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(\n",
    "    \"Experiment results directory: \",\n",
    "    experiment_results_dir,\n",
    ")\n",
    "assert experiment_results_dir.exists()\n",
    "\n",
    "experiment_results_log = experiment_results_dir / \"log.txt\"\n",
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "    f.write(f\"Timestamp: {ts}\\n\")\n",
    "    f.write(f\"Environment and Hardware Info:\\n {cap}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Global Function & Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downsample_factor = 2\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 1500)\n",
    "dti_fit_method = \"WLS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "batch_size = 64\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Output patch shapes\n",
    "h_out = 14\n",
    "w_out = 14\n",
    "d_out = 14\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)\n",
    "output_spatial_patch_shape = output_patch_shape[1:]\n",
    "# This is the factor that determines how over-extended the input patch should be\n",
    "# relative to the size of the full-res patch.\n",
    "# $low_res_patch_dim = \\frac{full_res_patch_dim}{downsample_factor} \\times low_res_sample_extension$\n",
    "# A value of 1 indicates that the input patch dims will be exactly divided by the\n",
    "# downsample factor. A dilation > 1 increases the \"spatial extent\" of the input\n",
    "# patch, providing information outside of the target HR patch.\n",
    "low_res_sample_extension = 1.57\n",
    "# Input patch parameters\n",
    "h_in = round(h_out / (downsample_factor) * low_res_sample_extension)\n",
    "w_in = round(w_out / (downsample_factor) * low_res_sample_extension)\n",
    "d_in = round(d_out / (downsample_factor) * low_res_sample_extension)\n",
    "input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "# Pre-shuffle output patch sizes.\n",
    "unshuffled_channels_out = channels * downsample_factor ** 3\n",
    "# Output before shuffling\n",
    "unshuffled_output_patch_shape = (unshuffled_channels_out, h_in, w_in, d_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Downsample Factor: {downsample_factor}\\n\")\n",
    "    f.write(f\"DTI Fit Method: {dti_fit_method}\\n\")\n",
    "    f.write(f\"Input Patch Size: {input_patch_shape}\\n\")\n",
    "    f.write(f\"Output Patch Size: {output_patch_shape}\\n\")\n",
    "    f.write(f\"Batch Size: {batch_size}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject ID Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, 5)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 90 scans are taken from the $b=1000 \\ s/mm^2$. However, the $b=0$ shells are still required for fitting the diffusion tensors (DTI's), so those will need to be kept, too.\n",
    "\n",
    "To find those, sub-select with the $0 < bvals < 1500$, or roughly thereabout. A b-val of $995$ or $1005$ still counts as a b=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Selected Subjects: {selected_ids}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    # grad = torchio.ScalarImage(subj_dir/\"grad_dev.nii.gz\")\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "\n",
    "    # The brain mask is binary.\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    # Padding amount for the downsampled DWI's. Padded to account for low-res patches\n",
    "    # that extend beyond the corresponding full-res patch; otherwise, patches will be\n",
    "    # sampled outside the low-res volume.\n",
    "    lr_spatial_padding = int(\n",
    "        max(\n",
    "            0,\n",
    "            np.ceil(\n",
    "                low_res_sample_extension * (h_out // downsample_factor)\n",
    "                - (h_out // downsample_factor)\n",
    "            )\n",
    "            + 1,\n",
    "        )\n",
    "    )\n",
    "    channel_mean = dwi.data.mean(dim=(1, 2, 3))\n",
    "    #     channel_std\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "\n",
    "    preproc_transforms = torchio.Compose(\n",
    "        [\n",
    "            torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "            pitn.transforms.BValSelectionTransform(\n",
    "                bval_range=bval_range,\n",
    "                bval_key=\"bvals\",\n",
    "                bvec_key=\"bvecs\",\n",
    "                include=\"dwi\",\n",
    "                copy=False,\n",
    "            ),\n",
    "            pitn.transforms.MeanDownsampleTransform(\n",
    "                downsample_factor,\n",
    "                spatial_padding=lr_spatial_padding,\n",
    "                include=(\"dwi\", \"brain_mask\"),\n",
    "                keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "                copy=False,\n",
    "            ),\n",
    "            pitn.transforms.RenameImageTransform(\n",
    "                {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "            ),\n",
    "            pitn.transforms.FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"fr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"fr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            pitn.transforms.FitDTITransform(\n",
    "                \"bvals\",\n",
    "                \"bvecs\",\n",
    "                \"lr_brain_mask\",\n",
    "                fit_method=dti_fit_method,\n",
    "                include=(\"lr_dwi\"),\n",
    "                copy=False,\n",
    "            ),\n",
    "            pitn.transforms.RenameImageTransform(\n",
    "                {\"fr_dwi\": \"fr_dti\", \"lr_dwi\": \"lr_dti\"}, copy=False\n",
    "            ),\n",
    "            pitn.transforms.ImageToDictTransform(\n",
    "                include=(\"lr_dti\", \"lr_brain_mask\"), copy=False\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    # Normalize DTI's to have mean 0 and variance 1.\n",
    "    preproced_subj = preproc_transforms(subject_dict)\n",
    "    # Store subject-and-channel-wise means and vars in order to reverse the normalization\n",
    "    # for the final visualization/output.\n",
    "    fr_mask = preproced_subj.fr_brain_mask.tensor\n",
    "\n",
    "    preproced_subj[\"channel_means\"] = (\n",
    "        (preproced_subj.fr_dti.tensor * fr_mask)\n",
    "        .mean(dim=(1, 2, 3), keepdim=True)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    preproced_subj[\"channel_var\"] = (\n",
    "        (preproced_subj.fr_dti.tensor * fr_mask)\n",
    "        .var(dim=(1, 2, 3), keepdim=True)\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "\n",
    "    #     preproced_subj[\"fr_dti\"].set_data(\n",
    "    #         pitn.data.norm.normalize_dti(\n",
    "    #             preproced_subj[\"fr_dti\"].data,\n",
    "    #             torch.as_tensor(preproced_subj[\"channel_means\"]),\n",
    "    #             torch.as_tensor(preproced_subj[\"channel_var\"]),\n",
    "    #         )\n",
    "    #     )\n",
    "    #     preproced_subj[\"lr_dti\"][\"data\"] = pitn.data.norm.normalize_dti(\n",
    "    #         preproced_subj[\"lr_dti\"][\"data\"],\n",
    "    #         torch.as_tensor(preproced_subj[\"channel_means\"]),\n",
    "    #         torch.as_tensor(preproced_subj[\"channel_var\"]),\n",
    "    #     )\n",
    "    subj_data[subj_id] = preproced_subj\n",
    "\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj_dataset = torchio.SubjectsDataset(list(subj_data.values()), load_getitem=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Patch-Based Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data train/validation/test split\n",
    "test_percent = 0.2\n",
    "# test_percent = 0.5\n",
    "train_percent = 1 - test_percent\n",
    "# val_percent = 0.1\n",
    "\n",
    "num_subjs = len(subj_dataset)\n",
    "num_test_subjs = int(np.ceil(num_subjs * test_percent))\n",
    "num_train_subjs = num_subjs - num_test_subjs\n",
    "subj_list = subj_dataset.dry_iter()\n",
    "# Randomly shuffle the list of subjects, then choose the first `num_test_subjs` subjects\n",
    "# for testing.\n",
    "random.shuffle(subj_list)\n",
    "\n",
    "# Create partial function to collect list of samples and form a tuple of tensors.\n",
    "collate_fn = functools.partial(\n",
    "    pitn.samplers.collate_subj, full_res_key=\"fr_dti\", low_res_key=\"lr_dti\"\n",
    ")\n",
    "\n",
    "test_dataset = torchio.SubjectsDataset(subj_list[:num_test_subjs], load_getitem=False)\n",
    "# Choose the remaining for training/validation.\n",
    "# If only 1 subject is available, assume this is a debugging run.\n",
    "if num_subjs == 1 and num_train_subjs == 0:\n",
    "    print(\"DEBUG: Only 1 subject with no training subjects, mixing train and test set\")\n",
    "    subj_list = subj_list[:]\n",
    "    num_train_subjs = num_test_subjs\n",
    "else:\n",
    "    subj_list = subj_list[num_test_subjs:]\n",
    "\n",
    "train_dataset = torchio.SubjectsDataset(subj_list, load_getitem=False)\n",
    "\n",
    "# Training patch sampler, random across all patches of all volumes.\n",
    "train_sampler = pitn.samplers.MultiresSampler(\n",
    "    source_img_key=\"fr_dti\",\n",
    "    low_res_key=\"lr_dti\",\n",
    "    downsample_factor_key=\"downsample_factor\",\n",
    "    label_name=\"fr_brain_mask\",\n",
    "    source_spatial_patch_size=output_spatial_patch_shape,\n",
    "    low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "    label_probabilities={0: 0, 1: 1},\n",
    ")\n",
    "\n",
    "patches_per_subj = 8000\n",
    "# Hold enough for 3 epochs at a time.\n",
    "queue_max_length = patches_per_subj * num_train_subjs * 2\n",
    "\n",
    "# Set up a torchio.Queue to act as a sampler proxy for the torch DataLoader\n",
    "train_queue = torchio.Queue(\n",
    "    train_dataset,\n",
    "    max_length=queue_max_length,\n",
    "    samples_per_volume=patches_per_subj,\n",
    "    sampler=train_sampler,\n",
    "    shuffle_patches=True,\n",
    "    shuffle_subjects=True,\n",
    "    num_workers=7,\n",
    "    #     verbose=True,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_queue,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Set up testing objects.\n",
    "# Calculate the patch overlap needed for ~50% of the patch volume overlapping (which is\n",
    "# not the same as dividing each dimension by 2).\n",
    "input_vol_half_overlap = int(\n",
    "    np.floor(np.power(np.prod(input_spatial_patch_shape) / 2, 1 / 3))\n",
    ")\n",
    "# torchio requires an even-numbered overlap.\n",
    "if input_vol_half_overlap % 2 == 1:\n",
    "    input_vol_half_overlap += 1\n",
    "input_vol_half_overlap = np.repeat(input_vol_half_overlap, 3)\n",
    "\n",
    "# Repeat for the output.\n",
    "output_vol_half_overlap = np.floor(\n",
    "    np.power(np.prod(output_spatial_patch_shape) / 2, 1 / 3)\n",
    ").astype(int)\n",
    "# torchio requires an even-numbered overlap.\n",
    "if output_vol_half_overlap % 2 == 1:\n",
    "    output_vol_half_overlap += 1\n",
    "output_vol_half_overlap = np.repeat(output_vol_half_overlap, 3)\n",
    "\n",
    "# Test samplers\n",
    "test_samplers = list()\n",
    "for subj in test_dataset.dry_iter():\n",
    "    test_samplers.append(\n",
    "        pitn.samplers.MultiresGridSampler(\n",
    "            subject=subj,\n",
    "            source_img_key=\"fr_dti\",\n",
    "            low_res_key=\"lr_dti\",\n",
    "            downsample_factor_key=\"downsample_factor\",\n",
    "            source_spatial_patch_size=output_spatial_patch_shape,\n",
    "            low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "            patch_overlap=output_vol_half_overlap,\n",
    "        )\n",
    "    )\n",
    "\n",
    "concat_test_dataset = torch.utils.data.ConcatDataset(test_samplers)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    concat_test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Test subject(s) IDs: \", [s.subj_id for s in test_dataset.dry_iter()])\n",
    "print(\"Training subject(s) IDs: \", [s.subj_id for s in train_dataset.dry_iter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Training Set Subjects: {[s.subj_id for s in test_dataset.dry_iter()]}\\n\")\n",
    "    f.write(f\"Test Set Subjects: {[s.subj_id for s in train_dataset.dry_iter()]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Full pytorch-lightning module for contained training, validation, and testing.\n",
    "class DIQTSystem(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self, channels, downsample_factor, norm_method=None, train_loss_log_file=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self._channels = channels\n",
    "        self._downsample_factor = downsample_factor\n",
    "\n",
    "        # Parameters\n",
    "        # Network parameters\n",
    "        self.net = pitn.nn.models.ThreeConv(\n",
    "            self._channels, self._downsample_factor, norm_method=norm_method\n",
    "        )\n",
    "\n",
    "        ## Training parameters\n",
    "        self._lr = 10e-3\n",
    "        self._betas = (0.9, 0.999)\n",
    "        self._loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        #         self._loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "#         self._loss_fn = torch.nn.L1Loss(reduction=\"mean\")\n",
    "        #         self._loss_fn = lambda y_hat, y: torch.sqrt(\n",
    "        #             F.mse_loss(y_hat, y, reduction=\"mean\")\n",
    "        #         )\n",
    "\n",
    "        # My own dinky logging object.\n",
    "        self.plain_log = {\"train_loss\": list(), \"val_loss\": list(), \"test_loss\": list()}\n",
    "        self.train_loss_log_file = train_loss_log_file\n",
    "        if self.train_loss_log_file is not None:\n",
    "            with open(self.train_loss_log_file, \"a+\") as f:\n",
    "                f.write(\"epoch, batch_idx, loss\\n\")\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_gt(norm_layer, y):\n",
    "        if norm_layer is not None:\n",
    "            if not norm_layer.track_running_stats:\n",
    "                y = norm_layer(y)\n",
    "            else:\n",
    "                if isinstance(norm_layer, torch.nn.InstanceNorm3d):\n",
    "                    y = F.instance_norm(\n",
    "                        y, norm_layer.running_mean, norm_layer.running_var\n",
    "                    )\n",
    "                elif isinstance(norm_layer, torch.nn.BatchNorm3d):\n",
    "                    # Normalize the ground truths based on the tracked stats, rather than the\n",
    "                    # ground truth patches themselves. Otherwise, we would estimate the\n",
    "                    # mean and var from only this sample, leading to a biased estimate.\n",
    "                    # By using BatchNorm's accumulated stats, which come from the input\n",
    "                    # patches which should have the same distribution as the high-res\n",
    "                    # ground truths, we get a more \"realistic\" normalization.\n",
    "                    y = F.batch_norm(y, norm_layer.running_mean, norm_layer.running_var)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.net(x)\n",
    "        return y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # We need both prediction and ground truth to be a standard normal distribution\n",
    "        # for a fair calculation of the loss.\n",
    "        y_pred = self.net(x, denorm_output=True)\n",
    "#         if self.net.norm is not None:\n",
    "#             y = self.normalize_gt(self.net.norm, y)\n",
    "\n",
    "        loss = self._loss_fn(y_pred, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, logger=False)\n",
    "        self.plain_log[\"train_loss\"].append(float(loss.cpu()))\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        # Only log each epoch if a log filename was given.\n",
    "        if self.train_loss_log_file:\n",
    "\n",
    "            row_iter = itertools.product(\n",
    "                (self.current_epoch,),\n",
    "                range(len(training_step_outputs)),\n",
    "                [str(float(l[\"loss\"].detach().cpu())) for l in training_step_outputs],\n",
    "            )\n",
    "\n",
    "            loss_rows = \"\".join(\n",
    "                [\n",
    "                    f\"{epoch}, {batch_idx}, {loss}\\n\"\n",
    "                    for epoch, batch_idx, loss in row_iter\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            with open(self.train_loss_log_file, \"a+\") as f:\n",
    "                f.write(loss_rows)\n",
    "\n",
    "    #     def validation_step(self, batch, batch_idx):\n",
    "    #         pass\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        # We need normalized outputs and ground truths so that RMSE can produce a valid\n",
    "        # metric.\n",
    "        y_pred = self.net(x, denorm_output=True)\n",
    "#         if self.net.norm is not None:\n",
    "#             y = self.normalize_gt(self.net.norm, y)\n",
    "\n",
    "        test_loss = torch.sqrt(F.mse_loss(y_pred, y))\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "        self.plain_log[\"test_loss\"].append(float(test_loss.cpu()))\n",
    "\n",
    "        return test_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.net.parameters(), lr=self._lr, betas=self._betas\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "max_epochs = 30\n",
    "norm_method = \"batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_name = \"MSE\"\n",
    "train_loss_log_file = experiment_results_dir / \"train_loss.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_start_timestamp = datetime.datetime.now().replace(microsecond=0)\n",
    "\n",
    "model = DIQTSystem(\n",
    "    channels=channels,\n",
    "    downsample_factor=downsample_factor,\n",
    "    train_loss_log_file=train_loss_log_file,\n",
    "    norm_method=norm_method,\n",
    ")\n",
    "\n",
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Max epochs: {max_epochs}\\n\")\n",
    "    f.write(f\"Model overview: {model}\\n\")\n",
    "    f.write(f\"Training loss function: {train_loss_name}\\n\")\n",
    "\n",
    "# Create trainer object. Note: `automatic_optimization` needs to be set to `False` when\n",
    "# manually performing backprop. See\n",
    "# <https://colab.research.google.com/drive/1nGtvBFirIvtNQdppe2xBes6aJnZMjvl8?usp=sharing>\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=max_epochs, progress_bar_refresh_rate=100)\n",
    "\n",
    "trainer.fit(model, train_loader)\n",
    "train_duration = datetime.datetime.now().replace(microsecond=0) - train_start_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(\"\\n\")\n",
    "    f.write(f\"Training time: {train_duration}\\n\")\n",
    "    f.write(\n",
    "        f\"\\t{train_duration.days} Days, \"\n",
    "        + f\"{train_duration.seconds // 3600} Hours,\"\n",
    "        + f\"{(train_duration.seconds // 60) % 60} Minutes,\"\n",
    "        + f'{train_duration.seconds % 60} Seconds\"\\n'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot rolling average window of training loss values.\n",
    "plt.figure(dpi=110)\n",
    "window = 1000\n",
    "rolling_mean = (\n",
    "    np.convolve(model.plain_log[\"train_loss\"], np.ones(window), \"valid\") / window\n",
    ")\n",
    "rolling_start = 100\n",
    "plt.plot(\n",
    "    np.arange(\n",
    "        window + rolling_start,\n",
    "        window + rolling_start + len(rolling_mean[rolling_start:]),\n",
    "    ),\n",
    "    rolling_mean[rolling_start:],\n",
    ")\n",
    "plt.title(\"Training Loss \" + train_loss_name + f\"\\nRolling Mean {window}\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "# plt.ylim(0, 1)\n",
    "print(np.median(rolling_mean))\n",
    "print(\n",
    "    np.mean(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.var(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    "    np.max(model.plain_log[\"train_loss\"][: window + rolling_start]),\n",
    ")\n",
    "\n",
    "plt.savefig(experiment_results_dir / \"train_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(model.plain_log[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss_name = \"RMSE\"\n",
    "test_loss_log_file = experiment_results_dir / \"test_loss.csv\"\n",
    "\n",
    "with open(experiment_results_log, \"a+\") as f:\n",
    "    f.write(f\"Test loss function: {test_loss_name}\\n\")\n",
    "\n",
    "row_iter = enumerate(model.plain_log[\"test_loss\"])\n",
    "test_loss_tabular = \"\".join([f\"{batch_idx}, {loss}\\n\" for batch_idx, loss in row_iter])\n",
    "\n",
    "with open(test_loss_log_file, \"a+\") as f:\n",
    "    f.write(\"batch_idx, loss\\n\")\n",
    "    f.write(test_loss_tabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax_prob = plt.subplots(figsize=(8, 4), dpi=120)\n",
    "\n",
    "sns.histplot(\n",
    "    np.asarray(model.plain_log[\"test_loss\"]),\n",
    "    kde=True,\n",
    "    stat=\"probability\",\n",
    "    log_scale=True,\n",
    "    ax=ax_prob,\n",
    "    label=\"Current Model\",\n",
    "    legend=False,\n",
    ")\n",
    "plt.xlabel(\"Loss in $mm^2/second$\")\n",
    "\n",
    "# Draw an entire other plot, invisibly, to have another y-axis.\n",
    "ax_count = ax_prob.twinx()\n",
    "sns.histplot(\n",
    "    np.asarray(model.plain_log[\"test_loss\"]), alpha=0, ax=ax_count, stat=\"count\"\n",
    ")\n",
    "\n",
    "comparison_kwargs = {\"ls\": \"--\", \"alpha\": 0.8, \"lw\": 2.5}\n",
    "plt.axvline(\n",
    "    31.738 * 10e-4,\n",
    "    label=\"(Tanno et. al., 2021) C-spline Mean\",\n",
    "    color=\"red\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    23.139 * 10e-4,\n",
    "    label=\"(Tanno et. al., 2021) RF\",\n",
    "    color=\"orange\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    13.609 * 10e-4,\n",
    "    label=\"(Tanno et. al., 2021) ESPCN Baseline\",\n",
    "    color=\"green\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.axvline(\n",
    "    13.412 * 10e-4,\n",
    "    label=\"(Tanno et. al., 2021) Best\",\n",
    "    color=\"purple\",\n",
    "    **comparison_kwargs,\n",
    ")\n",
    "plt.legend()\n",
    "plt.title(f\"Test Loss Histogram with Loss Function {test_loss_name}\")\n",
    "plt.savefig(experiment_results_dir / \"test_loss_hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.asarray(model.plain_log[\"test_loss\"]).max())\n",
    "print(np.asarray(model.plain_log[\"test_loss\"]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create full 3D volume of full-res ground truth, low-res downsample, and high-res\n",
    "# inferences.\n",
    "@dataclass\n",
    "class SubjResult:\n",
    "    subj_id: int\n",
    "    full_res: torch.Tensor\n",
    "    low_res: torch.Tensor\n",
    "    full_res_predicted: torch.Tensor\n",
    "\n",
    "\n",
    "test_vol_results = list()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for subj in test_dataset.dry_iter():\n",
    "\n",
    "        # Create a grid sampler for this subject.\n",
    "        subj_sampler = pitn.samplers.MultiresGridSampler(\n",
    "            subject=subj,\n",
    "            source_img_key=\"fr_dti\",\n",
    "            low_res_key=\"lr_dti\",\n",
    "            downsample_factor_key=\"downsample_factor\",\n",
    "            source_spatial_patch_size=output_spatial_patch_shape,\n",
    "            low_res_spatial_patch_size=input_spatial_patch_shape,\n",
    "            #             patch_overlap=output_vol_half_overlap,\n",
    "            patch_overlap=0,\n",
    "        )\n",
    "\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            subj_sampler, batch_size=256, pin_memory=True\n",
    "        )\n",
    "        aggregator = torchio.GridAggregator(subj_sampler, overlap_mode='average')\n",
    "\n",
    "        # Disable gradient computation and put network into eval mode while inferencing\n",
    "        # patches.\n",
    "        with torch.no_grad():\n",
    "            model.net.eval()\n",
    "            # Iterate over all batches of patches.\n",
    "            for batch in loader:\n",
    "\n",
    "                x = batch[\"lr_dti\"][\"data\"]\n",
    "                # Locations are in reference to the full-res groud truth.\n",
    "                locations = batch[\"location\"]\n",
    "                predictions = model.net(x.to(model.device)).detach().cpu()\n",
    "\n",
    "                aggregator.add_batch(predictions, locations)\n",
    "        # Set model state back to its original state.\n",
    "        model.net.train()\n",
    "        # Collect the full-res ground truth, the low-res input, and the high-res\n",
    "        # prediction (aggregated) into one container object.\n",
    "        subj_means = torch.as_tensor(subj[\"channel_means\"])\n",
    "        subj_vars = torch.as_tensor(subj[\"channel_var\"])\n",
    "        subj_result = SubjResult(\n",
    "            subj_id=subj[\"subj_id\"],\n",
    "            full_res=subj[\"fr_dti\"].tensor,\n",
    "            low_res=subj[\"lr_dti\"][\"data\"],\n",
    "            full_res_predicted=aggregator.get_output_tensor(),\n",
    "        )\n",
    "\n",
    "        test_vol_results.append(subj_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_subj_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate FA-weighted diffusion direction map.\n",
    "tensor_key = \"full_res_predicted\"\n",
    "pred_dir_map = pitn.viz.direction_map(\n",
    "    test_vol_results[vis_subj_idx].__getattribute__(tensor_key).data.cpu().numpy()\n",
    ")\n",
    "# Set channels last for matplotlib\n",
    "pred_dir_map = pred_dir_map.transpose(1, 2, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate FA-weighted diffusion direction map.\n",
    "tensor_key = \"full_res\"\n",
    "fr_dir_map = pitn.viz.direction_map(\n",
    "    test_vol_results[vis_subj_idx].__getattribute__(tensor_key).data.cpu().numpy()\n",
    ")\n",
    "# Set channels last for matplotlib\n",
    "fr_dir_map = fr_dir_map.transpose(1, 2, 3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "slice_idx = (89, slice(None, None, None), slice(None, None, None))\n",
    "low_res_slice_idx = tuple(s // 2 if isinstance(s, int) else s for s in slice_idx)\n",
    "print(slice_idx)\n",
    "print(low_res_slice_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(pred_dir_map[slice_idx]))\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(experiment_results_dir / \"pred_dir_map_sample.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.imshow(np.rot90(fr_dir_map[slice_idx]))\n",
    "# plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "plt.savefig(experiment_results_dir / \"ground_truth_dir_map_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, and root squared error\n",
    "\n",
    "channel_names = [\"Dxx\", \"Dxy\", \"Dyy\", \"Dxz\", \"Dyz\", \"Dzz\"]\n",
    "dti_names = [\n",
    "    \"Full-Res\",\n",
    "    \"Low-Res Input\",\n",
    "    \"Predicted\",\n",
    "    \"Root Squared Error\\nFR vs. Prediction\",\n",
    "]\n",
    "cmap = \"jet\"\n",
    "# cmaps = [\"Reds\", \"Greys\", \"Greens\", \"Purples\", \"Greys\", \"Blues\", ]\n",
    "\n",
    "dtis = [\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res.data[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .low_res[(slice(None), *low_res_slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res_predicted[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "]\n",
    "\n",
    "# Add root squared error between FR and prediction.\n",
    "dtis.append(np.sqrt((dtis[0] - dtis[-1]) ** 2))\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dtis[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(np.rot90(dti[j_col]), cmap=cmap, interpolation=None)\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(dti_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "plt.suptitle(\n",
    "    \"DTI Channel Breakdown, Per-Image Normalization\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "plt.savefig(experiment_results_dir / \"DTI_channel_sample_per_img_norm.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, and root squared error\n",
    "\n",
    "channel_names = [\"Dxx\", \"Dxy\", \"Dyy\", \"Dxz\", \"Dyz\", \"Dzz\"]\n",
    "dti_names = dti_names = [\n",
    "    \"Full-Res\",\n",
    "    \"Low-Res Input\",\n",
    "    \"Predicted\",\n",
    "    \"Root Squared Error\\nFR vs. Prediction\",\n",
    "]\n",
    "cmap = \"jet\"\n",
    "# cmaps = [\"Reds\", \"Greys\", \"Greens\", \"Purples\", \"Greys\", \"Blues\", ]\n",
    "\n",
    "dtis = [\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res.data[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .low_res[(slice(None), *low_res_slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res_predicted[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "]\n",
    "\n",
    "# Add root squared error\n",
    "dtis.append(np.sqrt((dtis[0] - dtis[-1]) ** 2))\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "# Reshape and concatenate the dtis in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "max_dti = np.quantile(np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.95)\n",
    "min_dti = np.quantile(np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.05)\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dtis[i_row]\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(dti[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_dti,\n",
    "            vmax=max_dti,\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(dti_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        axs.append(ax)\n",
    "\n",
    "color_norm = mpl.colors.Normalize(vmin=min_dti, vmax=max_dti)\n",
    "fig.colorbar(\n",
    "    mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap),\n",
    "    ax=axs,\n",
    "    location=\"right\",\n",
    "    fraction=0.1,\n",
    "    pad=0.03,\n",
    ")\n",
    "plt.suptitle(\n",
    "    \"DTI Channel Breakdown, Normalized over All Images\",\n",
    "    y=max_subplot_height + 0.015,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "plt.savefig(experiment_results_dir / \"DTI_channel_sample_global_norm.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display all 6 DTIs for ground truth, predicted, low-res input, and root squared error\n",
    "# Normalize by index in the DTI coefficients.\n",
    "# Reshape and concatenate the dtis in order to compute the quantiles of images with\n",
    "# different shapes (e.g., the low-res input patch).\n",
    "\n",
    "channel_names = [\"Dxx\", \"Dxy\", \"Dyy\", \"Dxz\", \"Dyz\", \"Dzz\"]\n",
    "dti_names = dti_names = [\n",
    "    \"Full-Res\",\n",
    "    \"Low-Res Input\",\n",
    "    \"Predicted\",\n",
    "    \"Root Squared Error\\nFR vs. Pred\",\n",
    "]\n",
    "cmap = \"coolwarm\"\n",
    "\n",
    "dtis = [\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res.data[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .low_res[(slice(None), *low_res_slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "    test_vol_results[vis_subj_idx]\n",
    "    .full_res_predicted[(slice(None), *slice_idx)]\n",
    "    .cpu()\n",
    "    .numpy(),\n",
    "]\n",
    "\n",
    "# Add root squared error\n",
    "dtis.append(np.sqrt((dtis[0] - dtis[-1]) ** 2))\n",
    "\n",
    "\n",
    "# Don't take the absolute max and min values, as there exist some extreme (e.g., > 3\n",
    "# orders of magnitude) outliers. Instead, take some percente quantile.\n",
    "max_dtis = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.95, axis=1\n",
    ")\n",
    "min_dtis = np.quantile(\n",
    "    np.concatenate([di.reshape(6, -1) for di in dtis], axis=1), 0.05, axis=1\n",
    ")\n",
    "\n",
    "max_dtis = np.max(np.abs([max_dtis, min_dtis]), axis=0)\n",
    "min_dtis = -1 * max_dtis\n",
    "\n",
    "nrows = len(dtis)\n",
    "ncols = len(channel_names)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 7), dpi=160)\n",
    "\n",
    "grid = mpl.gridspec.GridSpec(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figure=fig,\n",
    "    hspace=0.05,\n",
    "    wspace=0.05,\n",
    ")\n",
    "\n",
    "axs = list()\n",
    "max_subplot_height = 0\n",
    "for i_row in range(nrows):\n",
    "    dti = dtis[i_row]\n",
    "    axs_cols = list()\n",
    "\n",
    "    for j_col in range(ncols):\n",
    "        ax = fig.add_subplot(grid[i_row, j_col])\n",
    "        ax.imshow(\n",
    "            np.rot90(dti[j_col]),\n",
    "            cmap=cmap,\n",
    "            interpolation=None,\n",
    "            vmin=min_dtis[j_col],\n",
    "            vmax=max_dtis[j_col],\n",
    "        )\n",
    "        if ax.get_subplotspec().is_first_col():\n",
    "            ax.set_ylabel(dti_names[i_row], size=\"small\")\n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel(channel_names[j_col])\n",
    "\n",
    "        # Update highest subplot to put the `suptitle` later on.\n",
    "        max_subplot_height = max(\n",
    "            max_subplot_height, ax.get_position(original=False).get_points()[1, 1]\n",
    "        )\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        axs_cols.append(ax)\n",
    "\n",
    "    axs.append(axs_cols)\n",
    "\n",
    "# Place colorbars on each column.\n",
    "for j_col in range(ncols):\n",
    "\n",
    "    full_col_ax = [axs[i][j_col] for i in range(nrows)]\n",
    "\n",
    "    color_norm = mpl.colors.Normalize(vmin=min_dtis[j_col], vmax=max_dtis[j_col])\n",
    "\n",
    "    color_mappable = mpl.cm.ScalarMappable(norm=color_norm, cmap=cmap)\n",
    "    cbar = fig.colorbar(\n",
    "        color_mappable,\n",
    "        ax=full_col_ax,\n",
    "        location=\"top\",\n",
    "        orientation=\"horizontal\",\n",
    "        pad=0.01,\n",
    "        shrink=0.85,\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=8, rotation=35)\n",
    "    cbar.ax.xaxis.set_major_formatter(mpl.ticker.StrMethodFormatter(\"{x:g}\"))\n",
    "#     cbar.ax.ticklabel_format(scilimits=(3, -3), useOffset=False)\n",
    "\n",
    "plt.suptitle(\n",
    "    \"DTI Channel Breakdown, Channel-Wise Normalization\",\n",
    "    y=max_subplot_height + 0.01,\n",
    "    verticalalignment=\"bottom\",\n",
    ")\n",
    "plt.savefig(experiment_results_dir / \"DTI_channel_sample_channel_wise_norm.png\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Experiment is complete, move the results directory to its final location.\n",
    "experiment_results_dir = experiment_results_dir.rename(final_experiment_results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "183051735d0942a2bb0235ad5165b9f4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2f7d6f5e09554d7dbb7105f86681db5d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c77ba20078f44b35b0b06e2a0591927a",
        "IPY_MODEL_eb7541a0cccd4866b8ad95c4c966453d",
        "IPY_MODEL_cd5d0c4275224c01a0ec15be0452b24a"
       ],
       "layout": "IPY_MODEL_a6993ab0144c44aaa7f2872c7b39f57b"
      }
     },
     "2fe05e104b984c0ba3b2e898e4bc063d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "56e40bd503ad4b9ab9f816a532107110": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_183051735d0942a2bb0235ad5165b9f4",
       "style": "IPY_MODEL_bee7937ad86342c8bb38ebb2d6e66b65",
       "value": " 1000/1000 [02:22&lt;00:00,  7.04it/s, loss=0.0304, v_num=72]"
      }
     },
     "59bcc6126ece41fdb63877abfa59ea00": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "70d918864e714881a5c89882043f0603": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "82882e65f4b349409eb6c7cf160b17a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8469f5f45e7b491ea30d9d3c454703e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8657baa061a24ee581314a3c5da5fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "95db362a70ba4406845e670f33009dcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_a122e557112e4912acb4014e2738517c",
        "IPY_MODEL_d53ac7c4d2f84ac1bb7f629c66d67a9b",
        "IPY_MODEL_56e40bd503ad4b9ab9f816a532107110"
       ],
       "layout": "IPY_MODEL_d57a14cf97e740abbbe1fd498a8d4a3b"
      }
     },
     "a122e557112e4912acb4014e2738517c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_82882e65f4b349409eb6c7cf160b17a1",
       "style": "IPY_MODEL_8469f5f45e7b491ea30d9d3c454703e0",
       "value": "Epoch 199: 100%"
      }
     },
     "a6993ab0144c44aaa7f2872c7b39f57b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "bee7937ad86342c8bb38ebb2d6e66b65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c1a3d8d68b40433b8babfc958d65bc03": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c77ba20078f44b35b0b06e2a0591927a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fa5895996fc54053b1cae18441a7e933",
       "style": "IPY_MODEL_8657baa061a24ee581314a3c5da5fb63",
       "value": "Testing: 100%"
      }
     },
     "cd5d0c4275224c01a0ec15be0452b24a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_59bcc6126ece41fdb63877abfa59ea00",
       "style": "IPY_MODEL_70d918864e714881a5c89882043f0603",
       "value": " 11363/11363 [00:49&lt;00:00, 237.42it/s]"
      }
     },
     "d53ac7c4d2f84ac1bb7f629c66d67a9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_fc55ee197bb3427c943e795c24e733c7",
       "max": 1000,
       "style": "IPY_MODEL_c1a3d8d68b40433b8babfc958d65bc03",
       "value": 1000
      }
     },
     "d57a14cf97e740abbbe1fd498a8d4a3b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "daa8cfccd4d5475ca3958c24811c7d39": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "eb7541a0cccd4866b8ad95c4c966453d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_daa8cfccd4d5475ca3958c24811c7d39",
       "max": 1,
       "style": "IPY_MODEL_2fe05e104b984c0ba3b2e898e4bc063d",
       "value": 1
      }
     },
     "fa5895996fc54053b1cae18441a7e933": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fc55ee197bb3427c943e795c24e733c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
