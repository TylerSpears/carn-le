{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "897dea55-484c-4298-834e-395f05cf9204",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpolation with Rician Noise on DWI's\n",
    "\n",
    "Code by:\n",
    "\n",
    "Tyler Spears - tas6hh@virginia.edu\n",
    "\n",
    "Dr. Tom Fletcher\n",
    "\n",
    "---\n",
    "\n",
    "Source works:\n",
    "\n",
    "`Basu S, Fletcher T, Whitaker R. Rician Noise Removal in Diffusion Tensor MRI. In: Larsen R, Nielsen M, Sporring J, eds. Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2006. Vol 4190. Lecture Notes in Computer Science. Springer Berlin Heidelberg; 2006:117-125. doi:10.1007/11866565_15\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5700e8c-e0db-424f-88bd-94f1b789d8f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports & Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d2585-579d-4597-9600-8033cf36fbb2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74977980-c725-4295-81f8-5e6498338b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automatically re-import project-specific modules.\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# imports\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import copy\n",
    "import warnings\n",
    "import pdb\n",
    "import inspect\n",
    "import sys\n",
    "\n",
    "import os\n",
    "import io\n",
    "import subprocess\n",
    "\n",
    "import dotenv\n",
    "\n",
    "# Toolbelt/utility imports\n",
    "import natsort\n",
    "from natsort import natsorted\n",
    "import addict\n",
    "from addict import Addict\n",
    "import pprint\n",
    "from pprint import pprint as ppr\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# Computation & ML libraries.\n",
    "import numpy as np\n",
    "import scipy\n",
    "import skimage\n",
    "import skimage.feature\n",
    "import skimage.filters\n",
    "import skimage.measure\n",
    "import nibabel as nib\n",
    "import dipy\n",
    "import dipy.viz\n",
    "import dipy.align\n",
    "import dipy.reconst\n",
    "import ants\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchio\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Try importing GPUtil for printing GPU specs.\n",
    "# May not be installed if using CPU only.\n",
    "try:\n",
    "    import GPUtil\n",
    "except ImportError:\n",
    "    warnings.warn(\"WARNING: Package GPUtil not found, cannot print GPU specs\")\n",
    "\n",
    "plt.rcParams.update({\"figure.autolayout\": True})\n",
    "plt.rcParams.update({\"figure.facecolor\": [1.0, 1.0, 1.0, 1.0]})\n",
    "\n",
    "# Set print options for ndarrays/tensors.\n",
    "np.set_printoptions(threshold=100, linewidth=88)\n",
    "torch.set_printoptions(precision=8, threshold=100, linewidth=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a3facf-b02d-4720-add3-ce33c9be37a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Update notebook's environment variables with direnv.\n",
    "# This requires the python-dotenv package, and direnv be installed on the system\n",
    "# This will not work on Windows.\n",
    "# NOTE: This is kind of hacky, and not necessarily safe. Be careful...\n",
    "# Libraries needed on the python side:\n",
    "# - os\n",
    "# - subprocess\n",
    "# - io\n",
    "# - dotenv\n",
    "\n",
    "# Form command to be run in direnv's context. This command will print out\n",
    "# all environment variables defined in the subprocess/sub-shell.\n",
    "command = \"direnv exec {} /usr/bin/env\".format(os.getcwd())\n",
    "# Run command in a new subprocess.\n",
    "proc = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True, cwd=os.getcwd())\n",
    "# Store and format the subprocess' output.\n",
    "proc_out = proc.communicate()[0].strip().decode(\"utf-8\")\n",
    "# Use python-dotenv to load the environment variables by using the output of\n",
    "# 'direnv exec ...' as a 'dummy' .env file.\n",
    "dotenv.load_dotenv(stream=io.StringIO(proc_out), override=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc90c56-6a15-4dff-a35d-a57ed0c0028d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Project-specific scripts\n",
    "# It's easier to import it this way rather than make an entirely new package, due to\n",
    "# conflicts with local packages and anaconda installations.\n",
    "# You made me do this, poor python package management!!\n",
    "if \"PROJECT_ROOT\" in os.environ:\n",
    "    lib_location = str(Path(os.environ[\"PROJECT_ROOT\"]).resolve())\n",
    "else:\n",
    "    lib_location = str(Path(\"../../\").resolve())\n",
    "if lib_location not in sys.path:\n",
    "    sys.path.insert(0, lib_location)\n",
    "import lib as pitn\n",
    "\n",
    "# Include the top-level lib module along with its submodules.\n",
    "%aimport lib\n",
    "# Grab all submodules of lib, not including modules outside of the package.\n",
    "includes = list(\n",
    "    filter(\n",
    "        lambda m: m.startswith(\"lib.\"),\n",
    "        map(lambda x: x[1].__name__, inspect.getmembers(pitn, inspect.ismodule)),\n",
    "    )\n",
    ")\n",
    "# Run aimport magic with constructed includes.\n",
    "ipy = IPython.get_ipython()\n",
    "ipy.run_line_magic(\"aimport\", \", \".join(includes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e66c44-b666-45a3-9e59-99f05c04b2af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch setup\n",
    "# allow for CUDA usage, if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "# keep device as the cpu\n",
    "# device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e082452-58f2-4327-908f-cd2507d5efbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Specs Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d678c92-a393-4801-87eb-0e2921856a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "# Capture output and save to log. Needs to be at the *very first* line of the cell.\n",
    "# Watermark\n",
    "%load_ext watermark\n",
    "%watermark --author \"Tyler Spears\" --updated --iso8601  --python --machine --iversions --githash\n",
    "# if torch.cuda.is_available():\n",
    "\n",
    "#     # GPU information\n",
    "#     # Taken from\n",
    "#     # <https://www.thepythoncode.com/article/get-hardware-system-information-python>.\n",
    "#     # If GPUtil is not installed, skip this step.\n",
    "#     try:\n",
    "#         gpus = GPUtil.getGPUs()\n",
    "#         print(\"=\" * 50, \"GPU Specs\", \"=\" * 50)\n",
    "#         list_gpus = []\n",
    "#         for gpu in gpus:\n",
    "#             # get the GPU id\n",
    "#             gpu_id = gpu.id\n",
    "#             # name of GPU\n",
    "#             gpu_name = gpu.name\n",
    "#             driver_version = gpu.driver\n",
    "#             cuda_version = torch.version.cuda\n",
    "#             # get total memory\n",
    "#             gpu_total_memory = f\"{gpu.memoryTotal}MB\"\n",
    "#             gpu_uuid = gpu.uuid\n",
    "#             list_gpus.append(\n",
    "#                 (\n",
    "#                     gpu_id,\n",
    "#                     gpu_name,\n",
    "#                     driver_version,\n",
    "#                     cuda_version,\n",
    "#                     gpu_total_memory,\n",
    "#                     gpu_uuid,\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#         print(\n",
    "#             tabulate(\n",
    "#                 list_gpus,\n",
    "#                 headers=(\n",
    "#                     \"id\",\n",
    "#                     \"Name\",\n",
    "#                     \"Driver Version\",\n",
    "#                     \"CUDA Version\",\n",
    "#                     \"Total Memory\",\n",
    "#                     \"uuid\",\n",
    "#                 ),\n",
    "#             )\n",
    "#         )\n",
    "#     except NameError:\n",
    "#         print(\"CUDA Version: \", torch.version.cuda)\n",
    "\n",
    "# else:\n",
    "#     print(\"CUDA not in use, falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0afeb-8324-4df6-9af1-9b8ea0b63c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cap is defined in an ipython magic command\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f1f1f9-4d65-427b-9c0d-cfecf330087d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Variables & Definitions Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c52bfa-a14a-482e-b3ce-16e2088c5896",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up directories\n",
    "data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"hcp\"\n",
    "assert data_dir.exists()\n",
    "write_data_dir = pathlib.Path(os.environ[\"WRITE_DATA_DIR\"]) / \"hcp\"\n",
    "assert write_data_dir.exists()\n",
    "results_dir = pathlib.Path(os.environ[\"RESULTS_DIR\"])\n",
    "assert results_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63218aa-ac0b-4810-8e6a-3f7f686bf365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dict to keep track of experiment configuration parameters. Will not be logged to\n",
    "# tensorboard.\n",
    "exp_params = Addict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07922a29-9020-4f9d-960f-840f053cb63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "downsample_factor = 2\n",
    "# Include b=0 shells and b=1000 shells for DTI fitting.\n",
    "bval_range = (0, 3500)\n",
    "dti_fit_method = \"WLS\"\n",
    "exp_params.update(\n",
    "    {\n",
    "        \"downsample_factor\": downsample_factor,\n",
    "        \"bval_range\": bval_range,\n",
    "        \"dti_fit_method\": dti_fit_method,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a846b56-325b-4d7d-ae84-796faf451280",
   "metadata": {},
   "source": [
    "### Patch Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f8663-14aa-4587-89f1-a9b59c0e15a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch parameters\n",
    "# 6 channels for the 6 DTI components\n",
    "channels = 6\n",
    "\n",
    "# Output patch shapes\n",
    "h_out = 14\n",
    "w_out = 14\n",
    "d_out = 14\n",
    "\n",
    "# This is the factor that determines how over-extended the input patch should be\n",
    "# relative to the size of the full-res patch.\n",
    "# $low_res_patch_dim = \\frac{full_res_patch_dim}{downsample_factor} \\times low_res_sample_extension$\n",
    "# A value of 1 indicates that the input patch dims will be exactly divided by the\n",
    "# downsample factor. A dilation > 1 increases the \"spatial extent\" of the input\n",
    "# patch, providing information outside of the target HR patch.\n",
    "low_res_sample_extension = 1.57\n",
    "\n",
    "# Output shape after shuffling.\n",
    "output_patch_shape = (channels, h_out, w_out, d_out)\n",
    "output_spatial_patch_shape = output_patch_shape[1:]\n",
    "\n",
    "# Input patch parameters\n",
    "h_in = round(h_out / (downsample_factor) * low_res_sample_extension)\n",
    "w_in = round(w_out / (downsample_factor) * low_res_sample_extension)\n",
    "d_in = round(d_out / (downsample_factor) * low_res_sample_extension)\n",
    "input_patch_shape = (channels, h_in, w_in, d_in)\n",
    "input_spatial_patch_shape = input_patch_shape[1:]\n",
    "\n",
    "# Pre-shuffle output patch sizes.\n",
    "unshuffled_channels_out = channels * downsample_factor ** 3\n",
    "# Output before shuffling\n",
    "unshuffled_output_patch_shape = (unshuffled_channels_out, h_in, w_in, d_in)\n",
    "\n",
    "# Patch size in FR-space when accounting for the low-res over-extension/over-sampling\n",
    "# factor.\n",
    "fr_extension_patch_size = tuple(\n",
    "    np.asarray(input_spatial_patch_shape) * downsample_factor\n",
    ")\n",
    "fr_extension_amount = tuple(\n",
    "    np.asarray(fr_extension_patch_size) - np.asarray(output_spatial_patch_shape)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db856dd-72ba-46b3-9125-db6f2a78c2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.patch.update(\n",
    "    channels=channels,\n",
    "    low_res_sample_extension=low_res_sample_extension,\n",
    "    input_shape=input_patch_shape,\n",
    "    output_shape=output_patch_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2b412-07ca-4b89-9482-5b1ec453b205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data parameters.\n",
    "num_subject_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88174502-5200-4d9c-841c-765efef3bc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.data.update(\n",
    "    num_subject=num_subject_samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790d58-0fdd-434a-8938-e740023319d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training and Testing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06c0d3-16e5-4783-8539-3cd552dc6848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Spline interpolation baseline parameters.\n",
    "spline_interp_order = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a54aa3-67c9-41f2-b410-1addfdda9bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Number of voxels to dilate the mask in FR space.\n",
    "# Just make it 0.\n",
    "dilation_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197314f-8666-4c4f-8612-ecb8f739ed88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_params.spline.update(order=spline_interp_order)\n",
    "exp_params.preproc.update(dilation_size=dilation_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faa0a10-e760-4084-8aff-dd8deda7415e",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970f94c-9803-4eea-87d3-b2909081d103",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639b4177-5fe3-4171-a06f-b6d8ee726612",
   "metadata": {},
   "source": [
    "## Subject ID Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7205a-2758-48bd-a6ef-ec4ba2d9f183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find data directories for each subject.\n",
    "subj_dirs: dict = dict()\n",
    "\n",
    "selected_ids = [\n",
    "    \"397154\",\n",
    "    \"224022\",\n",
    "    \"140117\",\n",
    "    \"751348\",\n",
    "    \"894774\",\n",
    "    \"156637\",\n",
    "    \"227432\",\n",
    "    \"303624\",\n",
    "    \"185947\",\n",
    "    \"810439\",\n",
    "    \"753251\",\n",
    "    \"644246\",\n",
    "    \"141422\",\n",
    "    \"135528\",\n",
    "    \"103010\",\n",
    "    \"700634\",\n",
    "]\n",
    "\n",
    "## Sub-set the chosen participants for dev and debugging!\n",
    "selected_ids = random.sample(selected_ids, num_subject_samples)\n",
    "warnings.warn(\n",
    "    \"WARNING: Sub-selecting participants for dev and debugging. \"\n",
    "    + f\"Subj IDs selected: {selected_ids}\"\n",
    ")\n",
    "# ### A nested warning! For debugging only.\n",
    "# warnings.warn(\"WARNING: Mixing training and testing subjects\")\n",
    "# selected_ids.append(selected_ids[0])\n",
    "# ###\n",
    "##\n",
    "\n",
    "selected_ids = natsorted(list(map(lambda s: int(s), selected_ids)))\n",
    "\n",
    "for subj_id in selected_ids:\n",
    "    subj_dirs[subj_id] = data_dir / f\"{subj_id}/T1w/Diffusion\"\n",
    "    assert subj_dirs[subj_id].exists()\n",
    "subj_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da18eb-2c17-45b6-86fd-2e1931503820",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829406ab-a588-47c9-a7d1-1e4b9206ae0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the transformation pipeline.\n",
    "\n",
    "preproc_transforms = torchio.Compose(\n",
    "    [\n",
    "        torchio.transforms.ToCanonical(include=(\"dwi\", \"brain_mask\"), copy=False),\n",
    "        pitn.transforms.BValSelectionTransform(\n",
    "            bval_range=bval_range,\n",
    "            bval_key=\"bvals\",\n",
    "            bvec_key=\"bvecs\",\n",
    "            include=\"dwi\",\n",
    "            copy=False,\n",
    "        ),\n",
    "        # Pad by the dilation factor, then dilate the mask.\n",
    "        torchio.transforms.Pad(\n",
    "            dilation_size,\n",
    "            padding_mode=0,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.DilateMaskTransform(\n",
    "            dilation_size=dilation_size, include=(\"brain_mask\",), copy=False\n",
    "        ),\n",
    "        # Pad by the amount of extension voxels in FR space, so LR indices cannot\n",
    "        # go out of bounds.\n",
    "        torchio.transforms.Pad(\n",
    "            fr_extension_amount,\n",
    "            padding_mode=0,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            copy=False,\n",
    "        ),\n",
    "        # Ensure FR dims are divisible by the downsample factor, to more reliably\n",
    "        # convert between FR indices and LR indices.\n",
    "        torchio.transforms.EnsureShapeMultiple(\n",
    "            downsample_factor, method=\"pad\", include=(\"dwi\", \"brain_mask\"), copy=False\n",
    "        ),\n",
    "        pitn.transforms.MeanDownsampleTransform(\n",
    "            downsample_factor,\n",
    "            include=(\"dwi\", \"brain_mask\"),\n",
    "            keep={\"dwi\": \"fr_dwi\", \"brain_mask\": \"fr_brain_mask\"},\n",
    "            copy=False,\n",
    "        ),\n",
    "        pitn.transforms.RenameImageTransform(\n",
    "            {\"dwi\": \"lr_dwi\", \"brain_mask\": \"lr_brain_mask\"}, copy=False\n",
    "        ),\n",
    "        pitn.transforms.ImageToDictTransform(\n",
    "            include=(\"lr_dwi\", \"lr_brain_mask\"), copy=False\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d9c77-b4b7-435d-854d-9880bc52d85c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all image data into a sequence of `torchio.Subject` objects.\n",
    "subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals = torch.as_tensor(np.loadtxt(subj_dir / \"bvals\").astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(subj_dir / \"bvecs\"))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        subj_dir / \"nodif_brain_mask.nii.gz\",\n",
    "        type=torchio.LABEL,\n",
    "        channels_last=False,\n",
    "    )\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "    mask_volume = brain_mask[\"data\"].sum()\n",
    "    print(f\"Brain mask volume before dilation: {mask_volume}\")\n",
    "    dwi = torchio.ScalarImage(\n",
    "        subj_dir / \"data.nii.gz\",\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "    preproced_subj = preproc_transforms(subject_dict)\n",
    "\n",
    "    subj_data[subj_id] = preproced_subj\n",
    "    print(\"=\" * 20)\n",
    "#     breakpoint()\n",
    "\n",
    "print(\"===Data Loaded & Transformed===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f139a-7e54-4436-bd05-c8a99fa0134a",
   "metadata": {},
   "source": [
    "## Load UVA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba2548-e5f2-40b0-b3bc-429344390cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the UVA subject data\n",
    "uva_data_dir = pathlib.Path(os.environ[\"DATA_DIR\"]) / \"uva\"\n",
    "assert uva_data_dir.exists()\n",
    "uva_subj_dirs = dict()\n",
    "\n",
    "for subj_dir in uva_data_dir.glob(\"*\"):\n",
    "    subj_id = subj_dir.name\n",
    "    uva_subj_dirs[subj_id] = uva_data_dir / subj_id\n",
    "    assert uva_subj_dirs[subj_id].exists()\n",
    "uva_subj_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3c3b3e-6863-4dc1-b953-f6f6eef824b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "uva_subj_data: dict = dict()\n",
    "\n",
    "for subj_id, subj_dir in uva_subj_dirs.items():\n",
    "    # Sub-select volumes with only bvals in a certain range. E.x. bvals <= 1100 mm/s^2,\n",
    "    # a.k.a. only the b=0 and b=1000 shells.\n",
    "    bvals_file = list(subj_dir.glob(\"*.bval\"))[0]\n",
    "    bvecs_file = list(subj_dir.glob(\"*.bvec\"))[0]\n",
    "    bvals = torch.as_tensor(np.loadtxt(bvals_file).astype(int))\n",
    "    bvecs = torch.as_tensor(np.loadtxt(bvecs_file))\n",
    "    # Reshape to be N x 3\n",
    "    if bvecs.shape[0] == 3:\n",
    "        bvecs = bvecs.T\n",
    "\n",
    "    dwi_file = list(subj_dir.glob(\"*dwi.nii.gz\"))[0]\n",
    "    dwi = torchio.ScalarImage(\n",
    "        dwi_file,\n",
    "        type=torchio.INTENSITY,\n",
    "        bvals=bvals,\n",
    "        bvecs=bvecs,\n",
    "        reader=pitn.io.nifti_reader,\n",
    "        channels_last=True,\n",
    "    )\n",
    "\n",
    "    gtab = dipy.core.gradients.gradient_table(bvals, bvecs)\n",
    "\n",
    "    _, threshold_brain_mask = dipy.segment.mask.median_otsu(\n",
    "        np.moveaxis(dwi.tensor.numpy(), 0, -1),\n",
    "        vol_idx=np.where(bvals == 0)[0],\n",
    "        median_radius=3,\n",
    "        numpass=1,\n",
    "        dilate=None,\n",
    "    )\n",
    "\n",
    "    brain_mask = torchio.LabelMap(\n",
    "        tensor=torch.as_tensor(threshold_brain_mask[None, ...]),\n",
    "        affine=dwi.affine,\n",
    "        type=torchio.LABEL,\n",
    "    )\n",
    "    brain_mask.set_data(brain_mask.data.bool())\n",
    "\n",
    "    subject_dict = torchio.Subject(subj_id=subj_id, dwi=dwi, brain_mask=brain_mask)\n",
    "    preproced_subj = preproc_transforms(subject_dict)\n",
    "\n",
    "    uva_subj_data[subj_id] = preproced_subj\n",
    "    print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba4d43-dfda-42bb-882c-1a051ae88a6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rician Noise Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185a8c6-d966-4bfa-8d16-46ff9a0fedd9",
   "metadata": {},
   "source": [
    "$ \\log p(x ; \\nu) = \\log \\frac{x}{\\sigma^2} -\\frac{x^2 + \\nu^2}{2\\sigma^2} +\n",
    "\\log I_0\\left(\\frac{x\\nu}{\\sigma^2}\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebaff03-58e3-4b5f-b37a-822529fbb551",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # How to scale parameters of Rician pdf to include variance of the Gaussian noise?\n",
    "\n",
    "\n",
    "# def pdf_noise_var(x, b, noise_var):\n",
    "#     result = x / noise_var\n",
    "#     result = result * np.exp(-(x ** 2 + b ** 2) / (2 * noise_var))\n",
    "#     result = result * scipy.special.i0(x * b / noise_var)\n",
    "\n",
    "#     return result\n",
    "\n",
    "\n",
    "# b = 1.3\n",
    "# sigma = 4.8\n",
    "# rice_rv = scipy.stats.rice(b=b / sigma, scale=sigma)\n",
    "# print(rice_rv.stats(moments=\"mv\"))\n",
    "# x = np.linspace(rice_rv.ppf(0.01), rice_rv.ppf(0.99), 100)\n",
    "\n",
    "# plt.plot(x, rice_rv.pdf(x), \"r--\", lw=5, alpha=0.3, label=\"rice pdf\")\n",
    "# plt.plot(x, pdf_noise_var(x, b, sigma ** 2), label=\"my rice\", color=\"black\")\n",
    "# plt.legend()\n",
    "# sigma ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151cafac-c816-4b69-a267-ae3cf3dd0bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Patch-wise interpolation\n",
    "\n",
    "\n",
    "def rician_interpolate(patches, interp_kernels, noise_var: float, init_pred=None):\n",
    "\n",
    "    B = patches.shape[0]\n",
    "    if len(interp_kernels.shape) == len(patches.shape) - 1:\n",
    "        interp_kernels = np.repeat(interp_kernels[None, ...], B, axis=0)\n",
    "\n",
    "    samples = patches.reshape(B, -1)\n",
    "    N = samples.shape[1]\n",
    "\n",
    "    weights = interp_kernels.reshape(B, N)\n",
    "    #     breakpoint()\n",
    "    if init_pred is None:\n",
    "        init_pred = np.sum(samples * weights, axis=1)\n",
    "\n",
    "    nu_0 = np.asarray(init_pred).reshape(B, 1)\n",
    "\n",
    "    fit_fn = lambda x_and_init: (\n",
    "        scipy.stats.rice.fit(\n",
    "            x_and_init[:-1],\n",
    "            x_and_init[-1],\n",
    "            floc=0,\n",
    "            fscale=np.sqrt(noise_var),\n",
    "        )[0]\n",
    "    )\n",
    "\n",
    "    result = list()\n",
    "    for sample_i, nu_0i in zip(samples, nu_0):\n",
    "        #         result.append(nu_0i)\n",
    "        result.append(fit_fn(np.concatenate([sample_i, nu_0i], axis=-1)))\n",
    "    result = np.asarray(result)\n",
    "    #     result = skimage.util.apply_parallel(\n",
    "    #         fit_fn, np.concatenate([samples, nu_0], axis=-1), dtype=np.float32,\n",
    "    #     )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4567b-cb03-4847-9422-6d3f16bcb516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform interpolation of a volume given floating-point 3D coordinates.\n",
    "\n",
    "\n",
    "def rician_interp_coords(vol, coords, noise_var, pad_mode=\"constant\", cval=0.0):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        vol: np.ndarray, D x H x W\n",
    "        coords: np.ndarray, 3 x (D_1 x D_2 x ...)\n",
    "            The first dimension should have an ordering of `(Z Y X)`\n",
    "        noise_var: float\n",
    "    \"\"\"\n",
    "    coords = np.asarray(coords)\n",
    "    batch_coords = coords.reshape(3, -1)\n",
    "    batch_coords = batch_coords.T\n",
    "    vol = np.asarray(vol)\n",
    "    B = batch_coords.shape[0]\n",
    "\n",
    "    # Pad the volume for any coords that index out-of-bounds.\n",
    "    # Pad left/bottom/back\n",
    "    lower_padding = np.ceil(\n",
    "        np.clip(-batch_coords.min(axis=0), a_min=0, a_max=np.inf)\n",
    "    ).astype(int)\n",
    "    # Pad right/top/front\n",
    "    upper_padding = (\n",
    "        np.ceil(\n",
    "            np.clip(\n",
    "                batch_coords.max(axis=0) - (np.asarray(vol.shape) - 1),\n",
    "                a_min=0,\n",
    "                a_max=np.inf,\n",
    "            )\n",
    "        ).astype(int)\n",
    "        + 1\n",
    "    )\n",
    "    pad_vol = np.pad(\n",
    "        vol,\n",
    "        tuple(zip(lower_padding, upper_padding)),\n",
    "        mode=pad_mode,\n",
    "        constant_values=cval,\n",
    "    )\n",
    "    # Offset previous coordinates with the lower padding values.\n",
    "    batch_coords = batch_coords + lower_padding[None, ...]\n",
    "\n",
    "    # Find coordinates of voxels neighboring the desired coordinate.\n",
    "    z_0 = np.floor(batch_coords[:, 0]).astype(int)\n",
    "    z_1 = np.ceil(batch_coords[:, 0] + 1e-8).astype(int)\n",
    "    y_0 = np.floor(batch_coords[:, 1]).astype(int)\n",
    "    y_1 = np.ceil(batch_coords[:, 1] + 1e-8).astype(int)\n",
    "    x_0 = np.floor(batch_coords[:, 2]).astype(int)\n",
    "    x_1 = np.ceil(batch_coords[:, 2] + 1e-8).astype(int)\n",
    "\n",
    "    #     breakpoint()\n",
    "    # Sample all the points needed for interpolation.\n",
    "    p = list()\n",
    "    # itertools.product considers the right-most element as changing the most\n",
    "    # often, opposite to that of numpy dimensions. So, z and x need to be\n",
    "    # swapped.\n",
    "    for (x, y, z) in itertools.product((x_0, x_1), (y_0, y_1), (z_0, z_1)):\n",
    "        p.append(pad_vol[(z, y, x)])\n",
    "    p = np.asarray(p).T\n",
    "\n",
    "    # Order the known point coordinates for all possible combinations.\n",
    "    z_v = np.tile(np.repeat(np.asarray([z_0, z_1]).T, repeats=1, axis=-1), 4)\n",
    "    y_v = np.tile(np.repeat(np.asarray([y_0, y_1]).T, repeats=2, axis=-1), 2)\n",
    "    x_v = np.tile(np.repeat(np.asarray([x_0, x_1]).T, repeats=4, axis=-1), 1)\n",
    "\n",
    "    # Construct weight vector as the volume of the shape opposite the point's location.\n",
    "    w_v = (\n",
    "        (1 - np.abs(batch_coords[:, 0][:, None] - z_v))\n",
    "        * (1 - np.abs(batch_coords[:, 1][:, None] - y_v))\n",
    "        * (1 - np.abs(batch_coords[:, 2][:, None] - x_v))\n",
    "    )\n",
    "\n",
    "    mle_interp = list()\n",
    "    for sample, weights in tqdm.tqdm(zip(list(p), list(w_v)), total=len(w_v)):\n",
    "        #         breakpoint()\n",
    "        # If all voxels are 0, then MLE will always return 0, so might as well save that\n",
    "        # computation.\n",
    "        if (sample == 0).all():\n",
    "            estimate = np.asarray(\n",
    "                [\n",
    "                    0.0,\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            estimate = rician_interpolate(\n",
    "                sample[None, ...], weights, noise_var=noise_var\n",
    "            )\n",
    "\n",
    "        mle_interp.append(estimate)\n",
    "    #     breakpoint()\n",
    "    mle_interp = np.asarray(mle_interp).reshape(*coords.shape[1:])\n",
    "\n",
    "    return mle_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e4937-600d-4a6e-9768-c0b9cefcf55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pull_coords(target_coords, backward_affine):\n",
    "    \"\"\"\n",
    "    Convert coordinates in the transformed image domain to the pre-image domain.\n",
    "\n",
    "    Parameters:\n",
    "        target_coords: np.ndarray, shape 3 x (N_1 x N_2 x N_3 x ...)\n",
    "            Coordinates of the transformed image.\n",
    "\n",
    "        backward_affine: np.ndarray, shape 4 x 4\n",
    "            Affine matrix that describes transform from transformed image coordinates\n",
    "                to the pre-image coordinates.\n",
    "    \"\"\"\n",
    "    # Assume homogeneous coordinates\n",
    "    transform = backward_affine[:-1, :-1]\n",
    "    translate = backward_affine[-1, :-1]\n",
    "\n",
    "    coords = target_coords.transpose(*range(1, target_coords.ndim), 0)\n",
    "    pre_coords = np.einsum(\"ij,...j -> ...i\", transform, coords)\n",
    "    pre_coords = pre_coords + translate[None, :]\n",
    "\n",
    "    pre_coords = pre_coords.transpose(-1, *range(0, target_coords.ndim - 1))\n",
    "    return pre_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8187b4-979d-4484-8348-2fe60121aa2c",
   "metadata": {},
   "source": [
    "# Rician MLE Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9fbfbb-2a97-4740-896a-ca848d8fe365",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic Center-Pixel Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3e131-e12b-4874-9a64-6f738be07605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Select a specific plane and visualize it.\n",
    "# dwi_plane = uva_subj_data[\"001\"].fr_dwi.tensor.numpy()[50, :, :, 50]\n",
    "# mask = uva_subj_data[\"001\"].fr_brain_mask.tensor.numpy()[0, :, :, 50].astype(bool)\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(dwi_plane)\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(mask)\n",
    "\n",
    "# # What is actually removed with the mask?\n",
    "# plt.figure()\n",
    "# plt.imshow(dwi_plane * ~mask)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c223a4-3cb5-4842-8975-242008d340a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Capture just the air around the skull, not the skull itself.\n",
    "# # Expand the mask to encompass the skull.\n",
    "# air_mask = ~skimage.morphology.binary_dilation(mask, selem=skimage.morphology.disk(15))\n",
    "# air_data = dwi_plane * air_mask\n",
    "# plt.figure()\n",
    "# plt.imshow(air_data)\n",
    "# plt.colorbar()\n",
    "\n",
    "# # Highlight where the actual 0's are, and make sure we aren't throwing away data that are\n",
    "# # actually 0-valued.\n",
    "# a = air_data.copy()\n",
    "# a[a == 0] = a.max() * 2\n",
    "# plt.figure()\n",
    "# plt.imshow(a)\n",
    "# plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c2c9e-253e-4bb1-a581-d1c74ed0da5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prep parameters.\n",
    "# # Estimate noise variance.\n",
    "# noise_vox = air_data[air_data != 0]\n",
    "# noise_var = np.mean(noise_vox ** 2) / 2\n",
    "# print(noise_var)\n",
    "\n",
    "# # Construct the interpolation kernel.\n",
    "# kernel = np.ones((3, 3))\n",
    "# kernel[1, 1] = 0\n",
    "# kernel = kernel / kernel.sum()\n",
    "# print(kernel)\n",
    "\n",
    "# # Break up dwi plane into patches with a simple tri-linear interpolation kernel, with the center\n",
    "# # removed.\n",
    "# mask_coords = np.asarray(np.where(mask))\n",
    "# comparison_window = dwi_plane[\n",
    "#     mask_coords[0].min() : mask_coords[0].max(),\n",
    "#     mask_coords[1].min() : mask_coords[1].max(),\n",
    "# ]\n",
    "\n",
    "# patches = skimage.util.view_as_windows(comparison_window, kernel.shape, step=1)\n",
    "# patches = patches.reshape(-1, *kernel.shape)\n",
    "\n",
    "# mle_pred = rician_interpolate(patches, kernel, noise_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1fdb2-5c77-43b7-ab58-7ce898ddd2c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(mle_pred.reshape(61, 78), interpolation=None)\n",
    "# plt.colorbar()\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.figure()\n",
    "# B = patches.shape[0]\n",
    "# test_kernels = np.repeat(kernel[None, ...], B, axis=0)\n",
    "# interp = np.sum(patches * test_kernels, axis=(-1, -2))\n",
    "# plt.imshow(interp.reshape(61, 78))\n",
    "# plt.axis(\"off\")\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(\n",
    "#     np.sqrt((mle_pred.reshape(61, 78) - interp.reshape(61, 78)) ** 2),\n",
    "#     interpolation=None,\n",
    "# )\n",
    "# plt.axis(\"off\")\n",
    "# plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3b93f-fe29-427a-910a-d0d2a9571863",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.imshow(comparison_window[1:-1, 1:-1], interpolation=None)\n",
    "# plt.colorbar()\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(\n",
    "#     np.sqrt(\n",
    "#         (\n",
    "#             comparison_window[1:-1, 1:-1] / (comparison_window[1:-1, 1:-1].max())\n",
    "#             - mle_pred.reshape(61, 78) / (mle_pred.max())\n",
    "#         )\n",
    "#         ** 2\n",
    "#     ),\n",
    "#     interpolation=None,\n",
    "# )\n",
    "# plt.colorbar()\n",
    "# plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09200ac2-fb16-48b8-b853-788c05fb66c3",
   "metadata": {},
   "source": [
    "## Interpolation Over Affine Rotation Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209eddfa-6e7a-49ea-938c-943a830bb3e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.where(subj_data[810439].fr_dwi['bvals'] == 3005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb2301-8f00-4bbd-8391-d95c819bcd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj = subj_data[810439]\n",
    "pre_img = subj.fr_dwi.tensor.numpy()[3, ...]\n",
    "noise_vox = pre_img[13:23, 13:23, 13:23]\n",
    "noise_var = np.mean(noise_vox ** 2) / 2\n",
    "noise_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372ed378-9a8f-40cc-baaf-87afc7851aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pre_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a683631f-a1be-45d9-8cba-bea68ff4ba6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.rot90(pre_img[:, 80]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb998f7b-1145-41c7-aef0-ea31fd48fca6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574f8d61-184a-4aa2-a1a9-a8e859e839d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subj = uva_subj_data[\"001\"]\n",
    "\n",
    "pre_img = subj.fr_dwi.tensor.numpy()[65, ...]\n",
    "\n",
    "# Calculate the noise variance from the non-masked voxels.\n",
    "mask = subj.fr_brain_mask.tensor.numpy()[0].astype(bool)\n",
    "\n",
    "air_mask = ~skimage.morphology.binary_dilation(mask, selem=skimage.morphology.ball(5))\n",
    "air_data = pre_img * air_mask\n",
    "\n",
    "noise_vox = air_data[air_data != 0]\n",
    "noise_var = np.mean(noise_vox ** 2) / 2\n",
    "print(noise_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ea213-9852-4fc6-b6fb-d2f7bd4bc1c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "air_mask = ~skimage.morphology.binary_dilation(mask, selem=skimage.morphology.ball(5))\n",
    "air_data = pre_img * air_mask\n",
    "\n",
    "noise_vox = air_data[13:23, 13:23, 13:23]\n",
    "noise_var = np.mean(noise_vox ** 2) / 2\n",
    "print(noise_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f20dd0-5c50-423a-9d7a-60744fd15a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(noise_vox.flatten(), stat='probability', bins=13)\n",
    "plt.plot(np.linspace(0, 12, num=200), scipy.stats.rice.pdf(np.linspace(0, 12, num=200), 0, scale=np.sqrt(noise_var)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115e95e-2d53-495a-81a4-f2e06742b86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noise_vox.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31cdecb-4dd0-445e-8841-26990e492503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Double-check the mask\n",
    "plt.figure()\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(np.rot90(pre_img[:, 148 // 2]))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Mask Removed\")\n",
    "plt.imshow(np.rot90((pre_img * ~mask)[:, 148 // 2]))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Mask\")\n",
    "plt.imshow(np.rot90(mask[:, 148 // 2]))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Dilated Mask Removed\")\n",
    "plt.imshow(np.rot90(air_data[:, 148 // 2]))\n",
    "plt.colorbar()\n",
    "\n",
    "# Highlight where the actual 0's are, and make sure we aren't throwing away data that are\n",
    "# actually 0-valued.\n",
    "a = air_data.copy()\n",
    "a[a == 0] = a.max() * 2\n",
    "plt.figure()\n",
    "plt.title(\"Voxels Removed from Variance Estimation\")\n",
    "plt.imshow(np.rot90(a[:, 148 // 2]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aacf66-79ca-4324-9fce-c24de8e082b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "space_shape = tuple(pre_img.shape)\n",
    "coords = np.mgrid[0 : space_shape[0], 0 : space_shape[1], 0 : space_shape[2]]\n",
    "\n",
    "rotation_val = np.asarray(\n",
    "    [\n",
    "        0,\n",
    "        np.pi / 6,\n",
    "        0,\n",
    "    ],\n",
    "    dtype=float,\n",
    ")\n",
    "rot_transform = dipy.align.transforms.RotationTransform3D()\n",
    "affine = rot_transform.param_to_matrix(rotation_val)\n",
    "back_affine = np.linalg.inv(affine)\n",
    "\n",
    "# pre_coords = pull_coords(coords, back_affine)\n",
    "pre_coords = pull_coords(coords, back_affine)\n",
    "# Round to a certain tolerance to avoid unnecessary computation\n",
    "pre_coords = np.around(pre_coords, 10)\n",
    "print(pre_coords.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acccf2-b258-4505-b4cd-e827ac40e16f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(affine)\n",
    "print()\n",
    "print(back_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9aada-35a0-4e2d-bab5-b4da50f5f421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Interpolate voxels in the pre-image\n",
    "rot_image = rician_interp_coords(\n",
    "    pre_img,\n",
    "    pre_coords[\n",
    "        :, :, range(pre_coords.shape[1] // 2 - 2, pre_coords.shape[1] // 2 + 3), :\n",
    "    ],\n",
    "    noise_var,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9923100b-5ee5-442e-a354-bef5e164a44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(dpi=80)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(np.rot90(pre_img[:, pre_img.shape[1] // 2, :]), interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.figure(dpi=80)\n",
    "plt.title(\"Rician MLE Interpolated\")\n",
    "plt.imshow(np.rot90(rot_image[:, 2, :]), interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ebf3b7-8f57-4cae-9d07-6506a513bb71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy_interp = scipy.ndimage.affine_transform(\n",
    "    pre_img, back_affine, order=1, prefilter=False\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Trilinear Interpolated with Scipy\")\n",
    "plt.imshow(np.rot90(scipy_interp[:, 148 // 2]), interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b98bca0-61bd-45b3-a03f-82128703e439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scipy_coord_interp = scipy.ndimage.map_coordinates(\n",
    "    pre_img, pre_coords, order=1, prefilter=False\n",
    ")\n",
    "plt.figure()\n",
    "plt.title(\"Trilinear Interpolated from My Calculated Coordinates\")\n",
    "plt.imshow(np.rot90(scipy_coord_interp[:, 148 // 2]), interpolation=None)\n",
    "plt.colorbar()\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f822783e-d898-401d-a2bf-7186fd04c7ee",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6053274-e4ab-4569-b1ef-72baf78c54eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient Descent in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f605f4-3377-4eb0-8501-baabc75d3936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rician_nll(x, nu, noise_var):\n",
    "    result = np.log(x / noise_var)\n",
    "    result -= (x ** 2 + nu ** 2) / (2 * nu)\n",
    "    result += np.log(scipy.special.i0(x * nu / noise_var))\n",
    "\n",
    "    return -result\n",
    "\n",
    "\n",
    "def weight_rician_nll(x, weights, nu, noise_var):\n",
    "    \"\"\"\n",
    "    Find weighted negative log-likelihood for one voxel given several voxels in a patch.\n",
    "\n",
    "    Parameters:\n",
    "        x: np.ndarray, B x N\n",
    "            Samples taken from real data, assumed to be Rician distributed\n",
    "        weights: np.ndarray, B x N, with each batch summing to 1.0\n",
    "            Weights to linearlly combine each sample\n",
    "        nu: np.ndarray, B x 1\n",
    "            Value of the \"clean\" signal\n",
    "        noise_var: float\n",
    "    \"\"\"\n",
    "\n",
    "    B = x.shape[0]\n",
    "    samples = x.reshape(B, -1)\n",
    "    pred = nu.reshape(B, 1)\n",
    "    w = weights.reshape(*samples.shape)\n",
    "    log_likes = rician_nll(samples, pred, noise_var)\n",
    "\n",
    "    weight_log_likes = np.sum(w * log_likes)\n",
    "\n",
    "    return weight_log_likes\n",
    "\n",
    "\n",
    "def grad_weight_rician_nll(x, nu, weights, noise_var):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    x: np.ndarray, B x D_1 x D_2 x ... x D_m\n",
    "        Samples taken from real data, assumed to be Rician distributed\n",
    "    weights: np.ndarray, B x D_1 x D_2 x ... x D_m, with each batch summing to 1.0\n",
    "        Weights to linearlly combine each sample\n",
    "    nu: np.ndarray, B x 1\n",
    "        Value of the \"clean\" signal\n",
    "    noise_var: float\n",
    "    \"\"\"\n",
    "\n",
    "    B = x.shape[0]\n",
    "    samples = x.reshape(B, -1)\n",
    "    pred = nu.reshape(B, 1)\n",
    "    w = weights.reshape(*samples.shape)\n",
    "\n",
    "    N = samples.shape[1]\n",
    "\n",
    "    offset_term = -N * signal / noise_var\n",
    "    inner_sum = (\n",
    "        scipy.special.i1(samples * signal / noise_var)\n",
    "        / scipy.special.i0(samples * signal / noise_var)\n",
    "        * samples\n",
    "        / noise_var\n",
    "    )\n",
    "\n",
    "    sum_term = np.sum(inner_sum)\n",
    "    result = offset_term + sum_term\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d96f9-949d-4444-89d4-20a85128edaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (10, 10)).astype(float)\n",
    "x += x / 3\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c5cd36-6ecc-413e-808d-12bf4d879d8b",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{t+1} = \\mu * v_{t} + g_{t+1}, \\\\\n",
    "p_{t+1} = p_{t} - \\text{lr} * v_{t+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3741f13-99e6-4af9-8f8d-b2fdbdf80e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rician_mle(\n",
    "    samples,\n",
    "    weights,\n",
    "    noise_var,\n",
    "    init_pred=None,\n",
    "    lr=0.01,\n",
    "    momentum=0,\n",
    "    tol=1e-5,\n",
    "    max_steps=1000,\n",
    "):\n",
    "    \"\"\"\n",
    "    samples: np.ndarray, B x N\n",
    "    weights: np.ndarray, B x N\n",
    "    noise_var: float\n",
    "    init_pred: np.ndarray, B x 1\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up dimensions to be the correct shape\n",
    "    B = samples.shape[0]\n",
    "    if len(weights.shape) == len(samples.shape) - 1:\n",
    "        weights = np.repeat(weights[None, ...], B, axis=0)\n",
    "\n",
    "    batch_samples = samples.reshape(B, -1)\n",
    "    N = batch_samples.shape[1]\n",
    "\n",
    "    weights = weights.reshape(B, N)\n",
    "\n",
    "    if init_pred is None:\n",
    "        init_pred = np.sum(batch_samples * weights, axis=1)\n",
    "\n",
    "    nu_0 = np.asarray(init_pred).reshape(B, 1)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6985a6ec-c858-474e-995f-ab8c022b8120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grad_interpolate(\n",
    "#     patch,\n",
    "#     interp_kernel,\n",
    "#     noise_var,\n",
    "#     lr,\n",
    "#     momentum=0,\n",
    "#     init_pred=None,\n",
    "#     tol=1e-5,\n",
    "#     max_steps=1000,\n",
    "# ):\n",
    "#     samples = patch.flatten()\n",
    "#     weights = interp_kernel.flatten()\n",
    "#     if init_pred is None:\n",
    "#         init_pred = np.sum(samples * weights)\n",
    "\n",
    "#     pred_t = init_pred\n",
    "#     vel_t = 1 * momentum\n",
    "\n",
    "#     for i in range(max_steps):\n",
    "#         grad_tp1 = grad_weight_ll(samples, pred_t, noise_var)\n",
    "#         if grad_tp1 <= tol:\n",
    "#             return pred_t\n",
    "\n",
    "#         vel_tp1 = momentum * vel_t + grad_tp1\n",
    "#         pred_tp1 = pred_t - lr * vel_tp1\n",
    "\n",
    "#         pred_t = pred_tp1\n",
    "#         vel_t = vel_tp1\n",
    "\n",
    "#     print(f\"WARNING: Did not converge to tol {tol}\")\n",
    "\n",
    "#     return pred_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ea7e96-9083-4524-87d4-d62c3cc1fab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class RicianNLL(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, pred_signals, samples, weights, noise_var):\n",
    "#         \"\"\"\n",
    "\n",
    "#         Parameters:\n",
    "#             samples: torch.Tensor with shape B x N\n",
    "#             pred_signals: torch.Tensor with shape B\n",
    "#             weights: torch.Tensor with shape B x N\n",
    "#             noise_var: float\n",
    "#         \"\"\"\n",
    "#         # ctx is a context object that can be used to stash information\n",
    "#         # for backward computation\n",
    "#         ctx.noise_var = noise_var\n",
    "#         ctx.samples = samples\n",
    "#         ctx.pred_signals = pred_signals\n",
    "\n",
    "# #         breakpoint()\n",
    "\n",
    "#         log_likes = torch.log(samples / noise_var)\n",
    "#         log_likes = log_likes - (samples ** 2 + pred_signals.view(-1, 1) ** 2) / (\n",
    "#             2 * noise_var\n",
    "#         )\n",
    "\n",
    "#         zero_bessel = scipy.special.i0(\n",
    "#             (samples * pred_signals.view(-1, 1) / noise_var).cpu().numpy()\n",
    "#         )\n",
    "#         zero_bessel = torch.as_tensor(zero_bessel).to(log_likes)\n",
    "#         ctx.zero_bessel = zero_bessel\n",
    "\n",
    "#         log_likes = log_likes + torch.log(zero_bessel)\n",
    "\n",
    "#         weight_log_likes = torch.sum(weights * log_likes, dim=1)\n",
    "\n",
    "#         return -weight_log_likes\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         # We return as many input gradients as there were arguments.\n",
    "#         # Gradients of non-Tensor arguments to forward must be None.\n",
    "#         samples = ctx.samples\n",
    "#         N = len(samples)\n",
    "#         pred_signals = ctx.pred_signals\n",
    "#         noise_var = ctx.noise_var\n",
    "\n",
    "#         offset_term = -N * pred_signals / noise_var\n",
    "\n",
    "#         zero_bessel = ctx.zero_bessel\n",
    "#         one_bessel = scipy.special.i1(\n",
    "#             (samples * pred_signals.view(-1, 1) / noise_var).cpu().numpy()\n",
    "#         )\n",
    "#         one_bessel = torch.as_tensor(one_bessel).to(zero_bessel)\n",
    "\n",
    "#         inner_sum = one_bessel / zero_bessel * samples / noise_var\n",
    "\n",
    "#         sum_term = torch.sum(inner_sum, dim=1)\n",
    "#         grads = offset_term + sum_term\n",
    "\n",
    "#         return grads, None, None, None\n",
    "\n",
    "\n",
    "# class RicianInterp(torch.nn.Module):\n",
    "#     def __init__(self, init_signals):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.N = len(init_signals)\n",
    "#         self.signals = torch.nn.Parameter(torch.as_tensor(init_signals))\n",
    "\n",
    "#     def forward(\n",
    "#         self, patches: torch.Tensor, interp_kernels: torch.Tensor, noise_var: float\n",
    "#     ):\n",
    "#         if interp_kernels.ndim == patches.ndim - 1:\n",
    "#             interp_kernels = interp_kernels.expand_as(patches)\n",
    "\n",
    "#         samples = patches.reshape(self.N, -1)\n",
    "#         interp_kernels = interp_kernels.reshape(self.N, -1)\n",
    "\n",
    "#         return RicianNLL.apply(self.signals, samples, interp_kernels, noise_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda-pitn]",
   "language": "python",
   "name": "conda-env-miniconda-pitn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
